[{"title":"CoDL：Efficient CPU-GPU Co-execution for Deep Learning Inference on Mobile Devices","url":"/2022/10/13/CoDL%EF%BC%9AEfficient%20CPU-GPU%20Co-execution%20for%20Deep%20Learning%20Inference%20on%20Mobile%20Devices/","content":"\n 来        源：MobiSys ‘22: Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and ServicesJune 2022 Pages 209–221https://doi.org/10.1145/3498361.3538932\n 原文速递：CoDL：Efficient CPU-GPU Co-execution for Deep Learning Inference on Mobile Devices\n\n1.Background &amp; Motivation当前，深度学习已经成为众多移动应用的支柱。由于隐私性，网络弹性和云计算开销上的优势，在边缘设备上进行推理成为一种趋势。然而，边缘设备的性能有限，只有在推理轻量小模型时才能获得能够容忍的推理速度，比如在移动处理器上运行YOLO目标检测网络具有超200ms的延时。为了能够在移动设备上缩短大模型推理的时延，该文致力于同时利用移动设备上的CPU和GPU来加速推理。该方案可行的两个主要原因是：\n\n1）与运行速度比CPU快一个数量级的服务器GPU不同，移动CPU和GPU在DL推理上具有近似的性能，因此可以进行并行推理；\n2）与服务器上CPU和GPU有独立的内存不同，移动设备的CPU和GPU具有统一的内存空间，可以避免不同内存间的数据复制。\n\n然而，现有的协同执行系统存在以下问题：\n\n1）在不同的处理器间使用相同的数据类型。统一的数据类型在多种处理器上的处理效率是不同的，如图1，在Adreno GPU上使用Image类型的数据比buffer类型更高效；\n\n    \n\n\n图1 使用buffer和image数据类型进行3×3卷积的时延对比\n\n2）处理器间的数据传输开销是不可忽视的，特别是对于小尺寸的算子。如图2，CPU和GPU并行推理存在数据类型转换（DT）、数据映射（M）、同步（Pre-sync &amp; Post-sync）和数据解映射四种额外开销。对于小算子，额外开销可能会占据总开销的主要部分（图3）；\n\n    \n\n\n图2 CPU-GPU协同执行的时延组成\n\n\n    \n\n\n图3 形状为的1×1卷积的计算时延和数据共享开销\n\n3）平衡的工作负载分割需要轻量且准确的时延预测器。现有协同执行系统大都使用轻量的时延预测模型去指导工作负载的切分，然而如表1所示，轻量的模型虽然适合于在线预测，但准确性很差（&lt;10%），这是由于执行的时延并不是FLOPs的线性函数，而是受平台特性影响的（图4）。同时，还有一些工作致力于准确的时延预测，例如nn-Meter，但由于缺乏对部署平台特性的知识，其获得高精度的代价是需要运行大规模模型，不适合在移动设备上部署。\n表1 两种时延预测器的±10%准确度和模型尺寸\n\n\n    \n\n\n\n    \n\n\n图4 （a）GPU（b）CPU对FLOPs增加表现出的非线性\n\n\n2. Contributions\n1）深入分析CPU+GPU协同推理的性能瓶颈；\n2）提出CPU和GPU间的多类型数据共享机制，通过设计多维分割和算子链来减小数据共享开销；\n3）提出轻量的准确时延预测方法；\n4）在设备上部署端到端的CoDL框架，验证其相较于SOTA方案的优点。\n\n3.Design DetailsCoDL的系统设计遵循三个原则：\n\n1）充分利用每个处理器的计算能力；\n2）最小化数据共享的额外开销；\n3）多种处理器间的工作负载最优分割。\n\nCoDL的系统架构和工作流程如图5所示，其由离线和在线两个阶段组成。\n\n    \n\n\n图5 CoDL的系统架构和工作流程\n\n在离线阶段，CoDL设计了一个轻量的高效时延预测器用于指导在现阶段的算子切分。在线阶段包含算子分割器和算子协同执行器两个模块，算子分割器用于找出最优算子分割方案，其使用多维数据分割和算子链两种技术得到最优方案。算子分割器首先通过多维数据分割找到每个算子的最优分割维度（H，OC）和分割比作为基础方案。基于分割方案，使用算子链技术将算子组成链状，使得连上的算子不需要共享数据。算子协同执行器基于分割方案在处理器上协同运行工作负载，并针对不同处理器使用处理器友好的数据类型。如图6，CoDL在CPU和GPU间共享数据，将GPU友好型数据转化为CPU友好型数据交给CPU处理，之后CPU和GPU并行执行一条算子链上的算子。最终，CPU的推理结果被转换回GPU友好型数据和GPU的推理结果合并。\n\n    \n\n\n图6 一个算子链的协同执行\n\n3.1多友好型数据共享3.1.1多维数据分割在张量的不同维度上进行分割会产生不同的性能影响，图7展示了在OC和H维度上进行张量分割的情况。虽然在H维度上进行分割可以减小数据共享量（模型权重是固定不变的，可以预分配），但相较于OC维度上的分割，在H和W较小时可能会导致较低的处理器利用率（图8）。因此，分割的维度应该由每个算子的形状决定。CoDL的多维数据分割基于提出的时延预测器，可以通过给定的输入算子设置、分割方案预测并行执行的总时延。基于预测的时延找到DL模型每个算子的最优分割维度和分割比。CoDL针对每个算子预测不同分割方案的总时延，预测时延最小的方案的分割维度将被选择。\n\n    \n\n\n图7 沿（a）OC（b）H进行数据分割示意\n\n\n    \n\n\n图8 不同张量尺寸下CPU+GPU相较于单CPU执行的加速\n\n3.1.2算子链为了减少数据同步的次数，该文提出算子链技术，如图9，数据只需要在链上的开始和结束的算子上进行同步，其余算子只需使用本地数据。\n\n    \n\n\n图9 算子链示意\n\n虽然算子链可以有效减小数据同步的开销，但算子链越长需要padding的数据尺寸越大，将会造成更大的计算量，如图10所示，当算子链过长时，总时延可能反而会增大。\n\n    \n\n\n图10 有无算子链的时延对比\n\n为了寻找能够最好地平衡数据同步和计算开销的算子链，该文提出了一个greedy-like算法。\n3.2 非线性并行执行时延预测数据分割和算子链技术都依赖于算子协同执行的时延预测，然而现有系统无法实现准确且轻量的时延预测，主要由于\n\n1）没有考虑数据共享的开销；\n2）由于忽视了应用平台的知识，预测器无法既准确又轻量。\n\n本文准确且轻量的时延预测器通过以下方法实现。\n\n1）引入所有数据共享的开销，T=T_{trans}+T_{map}+T_{psync}+max(T_{comp}^{cpu},T_{comp}^{gpu})；\n2）通过分析性地对由平台特征引起的非线性延迟响应建模，降低学习的难度。基于对非线性延迟响应的分析，该文建立了如下时延预测的模型\n\nT_{kernel}=\\lceil\\frac{Size_{output}}{Size_{block}\\cdot Core ＃} \\rceil \\cdot \\lceil\\frac{Size_{block}}{Size_{basicUnit}} \\rceil \\cdot t_{basicUnit}由于非线性已经被提取，只需使用轻量的线性回归模型学习给定处理器执行一个基本单元的时延即可。\n\n4. Comments本文提出了一个在移动设备的CPU和GPU上协同执行模型推理的系统架构和方法，许多方法的提出基于作者对大量实验现象的观察。通过联系实验现象和系统基本特性，该文实现了针对于移动设备的准确且轻量的时延预测。同时，该文数据分割和算子链的设计也具有启发意义。\n","categories":["Paper Notes"]},{"title":"An Efficient and Accurate Block Latency Predictor","url":"/2022/11/02/Lantency-Prediction/","content":"\n  990d3914865424503ccb870b93b6c2651f8791c940b0bbc6a9b91fa3a0d2f602b1bcd59c5e35f642fa268402618056d56bc5adef5d001d46ae9bd0abbe3af928eb8eda394efd400dbf615ef7652f738d396575afedabf822323a930641c2581cccac17a951432c7f15327b642c7da06665900e67133f7d7770fffd8d16661dc927ee89daa2b9fcacbf20a6f8db1a077d9aafc08b16b07679c6f492c2bb5ba4adc3886c84a9fe3b4b49e14d95c3883135634fa6187620478124a1c6e816b7e464545b3ea70f85415c76d51bb853c830e6be3ca42784d681d385e2aaa0bd73c718b3b5a90de93ada6595969f95d2bf8f27409e115e6f9f71cf86f5267a7400097c0584271a704156925284cdb1583c0b015e741ba84403b80af290656defb9152e4ca7e934a9ba38e5f15d4dae82dcaf01204a4bca44c1eaec8f0aea48933666b92707ee09ac3920b63c6e953f85c1c908e1e94fe4fed63698bbbf7990eb60edb917d825c42e0f71c872995e95bdcf5d2a76dbdbc19312d5e9c1de67d848fcf39290423a29fe535c1bd1e69dc89ac8bbf77a6be7e1836ba5b3e866c3115e7f1d9ce416ea516d86bc2007c78ade783b7873615fca6efa8b0e94329ea82a6e5f49db2844ada2498a81c2949dc9a0b66c1ecc0b0cf4982c70b00f264b82ab6c2269d133e954ef6cbbfbd40f806d76a9026d6ffcd2141dc57204a09789146f473c99649960e339f5f130bd9452735ee5e1b8e4c793821be7c020fb44863236b88bcbf97f27ded2bc0239e6d3c8a43024f3cee21d7f73b30e0b3e85f8c6a5d691af3bc7180f58d61d677ba0b769b6add0c196de734d3d3e8cca9c9cadb535a1e0047896a11936d33c925a5c20da0e5da79282f9c40efc9183cc4f104ed89c351fbf2890d570b9620bc3dae60a2bf8b7e29e337b7c11b5494a5bdc957af80f7aa85d993d125d91eca8200272923b72e7a0773346f06761ef5176ba5de7d5ebfc20e30b35c0f918b1fc8ad19ec178afca70a565b4378a6e588cccfdc80f6804b6c836334c3e01b7216f1da0580eb801bb0a853540d5768ee7d88330aaa0976390bda830410060aab81250ae9ce187ccb2c09fd8b27a9b1b66f342b09509c25e7992eb1535c5629f1585ce93fb2bb04b9bdb1fe7008ae442ac16cf6f008343249e37a11ae2ab98baab549c197cdb891b9bf613e9acee98caca80ef07e8d36c8201d4f16fbcb9bcf4af94461ecd5dbc5dbcf5c38a6bf8c6e2eb606376f96a1512142f7755cd0b4762866fa7b413226568cd37fb093e6c75de2f7796b66b00ddda67c718a84892e13275666cc7f775baa0c75665205d65797204192322fdabc819967bae69fd540e369d1d61f43c72d20eb1b0921e7c060277f98579f8110e6a771482f347eda0028f3ebb437bfee653958fe0e958fc8d3215787a4255314eef3ebd7e9738a46c22933190b56cdf215f411d252db54ab8aeeee12d49197943a15507e2d59aa392ff9bdda084bc23853eaa483883cb2342c17c617e1351e147241454094e1c5efec6cf6dc09610a5af041d0a5f5ecac35dc5cb295745273bb6637071f1907197672de320adc22a48420452eb3060caa6e2b91a3af8b3c61caf30ec3c206ee3940e7a8c90ee57f2fe3514e2b831af3c64ebb7c8b02da3ecac6e2294185d402e7471ef498dd83e1278c1311b1e6f86dc5dd34227ffde1193e6b9492a2d2ed78cff729d90d606049652e94542eea30bbee17525815922ea5c6acecce0960426588ce76b4e8847b223eeaff2897b8fb7ae3c592868a877cd1ed5afdc23cf73d4a361c597ae58665a13fdc5e1095c00f9cc99926502e8ba726193c81e1f56e1df63037ac35060da6a7b23fc40543ffd7e34746044a49abe4674f3bc223fc0c7325f76a881d86c397ae8e0c80655b7f4ac5bc1bb0c43fe8c542616b5687f8538486233411e7b47a78ceb7ffa87a64823b7a25b85b210dc302884dfecdbf152126df6f0b74f567c99faccdf19515c049b7dc9de40224c90f5ae4ac29bb86c08e828527b93bc33c0e96bd3d8705f76041a26295ab3c5b86aea1ebb5854a785be5d49aa605a75dc8ec9f70a5513526f77ba817e88fd53cd24a04c5bca20991521029e350ccafd0b19b52af3541eb179b0f4fb078e5c0164e5f8333d0ef99892d10af9c27ce0e495b20e6f719fb515fe5fd695b09704148ddb02c639504c8f9ec060b414f2115035038cfc073e96a3548d4ce7924263a26ad17e79b509af92d055c405cb4a2b3da91952e9ce1dfb638cb740505c3e95e1398e65ec286bce30f540f5211fec067b72dfbda57f07b996f9b464120e2c809193489023a79339bfbfa4fad18a94b5c01ce85b4a82b6f209b4aeb3773370d560a648aa2ecd6a6bc016e7f92d2e74ce77733756a80a8beb823f8c82ea8dc24e51ac36435f262bce59cbdf0162244803a1cf823b4f27fda7b2a481cfa9e459bf0a478744063eb05629f48563f859d7065d1f6ea1897460a8b1262911017d74255050ac252e619a288c4875d6a3aa756e6b1cdbc0816067fd28b37a711d2d3d26bb7f078e497a47b4c9dd907684b0fc65ae2464c7c3a48e0109ae2e84b9fafe0c9bc2b71963a992f72514b8cdc7136098f0c96d33b60e1e89c1f9a45bced80e45bbcbbc3bee1cc9aa7d752faf18d27058ecdeaea272e1888275d87499956e0907bce20f3644751984120af3c2d4b645823ea0fc09ee417cbcfde58ecd972d3eba3f35ddae249f711db512f46e1c8f5f5fd048c64e68674dac37437708e86a2753ed62a588ae2337332d40438c7b0dec8cb53c74932b59f3197b25599c7fcf1bf3cbe1fd11ee07ac85cee1194d0f8fad38bceb3f6acc36a3db80a9935b82782a00135958b08a15f9dc5fa6deaa42a72224822e280c239ca6769c8b9de74ff00005a09f3712253634db06ab4675c665090ef0cb15ebf4076384abba12f8c5c7e89baed057c7e55b4a22506368870581fb1064e674de3eb1e1ce7741478f3940fc593acf9e19400de81347ea27d0f304955cb7c681c3ec0f216d6a5dc2983aa801fc3fb98129773787e83e5f7b0379b5cfeddbd13610d5c2310d8a64df9d77b69d01c899fa15ee9ec3df18c162fa1710a5811bbbed39079f4738451bbbdb6f806c8da218b5b58c23d4a4562513631198567e71171c053119030505ae11a727d29ffde888fd2960abbfd23e5a128c112f9b8aa3df314d670e4d0ac9d64ad05aee5a1f2a478fd2f01f1e75bbb1da9056b5b9e86e608d4d537097103b56a6b839f46de0a113621c7e9ca9cc74092eb94aae157e37f4f4a99b084994ea91ae8234e08bfa79bd3b186f792dad2f01f351fa8e24b07b1f5e20013dc0bad803ab32a173840f17dcc497848ebc3cc0ebb2d68607cd8de469458b48ee239ad367c650da11b3d735e573c0f1e8b55552c99f591a121ca82f6e72b4bfe7e8692232372553710ff4c02ec1835f8e17657f74acc0639b4734f61df13eea719be86e478a510a71b8e7070e20d18db210898c1f1108ea2886cb87fe227278240aee7d1fa195edfd4d0b246d1cb040c4f991130e4a109649e94c62196fe5431ea2f22255ec76f39c280b1e9c458c62a8b217a4efe64313b4d8f223e82f4260127233cb4514c4376c5d4fe0703dc5837ede2a17ba567336b84f0a68653290e25418eb26b633d72a0832674f34558ca8a18acb6e11d81e6bb6985b1db018fc60f4aa908c565f16341c6ecbf8c497d74368c6f9d20859fe058ebc8fe5c2feea839c5ca65a853f3a858c7db8136980228ec1b580a78bea98c46adc18318ff805005797cd19a592b869762baed6b7b8301ea8dd9770d32cda961b6b3a0712bfd4d42b1dbc8b0d750eddd9c88c2b253df7ea838dd68db92a31c3d6e0d1594abdb374c5d8127322eaa87d0279a1f928a3bc35d2fe2fddcd49a004d210fe84fc4047ddc71af7a7eeafadade387e4654817c29edf824aca1e29bf381f7c73f076796ce5cf5a53deb480c56ab3b78f1f7e06db7b75f495fffb14ea5c1fb6c0abe81a650e6b607053e19bbfa084f7609dbf5ff6d2ec0da4f70fe2f87aaa3f0b6e4699bd83b0058d1ca7330c7ba6cbe601770421a396c715188bbfd26e76bc96edcd84e9e29eabc54e3cb753d7be1eb30583b7401888fbc7fd40f471c42a1e4211a25d50c91590e37d2da9933749504b70f9303e015b85db887028c5fb904d5d9aa3d0e6f9ed79b98ef6a8219a3c938288e026f18cbfdcbdeea3b06fc4836bd8b0652455b14f6d06dc21301b3e1cdeb8f70a89eaadd33d29fd0ac82be3aef8d7b7aa00127fa3366dfa2d4cd830eaa1da59a54b86b324831be7393ea7ebcf3ed83393d824d1e17ab9ca9d8f22b015dd373f1ac532c2996f2b438c5e92926ce10d29dd312f8e606409ed9221b6a34192f3a986c727958ea36ab8c287481eeab19bfb63edd623419ce5879bbcbe701623381c3707dd85c0475a30fcfc97b4e9e65f821b4aaaecf83a58afa07ebfa4d371c328158171a7e4e28a00b06cc666b60e512036c562420705b776a460c9b79b5d13046156ae7521369116c6554c1434155b5cecf7f786776967912aa3377d65cb05aaaa389533c1b07e0409f4063cf6d31f2135f781484b7929953030384458b864e61998b609cffcb53f7407eccda3af5f69859e2bebd62673e16c68ad5c2146e259e0f2d7d3647072283964c7dc9817cba780d4e43aa5cef4be1caf8dd1e25ac92d162584dbae4b0e0f5aff1a346e153dc745670a2f4a2f8190ef9897e2c3609c99b1c9f4c36e53777f8e3bf687d721c90c83c0d26f5aef0486406fac7b430d7d9ae9207c9ab0274ebadb78daf05fcb0e9a53f71cba6a2e66bc39c552a86807a19241d06f988298c590dbe9ce5219b6832ca874c1e50ad20370dda017cb954a07cc92fd445219339054be8b59103a0337a6aaaad6b0883328ac8e30460c75cceed9d8f1247d2f7d39b4028eb0105d7748fef22c81392942d41ff375f026d5c0993f88ce239e0ec31d31222c5be4aed01d9e17726fff0ef953fb853f24684da31f733a133f100ca2f238a58d76b7050b00e4dec3bde306bc4c962056a39d5ea1bb8278048e579d87d1d4b3c422cbca28384a470dc38083d4abcacdbf0540234dbe1c894c5f26d2a00af1548be8b872e203af782b5b4da64ed4498945bd102fdaea0087c6461dec95879f2f74a12bfafc2a9764ce7e84e08a1f7185b6b96bc207d928c7e4cd60f8e7adcf9c01cf82bb6fbc066f35754a6c8e3f6bc0ed675194d38d820e8833ea58bad804ca18b6c3370ace9e6215707b4a405b2b24ffab179a611dc6ea250fb8a1bfda099b43297f12f2268e9de3fae51c601fd9a099cbe040f816e3142c24d7eee742432e07e4ae77c572009440808edc3ccda3bdd09151459373c3703d02142a5a5111478525970bd6e6b11953299726ca833b0378a7ca4938caca3668a68fe18f00c4e31c957e7130da270e12988c15f41d5f7951e57f5eb83b2d960cc979979dddf498cb0900261bdc2cc9ce817404dfad1aefa17a6575aec474c729535a368429d48742099ec00ca7590a2e585911a3c9570287facaa40cdd0e6349285961cff08ef24f8533b8e87d101b595d1de0b79c4ee890baae7055866a614fcbdb045892970923684846b10deb755e51eaa5b15f031aba6c086f512dadfdc844ff1b885828374172460b689ada69211ecf0d545c75ad9a795bba1a09de80c174b6643a01312a06602c0fa5ab033a2a4dce7bd21f4dc5453ce42de6b36642bf2574fd5a0da9bfc3db9ff982fd72f96535727ec45a8dc73449f59641c87dd7dc3d2f9746af569c3373fa99320018d6c0e4b5a29246e324f6ff830daa295e5d22109dfacf87922ce297cb3f00143c6bafe9e00ced8a6a9d24f117a61424c982e3946a777f8f7909cb1c602b39451cf6169306ff1aba6e2e32406519722d6f216a65942b29868fc74a7b52f08fb30827eda1f467328ca7d6370a0d35913fadd4a74b6537bb1ef66b07280c8465656d1be743ef08e6deb3da7e3420478d015c46b950c80639d058d631fd943ca2dce734a1ee3325938f0dfd7689a23c98108cd665b91dffe3dd1baaf01799e924680f349ac576de99649aac22581a1c191f61c3cf83478b64fc23eed483cbb913968701307fa87452fd698ee8178f723b65a13ea3e49f9704f73bbd8dbe75f3d99328355e910d1b2fbd65055a844333e4b6f9e3d050d659b4afa8bf4b503ca4e13379e200238718dad37f150bff8a208acde74245ee393e3d29cd287d1c61c2012973ee82d263d7b50ca2a1cbfe176175d1d95fb68c822fa854d6b41b560a95bbe6b0868a74d0ca08fef3a754471d501d3413101b26a12f89fd72afc2576e7053738a24ccecf046117d1ec55d02f4d7b7bf4f3bcd1fb1c46f2d3a8951267cb661a0a1d70ecc3a0aab868f9c743b64baf80fb00dedb17ec02b64954e5dba75df999408b0aa9e341cd38b84aab303d4a98dc92efc39a0715dda0133bf0f205f17015282e5202c6931101a869b9accb8b611697fed47361ec65e34de264f8b56a30902e60d41c2d49dbd88914783ad86eaeb3cd304fbbc1664ad8e428d52c0465374f9cc869074e4c9a88d817daa80340b09fc23d6867914fe1c44b41d4f24c14da505207dddfbab434dcb8d45727e72ee7a72f810f1f854c34f96b7b4536b7e8a64b475c428fc294e240b6646fdda8d332016773b4c88cc87179059be5e74db79eee255112c2a6566953e73c9c9e2dc7c71d20fbd6bda46c086e10d1b9db071cd468a9fcb4dd0b00e19af68c71ca08f3ae8c96b64c597e7a2deba9fc53674fac1933706c5eeb7584928a945f203db1a4e75f41374b548b07aeeae5c177dfde37b95fad2fa8756a38dc71a456c9cf3029697eea6aac5752cf2934ae7a4538d8ee29e8f4d81a22797001324d993394d6aa94b8e3732dbd7fba3115e3c12c609fb82dee89b63bdc8e6767469c9c17676d848283837e993c04c5691864a7f9a7803388e52cbd617071e86e37a91a583daef335035f6e95a3df48e2d149800b5a526773c5f5fd9650c330671fa3ba30b7e0a7bc21b5c511e2924d45e00f23dcc5ca78cb443df8d151f60b31d90b6e0f58e5240a4306e5e5a33a0efb6f43ee9e0246f4580cd7e346e93ac5840a7d354ca7a43ad16dfd0666513c9528beda54a3e4ac26761ea9c8d9047e0b44509ae5a6501e61ebaf4a4385d535819056a5ee0ffbeed7016db713a121cf79873e22655674492306dfc90b61839782cbe14ba7d609aa838cc794f4d5afea903d5cce09bd638eb789e97ccd6c8b25f10ef5682fbe33056d14ce728fe7785a5192607f49493ff37ff4d66691115851388039e89a8ac3e9a6ccdea2e13e3979882b2f23e152e6d24cc180827a7c0625bb3b96f547082f637d86824dfba5be38d9fc914982e9bb7bfaa5d94aba9833da7ea9c7ce2f73024344ac73cd2bc8bcfab547e322ca7466f3808133acf0359a2d1ed2283f70e12447469c1c5cfd6c7384003fc89bd4d2c491f65849d7427877df0fc8fc0b82670df348743eed224b64e542b4eab7acfcacbe5667c042d04f06c9ab88672e86887a6a349bf57cb2eca5440494556c4388cbd2f5f4e1615dd6008b98821fde01ea614afe20d5104663f4b578a5cf8019b34632a3d92fbcf481556cc3f0acd4d9c3b6daf4111c8994ff046bd35d0c923cc0a1fc37a3a38ac743afe7590fb0627ee1dd7c2d0af34e0b54a37402360b36b5cab51a016b15322c007622f72a44391a00fe6cb460dac5eae88f1f96c68c3c953b15f44a8f778f9293e4ddd94fd9fced44ffe5cdb3c79ad94a5e7144ea8873db9421bb81c4a3a8fadf315d934e415a52dfd4322536646c3f588257d4806d8674229e8b8f3a96876775ac622727bbb24e0d14a083057b45148258c00c04131f4f32f06146606f27af60ccb07955272b345d058a62fe8e1f7e12d0dbe53f597b99585c2b87e8f37d63013e9ce29652dc1762897d964ac33efca7d574fe8c13b510ea2884d6c5ef1f1e8ae2241a19606187d164a496df8efc0298be32f3ae387e0cab04e2bdf10f6ae474500f5b482e44a81aa1985dfa63edf02df020fea19cb793f48b1df775dd1bb929386c0bf4c9207b6691de31f3bcc8d0a7ed1344c5520c04545b764848ca29d0d9bd4f9281c7548423d31b49ccb1a8e72db497fe18407e8147288548226cce7ce3ab4e80a38a70be02daa35b83970cf9bac26a219bb83f1cae6767f2b5ca925d428e44e4fe110e2cd139f77dbc6a7c01e241524a85f4268ea0433fb0b3710fb53831a1cee35558e133ca3649a7c0dcaf1d8aef440a9aee682c11a6f7f33535aaded8c812a3d96b3e87a8eb6380c9be610bd6d246422a634a699ec3931ec8fdcc9ab4925f6a344d0fbdf67874290b31052f4e210aac4e4d69b6faad28fa07f84b82a293256729060154ece92857896df1145b99812a87decd8e0ab744c5f621cc5128980aeeb5bccd6fc078ff91bfb3428507eeed79dbeada62a79f148cb1737c36cee53aba60915041e022e6891592f77fa795c01fdb4e1f4b99312a37d73af16570b8679b8d3567e5576f2d3a853784ad824fab1bd5ddc45f197f4bae0d5ed7e903bbc3771a17fc9704e008d1ef545fb091e991a10bf0a7e685ee723357cb2c610ed990831416a590c404ba3b16aa6b0daab7d9d1d246fc752be21e6d846beab2f2b87a5a8af5e61f6f314b3a1f9fa2cce5b68212a5797ac948d40287d4ef7cf66cb6a94a5c0fa6ec6cf39d2cf373f5a3b189f45c5ae411c7d2f2c07da673aa49da92a02ddd61947aa5f8d425baf059ee71d1f88d794f04c1e3733c10969cd156eb6c4319f117071236d493b16068746c9bc813ee915193adda1b83b4b2cec61559ae3c247341e0e12e2ee6d7687d67072e5ae3cb5f4d12b544c3231548f5226c52fcd2c9d802ae6adadd110ce64218053bcb01fd3c1ba115650ce588847e3bdd98176b6f8285bcd6e8e8778a0b475cb67ca6d8746c40fa9344f0456b887390701d5d6ee466678face2c4a3216b6310716ba8d620baa94939c381e870fe5b2fa7fee3ef24357cc561c68e0f92ac3e80bd8b1fc04d56b2565c509023cdab036c5c2616ce48bc63f52ace701121368fa9ea21558773367e98db052713d78ff5ecc05cb25c41f967dc26444831782a02fc009f716294e8b2a08369e229939b7d895d46e278f67e1277cffca9d90882e3b9ff65b9e864612d8e8f21973a732168bebeab7ff028afec603e1bb70f664acffea916ff2b04f24518735d643647a643d1758b74861ef08e33ce3547bd97acd543fe9b4d77ab4a8fb25dd3b5267c8fc826a8f404aa5c2f03f821dd7f36b72e6bf10ee7c036e4f168b72a17ff0d39374c4d694bb8070befb837af9d4cf693659cc1a06b2d16cc4f96feb967b723d2b5e4d74c02ff792001ecd39cd35ede8c2a499cdcdfb98f00432297846b18bafdb175f1cde99317ee831a98e33d133e1a69d8a86e87a97934aea179f926a0905860c65b67e0dfdf6581f235d37128dc81da1d8d7c4b73ad163c582b11c718c48650a5455cbeca61e6025773f191045bfad4ae721b68f291ee2153b0fa955dca0a14469415c4fa056569c4fc842353b79ea51f1b8a3935f250635d1dd101554afc81764f58ea47012f27ee790d2cd6fb86cac9d35491e2643a6c894396c6897c93676f6e0d7b6b934a9d83c73448bf410bfd56ecf9f2b10f467798c26612ffd57fc5b3aee0a325181c06defef47dc804e1254753b0ff19ef97959b93b751cc394932c85aab02dc285d266715f181ea9cd6d1733095f4f54291e6f2720cb262f81b3892b8314b65d996e0aaf21265820ff7b50040754bda2eadb03a0c583c7a3af83eceef7c6c41cc32ef00cf5ab2abc6748c48d6d1ec0428c98c74fdbe0f6d2529f7821532c302d22d30976e18c66b1a57ff9219612e327888e246049bb5cd08acf635b0d03669f6dcc69b9c676b0dbfd8cf50b8484863dfa7a28ca63bed2488a5451e96fac067b85b8b287989e0dcd66395cb7e0a49228adce88f333c02168f1bebf6e31a58bc5ef8a571d17170d2fc538ddabca476c7fd27bc179df8401f1002047926111e59ac3a6f84fd7e576b320173fbdc3c63b1f4b1179cd5d0fe6979bbe9adcb812a64b555078a77fc4da5283b59cdc7672a06d5de28d3b6e5e9f80188b1223eb712abb3b50ee4aff3069451a3d04c580b3df6b4d9e80c85ce93a6b87efa2bc9e9b306cafd03958a3210c2c7cccbd589cd71f564985ce685ded8f0be14416400a6b10570b8a2e52d2b439c13052bddec6d2f52dc9de70f82ba8ce5696275a295d415ee2607ae805c82840aefbc1c56b3e5146a32252c1b66a6a6c7ef4c5561136d5b1aa06b06a9d650ea2caa2ed1e8cdf46acbb21445e4d13bad124e2c5a98a42801a4d33d3d150ae539d70b422add0929fecd3ce6528a53506ccf16c84e4afa65788ceb86649d72fcb91e2eeda04918246fbfc5c033c8ea98dee48a3f56b1642471a79510e7073fa847d75f8f002e1e59d786ee525650dc4a805cf0629cc369537f5e834f6c96c73300d2e6d074dece23dc2b24dca1d155991f88b9d57bdfe249247bec8ba690740756635ff52f7ea3c91b9b85f7da68604f7958a8c49a2c02b7c299491bab5a216823583e608a8f2cc0c6641d94d34daece3a5ac18fbe6a5e0701be4aabba5d7e2889d374ffca0f8eff00777ecd80e7aa81088b42c75fd48c73f22faf3496573646ff82a95c912f849df9bdb75af8a5b5b295193f34d1ba7d69081b7d63dc299e2e7de7a7e0bb7d02c4361653fb8a6e00f4effb764ea31921586daa221dc735bbc417622ee9be23ba1b4e2cdd06538865728ede1c21250b15166941de737c13b68a88d27cd6ad8c1a9c0bd36b3cb0dfcf2e903db4925856ee72ba4a1e9f8e6dfb956cc9de0845eb41cecff019715ab8784dc9e22f5cee045e3f1099d55ccf5c9f2dbda4c8daae520ae05d20cd1a6260ecde26cec34e661c8721cdc5a77a6cbc1415cc07a735124f824d283a1e8a4093fdfb0d332ecb6084ee3f7408a2a36f548b142cced96d4f9f9fcf95ad4310739951078ece07ea608c95679bec79b74d25fc22c63236ab4230632dbf66ef172aa33e643f986913e79aa54cb5109f352677f8b9a229f59eb8ab92e7e1fa85835ca37b22c003502b0467a54a54415d0fa41177adcb5c90abf442978cbf11aa10edbdedd893dfd77468e48782eed86b5c2de2e2c6cb83342b4947d856d1aaf2bb695454efe722ca1c1b59262f0c57609b294f93a855455bbd9a45f7e5ebdbe59b4210cb030e6c69ed056fed2e82791becfe45c86e7a1a76678a6598e1cc1628fd3fe813c2ad8ba003f53fe98dad9c5455d1f99e33729ac03b28c950687425613f0a40ca56093b8c9bffa420538b4da640f8c373090db80cdb2029b0e5138fa1e87000a4625938bf4c95adb8970689e404d2de6d82c589768ba058e24d15450d301032aea926eb6592e5ebcdcbf82de29f602d2799fe38c40fa0ce0e08b8b34a9beecfd2bb26f8462d57f3f2b081cd5b5930200b72349ce9fd0be9fda7002727c98b34d41f2daf518c1be3ffb04f4cbbb620dcd956e6b047f444c7de7e2f7edbadb0b1c3b341ed388af90bc2639655ad65130d57d8c46ce4bb5969c49abc65040555a71af5b7b93c0d1775c3f432682946c25ec3fb7cbf6489744d20fc7663027c762e43459298a2b4a9e9697bac82c7546025830fc804bc3a11586c58f11920a14112588a285f09ce7f16532a8f0a155bfbd77667e956b5ebd20f4a2597adf9db1cc08d4d38d3265affbcf658c6933e42b86fe848e6b9e0dbafba6f1ba3ace075725c9074a20bedc201fb4f1a01f6d3bfbe05df045c924cf8e02fb3baed7f687df48e6f0837b0f0f20109d6f21e0995242cbff94d31ca05bd7ae5e022f20cfbcc70f85d5a74c58e46caa272e16222394dd9e2f49fb83664f0e596dc8e746f265eb7627c12eb933a1c0897c35a3dd3cb9c410fcbdd6fc91519e2ad797ae311f27f75efdf0f90cc05cad8b093cd89b80d8a079b4c36f1b92f1f3f1575b31d4ac95ac6c070685e0ef3be284fad1179894691207ecf4ce0b7d28e58b66467b0ff86e738e87dd5a594e850e80dc8af3be47588c41eb49eb93a3d3326c4f7623520c650bf8211a307acb1de42fa28b1452b3dc83f0956d1fe8f8e129d85287f9740517b3d8ba7ffce49eda8426ed9cfa3f9018aed9e442847ad6376a79cab13dcec5be05abc81e8756d104e68429f27deb807cbc8e21c719c5c3c4f8b7691e174d08cf968bd708c114a62bba31e568b3a56d5add0f5680adb345961f2a94d907be31bcedc849ccb205aa5421c35f25d791c1df87dfedb55a2da366f792b04ff81f68868f9c51c032e61e3b0c52979a699babe4696462e7e43122cbf9774cfbd90e2ff176e2bd29bd6554a2ac34dd92e50d9e100e736c79c6a47874a135528c77a9e43e40abdb6642d349eb8e9f72a37da16c9146b21229a03d63cf59e7fd484faa0806c9da60bf711002665558a6ee4300f732d8d499bdcb1b352d62ef533dd39bcf90df178905fbe8e311547d195290bf18e46f1f85bda1ec0eaa25aaef3c9e92813c9338edda181c6f7a7fee4d2f0a2f641e7cf16a3b95f42a7d439b2f43cb54c97edddacf9b7101e446610b7f85cba8696f9b9ac8812ffd1b97eb6a20090b11105da52166aff64975b9ec3707d4e6906977bb4bc395d7eb278d30a3701da80d843533d680340f5499b96e1e5451392b579f6e51c0eb19b3a73a6f58960483e4091ddac6bfd0bac1408a723d14499250564b089a5cf258000907c10bb92bdbc23445f5711ee66e16bf7128fc0f8e797f93d845aac150653c39087e19eb078dd2da7abb514b0ea452abda7ddd5ffea0afa82a517c41b32673a9fb843b02fab52d54c8e88306ad7c1232e2e7ede90dae1dd56d5ea01d74bf776ea7a9287d2c2309f162411569bb785044563518cdfe4f084a367d16d50aea96ec75ac9f72b823f54971a7f0f626da3a356c9d8a8581f81bbbfbb108f35bde59cae161e8af33473c6717e9eff134ba50c29f92fd6c014a7e9d1f309f2ad3320997b43cb2b03da822d740e0b4765a66ac147943c1aa7238382774dae9519cbfdea84c08b1c38ef3f043bacba18da8736641eecf7a96b19f6964911118fb0401e8eb3443598a102542120ed1aa47ecc5e5d5faf83738d2db22693a1817d5e0e479f7036bbeab5c35d5c59abd8c5589832e08288fadd9e5613658018e227f8ad3faf2cd86e54dace38a33b5a25d40ae4225bd636c3c4926bc5fe08dcdb124e5a6950804002bba2673a4fed03fde28d4f08b7ace0ffec03b2c563dde5adbb4ec8e97bccc56f9900e7f1c47dd527ff230d60967e4ddae0d4c759f4a09a4fc3d9ae8a85998369e94cae130695ce74171b263a99bc117d234c3d1730e09051ee8b480ade1ea68a3880a0f3194a9f29c3ac079169532286ccb001af499a0f4ce0726a3d9db86859622cdb5be8014555608e5faf11ed9010523d70bc0ad202cc92f7fc4867165612b817339ee5d3c9845bd5b30e84363616a135f0e15e50a52c8b2ff346df656a2f86ebf48482f90295cd2031ccb9adb2d10a7918adcd1e3b53a806499c4fb7f22a244733f2da0fdf329f291cb86cbfe561ef11b9979d134bdf8fae4602a7030dacc9d8d9979185aa5bf369ad1c89e2e082f37c70cbc79ca02511e933e20c9c7ac094fda9cd5cbdb7a2cd466e5c9a50ed01f4d51eadf81d42164fb9eedb7773d900c9f01821dabc87e22ad8cfa04011f6b58d17192cf4c494cc27e58b4be2c915530dc70d9bd1f9efa11e7aff097d152021d061e6b261d411e3df1a65e16f0e16ee8b4ff9a36c140e2149226f18cd49225c954daecf857c64b2e60284ea5a4014c090cca1734d893b2b91f245ed6e561e3c262bf92a7d18a6c4223627e2e067487915d092a58e5e4e0a1c29da83354c7832740b4832510926c8af24ada1b6e37ed42e7d1b2bd6447a2774706d3013b2284799c86ef044c29d3da7003538b3f16d8e1c6467f7c278616fe21fc2c48b59815d0edb4f71bf5799b308d72c656c6902fefcd0b604dcf9520b77ae5946ee6d64878189ed108d89ea70c75387245b08d7215aab5b05a80a8f1326fa4d40ebce15aa2c5d520089c390b0f5cf85ce64355d768e4528fe1739f6b4efa153359f00228d970786b778628fe4f8edc1116d55e1fc6e4785015ec7cdd41fc9e3d40589533e6c8e6d0ede8f3a129ae61077ba88ae3b26cfb6889cf973ad8b67ee3d128fcf155796b243e360d11710bc0f213b7bb514c4c6346314cf343050365472f8b430db22f4f420dc369d4d911bdd86856eaa2c52b860e2fecfe45029fae6132191fb0fca973d6d8287a88ebfed8476cec75ef3f23e7f2843f498c45cf84f1f3e1d14a6f4db4d564851951f50957abac65616029fb9ece404e1aaf910766bce497fba84914f36f945aab3ed1c8162266aec3f5fd090dc3bdead0aee304cbdd0d9b54ff95ddcd469c06303fde3dece3d54d176f264752b464e2842ccba19e21be98e689a354ef1ced57abdd29fb48f0ca4a28c28528135d04425ae82b508c5029f3fd8ce7d8294e36d4200900c941af3f5a9ee2fe40b533439437813420555899b7909de6c4000e74a001fc8ff4529917551f7accee3b4e2b9b6e6e536ff44608bc7a432beca06682144438ebbd9aea8ed7598d9c2016917ceadd9d531a99f669bd7a84893582d54a191ea6249e03c614aa4848665643b958ee0ffe40f12a8c6597dde5df8dc648ec551f32d46a2e16996de942c16be28b9093d3d99391d735c683476d3da133311d47bbaeb70807b67264b7ae30fa13cb72017f90ca7f83324182b413f983c5b3da995d90036726c9e234ccb18d629e6a86f425e328a0a3f3b4b87555a703727dd31162600a48a114af452b316e0bea4a477210d2603ee34be625c9a3f94385f42e8afb460e6faf5c853f50bc02c6b1945e66b0d1c387ce135d1d2ff5f44009b5152bea0568640ce688988fc77aa8a2b97e32876d827ddbffb7272f1b7bd359c3ef84f1d4343c293d317b05012e32cc59387536ebaccc8b2267595e8ba7e971d32ceeb06b970d383884774fbff2299110321f044972d7c04ab04df48acc4f759988d1ad4e6560b3fe69a58a6ae3f81ce0cd2d7982019bbea60e3245464c5525b61d2575c6c945d271cf8c2298f954e8f3e1e78d6e8b813e722379339276d2d7f099ace25c19ef72ab096a76a6fccb8ba9f19d6554cf20e3397078beed6da4fd636448831ea81d64a16e83f65d4846f2a1a5b6e042e9a292adbe8394184314a8a5a36f04ce79254e300e1240ab02c619ab9df314b973269867737206b78c216fc74457757b799dd51d3f380d470c863bf4d975c16a94496ec2df383452389835e504a5975b0bff331557fc716e6b7fb8efa95807b2ded81b78f1fb158727c3f118a1f18423144a39d6f84b9b7d30b4a45ef7b93f08b439e4d22f872aafc3c586407ebbd5140cf5a12cb2f32f35b45d5144bbfbbb2efc900de4a8d826b5f65be8e9bcb9eed05e4c3beb78959a24d887bde5cb5c2622f304c576f13e41a6a7f006f41bd40041bcf4e07a2efd9c7ac8cc1f1c2f6f437118c7e163929fe061a43d57c2caf84e8f8c0d23db11182bd6995d54bf2a8f8f7ec4b06d4ac35fba3addf75887c5db3cd65260775fbf8f1551ac789987f12813f29f16406c6716b6d4762d9df0cc28eb1a6fa65d702369b877bd69bb8a6cf9f9f0a273aaeda8550f66a9ab6087d5c14bd17c6faef2ce0d587bb08ad877f95b46174852ec85220d802b42f7c3fef6a8fb072eb5bb3424c791da8e46dbf67396e3139065ab952b883f79fcfbbf2a02ddc14bf7fc9a3a642e487ab632b493de6e6a99d606ba7b30bf7cc0b320d43ef2a8f0ea745a4339574e52d3a3804e2f923ab3b20745d33e043c318c37df7186a425300699bc9c4f2dc7182080d3392d75f179cced90776f800652511b4b15e017d9435dee962373b5e7f8b21898164a3959921a163801b563626ba9b3d67a72c61587aca6250c10217f088b9d4ace731fbaec90a624fdac71a36872afa9bc35f3d1ca1f274f91172f99e97755f7ded05c8662335560e73952c8f423be349aa48c8e013ae9d32c70fcf43eedf2126df80257ad27bb085e41f72bfb03fa26d648cf0bcdc5105380a63e0c276a75816eeaf8ff730eb2761146e2ef5439b0c7f86e176207df005d7ac2d4b6b727f26f8f5a3eb9535a3b2920a162a9134577b1a5a9dde9799f6f8af4a57a7d14b992318e8a948f77a96e6262d2b7b0fbc64b15cad7962f4dc7321569c77841255e306a5caff3675beb6cb32b065160dbb904ab6542699432fa673442a49110e516a2c8a2bdcc72f80519bffef7671615c48e74366efbc1aef4887a9260cfa876a666bef5b412d07eae6e14dd2c4271bf54be5eb49925b2a10d7596440c7dc01c21db0a6fbf4fecbc73ac61835543e992b7cf8dd6cf71e8829d98257e68b84fec4eaf57c57ca5fed0471eaa5e9e0d26a8e7083eaaeab168ecfdbcb9e14f35c748ca6494e03216db3d0d2ba24e2fac573cb89378ba66d3478d1435a3f3be29cb86d5014f761a3069170f31b5ed579f5e19ec395f869702a92c087ba9b42927a52f9423e5737046ed23b310e6835c826e6d9cb86f60f38b2249ac00e7fe8fd41d6755fc41384b2dab52e7961f3f9aa14df2d49af8dfa341e9fb3da18d69308c31afdf522af825cd4d4f4a151de236186bae53f407b13dc2110192f81adf27049f2f035fd5687dce8dfb86885593b77ed0765e47af2acf146ce10560b7d4263357489d44190b918975e69e5b329e21deb53e15c972228f9d088d9bbcd1350186889eb04c66ce9711baeb72df4e6e25a7b2f350ab63c59384d13d6588e469edd6a972c9396bbe709ac42762633fab4604b7bda543aa70c1c480cd9bc3ef9723a2742ec04c26fc1ac092382a5d3f7d549302907812f7b5f5c3684e8f09111538641a4e6a18adcb97dc94202e38ae27c24e901e632265c4e1f6c5b56c84e45fca5a3a34e9e5f8f973062cb419fbdc79f8fda7d07ac9570575ea61476a32f4cec2b41580c7e864ffd32af5ddb0ba3550da7c0b9c77082f13fdf7ac81fbdd7e5a9b25b321bf4280a40f0be4bbfa84cefaa31f00589e97d85d435324c9f1bb8b75dc2047aaedfe97fa8ec38068beeb07055fc5c56cd3304a1a3692e9c25eb5a5a79bd4fb7b41d145e3ab56ad46cbce1137b20c8bc9a9d7ecde8d2132a3b697b2acbe8896bdc896c90069589eb3d5e7f0a76e8c21167d0b3c03e610499951714e78964d5b678c0d699725043203c71e23ffb18b92488b6a75f3511bf454db9e566067a4cec77ec96ce19fb8a3c52ea2e5aec76759239dec07aab359d47a0c59c1c57730b5a9535f67ee9ec2bc3124f3682edbe95163f04f9db562c8074678b29df7a9b8e98558794711b604d87240cf1001e5608490f410e12174bdb9b3bb83c166cf68c3df930497066b51d9819dfaaa635234fa7e9d521dd3d4ec9d4f9e2e877310fb4919aa039558d193b8c70bedc07b58821e84ebbfc5b81ba5e3b23a412fe7504e0c5d3f435e4430d22104401ca9bdf82d657d6aaff1ed321393109e8ab166ca76364bb36aa328698b4b01be106038222f0aa07325a8eec5cde0ebf805a37c8d7c2bf3aa5d27c0f75e3211e34ab76dce1d85047a3e2d448df4fa4b72fbb62832eace1aead1a74e810ccd3c88737dde6f26b2ae73bf4e4927f7a99ecfa1e03ae43828f97141b771fd621ff685fdef4d85554a79663c36cb86e5677c0c9aef90f0924c7202197cecab0ca8547c34705527ce4489b06a24b47029f3f32a4c064608360cf7761496905bb491044d5f128dedf90bdcc978ca0f902a1493b2bd8651a14d6616898b3e99b9b64b5905bf8ad7000b823f8970b8e2289f6ba1dfbdea77fbec9c1656a2d0e983634a4835fc3adc136c4396566fb5eb3d30eddde15aa93ae8fb7a9f5ad66ba7d47cc5d92a37c6372e30c582b3f8aa7395a1fc581a16411cbbc410dc2eb23cb9024810c7e52fc303adc3a0e1550e0e1508ed63c4acac2f76fdab1a646f0a293536341a3de06d1aae6d8f8edf07720f8465605bd6edc4a5ae74589c452f8166e1c91ade51338f5e54deeea7023c887c4921b03583b28eac306ccc5bc41489ca355e9d3f1d348971bf196d1e4ad03397f10f94bd77c6722c021cb579da329a71dcd57f6191fd86f5f8e45faeb04d3899009b6e23a542d7019b710969aa6f950391eb52c1538571e63221542cb3cfaed10be66b81c55ac744b273619fb3fa8023846ff4df63dd6c2abf1e5c02db2d2c6b38dc0aae724ee99792fead45498e6d7c4284700365be1e7f9ddf8b99bd4bbfb3747358f162486ecaee15ec7056a8100ecebddb98e4702359da143b1bdbe2e37b042430304aff057b4bda23503efa4f30e19911a73d9f38759f3ada6fde632f51c8aa68906a2cb9d4d599f3ab875316f683dcad13be83b20fa2cd66696358a6d4df09f40f67b7ee7e46e32893f06d611b5bf026cef57ee954474ed5e52debf18799b56f0c8c736237959bff1eaa0d8a6d15e6a43ca25abc0441b6f3796785ebac4c7785bb7f067b03ed82f06517fc266426571f48e57e48241daab83471b45c5cace2cd7d4cfd84f9ddee73759711269681e7cc54b7ea5216964bc8dd6c9c9c28bd372b9c1f151ea02832a63683502acd93ad26f183c456ab70c0800bada354565367590c35d097f9aebfe690c47597572a473d1cd52ed291de972572501df78b006127fd1d984aba964aa902682df23fc21072b188ba9d1e46e5d815fd48d8cc4a3b727b1573f828e026090cf2d242a37bc319841fb1713aa811532e7b53b9ec385ed537adb036eef37035f467dcb47ed58408230c16b85aa9fd1cd7edb318c53a254782cbc193afc1294eaa7ff69cf4eef052964698f01741ae2896aa67e20f0ab91df7c7119d1b03d0b6681a3e47b3b306d4c24055e436c0af28c3a6476fdc576b131a74c24d846ba265cc71a580dc0c4da949d93a3fc83a00308e3e21267ca3c2b3db537140cc52a112c318bb2bc345afbcf8fea03aa82cc4447c0072ccc24d5d24e0179d07226e47a865476abcb6ecdf57e82b0d12bb5c4c0b8fb5c98982b9fd72a34468f053b159b1365dda32ac34b3e1512a6621081df7c9163b41525745f6dcfbfd1930f96043b46f50090ecf6a02fc173dc3f1818e739d543fa4d079f6beead068d3f5baf094eac41e888af66fc75587a9c4800e8d9fd5cb29ec87000ffdb0bc83b3af0ae55c58ed983e9f114f328b2ce4e0d548e89a1d496f23278f52cc178dde680db3228ed986c58bdbb93f8be9202656337f2640ab7d8919393c80c31b09133733c2a10c6d19e8acc197a9226bd06ae569f0471e137b54d50a62b0d2eb2149eedba8a68ecbcdd283d4ef6649fe1854fb705fa975b7e7e321a204bddcd3b94e3b661215aafe4e42da31b193d09b3405d100ddd3631535bea51b3f9a784dae0b1c1fbe4f1234f920318b89a291c66ee87af3fd05ae7119c94f41d68c82f0141e69b768c20928d69b76d9a1478067f332e7d21316b899633a9e070434c7e13a90a58f4f8baabd7955a4a52f19d5bb5445b320e0e420af7c87f1542647078ab34c73b1dabef08caf6b65ed4662517885ab466a676a1eb65cb47b7a9309d9e4b708671d132761e1feeb37f9e1503ae00fa2fd1735300875e8721e7d56ab36efd8cffe8e75d37c1d993e3a6a20b5db3aced438a8757ca957120c1bde93554be6a2dbf425b77a1dbedb0cb479cfda9ea48cf90f94d8a3fdcad799f7ce1d5fdf1190d7cff65ce715e849c2402a15f301139e265ac09366f163d11b8adef8c5b41537615fcc429851e169a1c244a68c1c9bc537bd7cf0bab9111807729b35b442ab9bd8371bad553e8e1efa9d772fc512793c35807333ef0970eb9a0b58f6bfa9a1fa70590839cb1b0c8ca4189b444361908aa49e28dda13fab8014769127b993c5707ff73318eba514bcdfc5\n  \n    \n      \n      \n        Input Key\n      \n    \n  \n\n"},{"title":"Elastic Collaborative Edge Intelligence for UAVs","url":"/2022/10/31/Elastic-Collaborative-Edge-Intelligence-for-UAVs/","content":"\n  fa0daee49b22b44f630baeb931fe2da7f5416d7bfdb76d73c41195ed5afe12ff47c77877829854073446567cd61d608c89fa3dd9599b00d8b731ecd7a049e2bbf60e76dcf582bf016217e319c5d7de828d8b22237ff6fc33c26fbdcc61a50a2cba3a424f27b7b503f09fd0fc20900e9fed861edd6103087473bd1b131d89d2ca32b735d1a5bf27ccebfb3110efc6fa6a6791f54a783e697ae50b8d7e7e557a13b2b00f5ccb2c613a09a23f99c9daf67839e2a091e87f1bfcbeeb8b55aca86232c41f31bd81f8b864278bbba198a4559f631ecdfa9b5b3c58802da7f71a9b461e75693918f1df3f6367af55c81cf03add895e8e0dd352866c03e7c6c492fbefa2db9c7ac9e136a9e327e5b13556b22033e4c923c204bc5c639626b09da6cc6278b1f842a70a31cb7be7da828c3651d34145396a92f8d50471af4a58eb5dc69622734f0500129f577f42b15d56a3b7212f19f46a6a98fdf49a095ff9010c5d02b9cdfe14705baa68e9dad7a42d59a475cb197afe95f67f5338cfad8bbd90e4ed3bd6fdf67b91571a93ea64a6e9ffde18516369529d856040d7ac202f297dec3a8b1398490a3e41398f2dc4b7340f5f4e0ce2be6640628a91cbefcba02f11cf7951c9c1c0d783d61de8e785e75a434a6b2d438a70a6c64423053f81c56c86d7f9b3c4aa314f2a8f93d594fbcfad1ba9f3abe8674e526dbefa3bcf4d3944bcf4d3b57786f5f61c8769d72ad23277f19d5bcd44b716d9a226c93a1aba5221133b3d110758c8c2bd2fb8b718528da7fa851ecf0eff5885b83eca65ed1b0d659ffa5f17c425da8457740992ac5115f708deab5ec7c479805086066b1c258805b26a3142f3dd409236a8e8040960a05cb5b5647eebee196413325ea233147933489fe795ac0d20f9f9750cbb18e093538dbcacde0158f53ca47c737fb6c3b6fd3bdcb32430d3f77fb2322dba671a5c8847c2d0942dc7f46c2895098ac1ae0cf575bbad3a604fb9953c1dfe0c609d4921781325198c58e7f6f0daa895e41747760ba10b4db0df0f9e151386343ab3bd1abf79a17ebd2c0cd546542bc34ee93cd6cfb28ec1b0e4c2c1cfad10d504b352333a03f6d8dc20cdf13d8baa5b5ad8335a144a5bc6f7b43d7b2ff767202b303440cf5ccd3bf713b4269362894babe9ec8e0c5c86e5ec9e9b2f1046f460add713b4b25de1a6f9488d35d49d4373c4b1f6d4387df7f12f8c83eacb40f36615d1677a8c7288b1005e323a2d58dfa0e359fc2825af9229b41b4d1e767028c07c41ea57f41abedc623b938433766013bac0a3ba2b218e0e46b416575e2a5b417a146c641aa074b10869d46af7c604e415052fb6194c07f6d60f4f2a8a5df94a51bcec7d5fe14fef2087907815a457290c76f3fcad36d3fa2900f3781fcfd705a00f3a9749d6dde44c8da232ce3daac1b52d76ebd48860e461d60861d98d745a1f75831816084cfd5cee94e1b51b87e70a0f09f00183446e9ffb36db1ff51d8d3ad9163d627c11c406ca0b87c91abff6131157583583b2419c5a3bcabdb081290cff7ed76a3915e233fde1b826fa332acd6f38ff915747fc241d6391c78a9213f8eefaa6fa0c94de4e5ee4f1241e50f97c290535a8b2ff249c8878651d6f7e0b983ecd8779801ab280b9cce1b04000dbbb98ac3abfb86aa96fbb52a91941bacf43bea99677428ed98426103d03b573d854aabf2f3c625616fe20c7868c726a5124704174cba70c91f6778cff1d943b599408edbff93c0709e3354fb1d614964d2693399e1af74cf3e12dd7ab4918b38b561dc92407dbffbe9769f0d83ef2a216082e33389937625f7396c484fd7d6651ebe35c090d3143fc81695d36e0249aca951cd2237d1a3c67db451258c4e8612a6028fa9e2b0adce307cb503fa0d46a7a2355955861c785295244a47d24b4a43ec0d3ab4175e21edeb05be1bb9d2655697b3fe7f140428574047fc61f5c38c7b32ffc277cc1f1b854d7043f3585f6ac3e2a599ce245054f25973e46f83083f06b4bfb8a373c37cd95d803e90679f34192789c1759574270e30184d4b50eff3590edda9e8a9eba3cf7eec374d5888881da811984960549bbd8b9280e6485f7e9010c07c00c949892e712e7997abb32648668819c4e21e8847b15816694a9a454160a3bd2116882049d89ccdc377f9991509f93b7dd06379eee415823ac8b80e998f7202257c1a5f86e5e7bb26ffa62932ebef125dc36f57c0f6bea89b530f3da28a83d470613f3cef805e2d66934f35d12785abfe1fc56445864d740be944d9179ac808360dbfad1ce8a3081d8ac86ce6e2dd82c44b6b8c9f64860a26d83660aa19517871bd1c92132e1a2386ac4d22d1a39d847ae5a06fd3a0257644ae6ae7139231d7d085ea00121017073abecc7dc6c83054fd41190e41f04b14d572602ce7dd05cd98aad67b5409ad4c3042b9748ea188df58450cd86f05370e4fac406daf608df015517b9bb65bcec35c2abe97a9afd22d20ee09e356b1eaf1ca3c1b11ddca542f67fbad37884c0c6f26016ee8370018f5945b998c4b9fff47dad8ae5f564f576deaf12bd3b7a8a460f722385c7f57d5a267b43978ded7bd92d363ed889a2835fa37b6cf6d042d08497edc4e0abd38ac9ccc22174e09c814e7fbbf39df1e1d356f7b4197aa489bcb71217db7fa50e92a55c6380057c1f6e4e1fc333ce85c80eef4eb6dfea5787f34a46c65b42d944ad044ea11fd2ab0affff18b3771a24c6c38fe601f67a25106da7006d6bc063aa7bd0dee981e5ce4be4f75c1f26e6f46f62762a182add45963b4755ae65a4d6fc5510ef6eaa812340012853e117aafdccfdd73d5b4a4d17aed32082cfe907158ff4a99f2a93846bf9771b8a5ede3867769114190822bc6954e3a91b6343e68e1cd60cdffcdbcb399ad714fa06d3e5f5add34c4e73941384dd5722f6d1e3fa056994cc1d6ccd4ec7326380dd183293c1864a8cb2372da17bd118b02c2175f10d403c62e6b48c1ac1cc9ed3d77be15a806e7600be480e9b2886d2ced56faf40e9be3731ed114e4e5c266ddeab728ccd075ec05505f9c95ee5c440aaafbc569209e77c38923bab0d047fd4625e549824b7182c76d9d5bfad32133a86e3eb02f921c345efa03dad0be72691e8c441ae16656118a53f016c6df8e7f487eb5807abc5d4bab4da75b595679776e77c40abf4201a04b1ae0f544cb9478fb3e1b55bc1a350bd59dc7e6319985d717ab0f484fb54de043160a417fe66bc55376c5f93de43a21d5971a1d3e1d1e1dedf703c77b3d35a67dc9e7a5e9699205eba2f0efb2a046e7bd9e86d907e3f193fb02c936c9f441a517a58df4347931d103e0fe82df8c4b1f10fbae72888bac952a01fe6fe52f4f73380caa95d8d4e2b71e6a94f46a45d27689b6d25ad5a6f90881548d0fea77f6c75fed6b3f40256dc0dfc2fb8ba035be53c547f1832140429977602efc4fef60df91f7bf41d78a3dd3a51620dd04a9bd5cfcfefb20fc341ad148792fdf73d9ea4def1331675e9c0006f0144ee81a3aacc4e9c59e7ae502cc0ff1ebdad5ba40b6edc04bc82da602a8384aef3ed0ebc37d44cd913b868c6d750f5f2e1b7214a46880e9a2d8ece032c07fa3bed90ad98cfa95e48ca8917c5060b3b3b4e135a4c48c4410e080f0006b04cb231d7b84cf01f60abbd6ba8074edbbbff20e5f12d04fd058f03305b089bfdee2789c90e2385fc592a36f89f68388399d9229a86b02695688193c0eac3a12282015bad42c814f8400237dc549eacbb6398dcd7f1ebd4267371cbc77150518479f7bb3eaa1a654984861578ed221abf885f8bd37e3c6bff84fd3be1140781ff2eabf85516b962624a88479e54ccd9ff44b1ad12020660c9ba10d304f8e862d1dd43217b75275b3435fc443c4518c211a09af14d1a9d4f018980f90383c1f82ce4dca941ee4f78a13153bdce755850ddbf19f7d6579307afd4619175c0045a025652a7ae57c0350f46943792e3afc26f8c730e73c036cdde503a8face10b474c5868a7d40e9961d0ea535d6252dfe41fdd5aa83125d5834a8f88a579804fa8999cfbdbf0b873116a68443d5ad0a466af32c34c5f5940da266b25800657bf586276cfedecdecd0db8c962d1d5e94704108e3d0570c8f2f8defade75e5cebf0ae2d1c2c9a8e0687b0a80d8f8ce22846b840a8f81933960631a9e95a66c7b984d5d7bef1b82ee98ff0f456742239b4d3b4790e06b7ba78daf48d92962d8288719b1cca48bdd2f3c5a54b3254967a6e432573c268bd0da0caceb180cc7f650e008ea6e3192a845839b2781cd92dd41e466a3bfefaaadcfbac86c26d1e3fca7d970034f7e5572098f2d2783aa26638b56dd72bd57250e61b3f037c5bd217753f44a32b4771bcc83b44c9bfd113a2dda445543e84ff89715669d035ee93bb8b0cade38d4d79a1aa4578a172896dbe8269e6ab85486d738684de6fc6a524aa1e492d2fa9a687780eb589365f374454ee58e70d83e53d8cc12855cfcffcd35c44f7a28973753bca6f7a73c151887b972697a02b9a68a49429493f6fdf1232754fad0eb553be0483ac56732d357ee2bc016c0ddb13f9c4b88a9b8b03d56a1b4e3532a90aad265655df7c4d9882880a8c50ccecea118a12ebd1c8647b2280758d7fb39ba25eb40dfd00e6092c554a6d3f87385f75044ae16fa3e3772ef0003c465d0b470a8df1ce06659af5b4afc25b4bb9202b6805dabdca79968d65b411b7ff8f7da5f37a1a2f266d3fc53a8b046907f6a70b877fa5ab7d734b09b8f1ce2059af3bebd1bedb0295fcc2641f3d137dd9b219e95072c670d6283040cd3a4789d7634bfb63871e87ec774359bf40f2512f0edcdf00e4a90799252de99cc28aed528572843a24c8cab8b627cef9e88c4b60fbef4e6450d78239da521e8accc304757f6ab19495b5bb9c09eb1403dfcc8289678d8677f643a38604d4d6e6bbd4c51f620e6be14e90462ff7935bb3117871f6b78f7c0397a4a9009a21fbf224434a5f11ebef2ada6c9af0370d8e779d3e05053b76ecfdf4ccaa93c4642d1621c31d15baf9c106d5195922afeee77e48d8f0e6220e971050e124a44c1ea5c099dc9a6bc9366c2eaa3c67bcd3e57832180568b052a576872c02400c483ac4e9f99ee49334280bd13e090d0273c459b9b87c94d18e20552c62c5d4e1e0f978e48bef10572ad17c094df1673cb134bd40918078662764f4d62defc5892e64589fa77b0788e0ca353586756665d8eba0597f2521cc5bf673fe1fa5d480e01f05efbf57e8986d6d1152ab40ecedbcf0f335f314dca4e611151437613cdcf04d4eafdde7a1197e2858f4155db5423a7ffc6c21430574b82027471faa721608fec7fcbd8540883dea9b8dc49509bc8d289c42103cc67d9e3868c4664fb97477f66942256bacbdff4b9f91a4391a4f1c4bb8d9e23a0712cdcac5889e17411d10f9d8a57b6a4ec2b374fb7ab66c6c06e74f9f2950dc4ec16057b566553a237047a60aeb44b03d4a34b5cbac5f6e1b6452c0a45562546696b010bafbac82ba52c3156ff5191c97c409f1881bdbc8f936f9c86bafe36afb340f61d9e9ff5c910c67b38a502690aa77945fc715242247ff6d447ad2a8139b6f345e6c431022630bad40148de4993f4384e35d84e453dd809eecfe8cc9ab35a7e4ffcb46a452085c793fa8f4d38d7083fe89fa86fa60002803f071193ad8dde4e373fc4c099972ab32dfdb43560fd8d13d589430490ba07f1e13f39dc1af42f8a8b08be4e4fe94312b33ad3faf443fb05d486cb5053a7fd76fef3a1762c11e5a7f5eaa643f87f4effeb9bd227ef98ec64e09f7dd6c0822bc85dd7a1c6eb11f4c68e8137e2668402b4b4bb2110ad32f8b8a3ed24c4ea40ced8e6ef7cd69a92ec7f74a3cc624253669580e38d06caf33762038d7444e747a47463c49820c033c54e622b7faa463dbbec91d41e3198f6079d330e3c64806d6139d1a1f53cf59c4f9273c0b3054721e481dae65fae257b0ff255ec5c1d9878e32db2a1961e3c1296483f74c60ddc38d1782144721fc2c726dff0f88df74ad5b081d04197557e410d3bcf162cb7b0be6b73a0b8d13de3a07805548850df2f45c555e67b51c22d081c31960dfbca735c107fdcf5514e743cadea999f860676f9dfcaa64e115a8d52d01561875f56bc171eedd7ee07e12570d57537ee14de4014ff8d67e56ac8a56e620b15821ca6113723555e2e0bb70c68b4cf740cfd93be1a75afafc844556de341b101abff399eac90d6f078a0c6d157549d9de2a0c143f362525dbdb0f02dd49aa3cf301ed0f8e2a873b4d7d8e84557626a8d6cca5645b878f4cf1da0d70a369a3b7ab943e6a6\n  \n    \n      \n      \n        Input Key\n      \n    \n  \n\n"},{"title":"Linux基础","url":"/2022/10/29/Linux%E5%9F%BA%E7%A1%80/","content":"1. 引言Linux内核最初是由李纳斯·托瓦兹（Linus Torvalds）在赫尔辛基大学读书时出于个人爱好而编写的，当时他觉得教学用的迷你版UNIX操作系统Minix太难用了，于是决定自己开发一个操作系统。第一个版本于1991年9月发布，当时仅有10000行代码。\n李纳斯·托瓦兹没有保留Linux源代码的版权，公开了代码，并邀请他人一起完善Linux。与Windows及其他有专利权的操作系统不同，Linux开放源代码，任何人都可以免费使用它。\n据估计，现在只有2%的Linux核心代码是由李纳斯·托瓦兹自己编写的，虽然他仍然拥有Linux内核（操作系统的核心部分），并且保留了选择新代码和需要合并的新方法的最终裁定权。现在大家所使用的Linux，笔者更倾向于说是由李纳斯·托瓦兹和后来陆续加入的众多Linux爱好者共同开发完成的。\nLinux受到广大计算机爱好者的喜爱，主要原因有两个：一是它属于开源软件，用户不用支付任何费用就可以获得它和它的源代码，并且可以根据自己的需要对它进行必要的修改，无偿使用，无约束地继续传播；二是它具有UNIX的全部功能，任何使用UNIX操作系统或想要学习UNIX操作系统的人都可以从Linux中获益。\n开源软件是不同于商业软件的一种模式，从字面上理解，就是开放源代码，大家不用担心里面会搞什么猫腻，这会带来软件的革新和安全。\n另外，开源其实并不等同于免费，而是一种新的软件盈利模式。目前很多软件都是开源软件，对计算机行业与互联网影响深远。\nUNIX/Linux系统可以粗糙地抽象为3个层次（所谓粗糙，就是不够细致、精准，但是便于初学者抓住重点理解），如图1所示。底层是UNIX/Linux操作系统，一般称之为系统内核（Kernel）；中间层是Shell层，即命令解释层；高层则是应用层。\n\n    \n\n\n图1 UNIX/Linux系统结构层次概要\n\n1.1 内核层内核层是UNIX/Linux系统的核心和基础，它直接附着在硬件平台之上，控制和管理系统内各种资源（硬件资源和软件资源），有效地组织进程的运行，从而扩展硬件的功能，提高资源的利用效率，为用户提供方便、高效、安全、可靠的应用环境。\n1.2 Shell层Shell层是与用户直接交互的界面。用户可以在提示符下输入命令行，由Shell解释执行并输出相应结果或者有关信息，所以我们也把Shell称作命令解释器，利用系统提供的丰富命令可以快捷而简便地完成许多工作。\n1.3 应用层应用层提供基于X Window协议的图形环境。X Window协议定义了一个系统所必须具备的功能（就如同TCP/IP是一个协议，定义软件所应具备的功能），任何系统能满足此协议及符合X协会其他的规范，便可称为X Window。\n现在大多数的UNIX系统上（包括Solaris、HP-UX、AIX等）都可以运行CDE（Common Desktop Environment，通用桌面环境，是运行于UNIX的商业桌面环境）的用户界面；而在Linux上广泛应用的有Gnome、KDE等。\nX Window与微软的Windows图形环境有很大的区别：UNIX/Linux系统与X Window没有必然捆绑的关系，也就是说，UNIX/Linux可以安装X Window，也可以不安装；而微软的Windows图形环境与内核捆绑密切。UNIX/Linux系统不依赖图形环境，依然可以通过命令行完成100%的功能，而且因为不使用图形环境还会节省大量的系统资源。\n2. Linux常用命令2.1 命令的基本格式2.1.1 命令提示符登录系统后，第一眼看到的内容是：\n[root@localhost ~]#\n这就是Linux系统的命令提示符。\n\n●[]：这是提示符的分隔符号，没有特殊含义。●root：显示的是当前的登录用户，超哥现在使用的是root用户登录。●@：分隔符号，没有特殊含义。●localhost：当前系统的简写主机名（完整主机名是localhost.localdomain）。●~：代表用户当前所在的目录，此例中用户当前所在的目录是家目录。●#：命令提示符，Linux用这个符号标识登录的用户权限等级。如果是超级用户，提示符就是#；如果是普通用户，提示符就是$。家目录是什么？Linux系统是纯字符界面，用户登录后，要有一个初始登录的位置，这个初始登录位置就称为用户的家。●超级用户的home目录：/root/。●普通用户的home目录：/home/用户名/。\n\n用户在自己的home目录中拥有完整权限，现在切换一下用户所在目录，看看有什么效果。\n[root@localhost ~]# cd /usr/local/[root@localhost local]#\n如果切换用户所在目录，那么命令提示符中的“~”会变成用户当前所在目录的最后一个目录（不显示完整的所在目录/usr/local/，只显示最后一个目录local）。\n2.1.2命令的基本格式Linux命令的基本格式为：\n[root@localhost ~]# 命令 [选项] [参数]\n命令格式中的[]代表可选项，也就是有些命令可以不写选项或参数，也能执行。下面用Linux中最常见的ls命令来解释一下命令的格式。如果按照命令的分类，那么ls命令应该属于目录操作命令。\n[root@localhost ~]# lsanaconda-ks.cfg  install.log  install.log.syslog\n1）选项的作用ls命令之后不加选项和参数也能执行，不过只能执行最基本的功能，即显示当前目录下的文件名。那么加入一个选项，会出现什么结果？\n[root@localhost ~]# ls -l总用量 44-rw-------. 1 root root  1207 1月  14 18:18 anaconda-ks.cfg-rw-r--r--. 1 root root 24772 1月  14 18:17 install.log-rw-r--r--. 1 root root  7690 1月  14 18:17 install.log.syslog\n如果加一个“-l”选项，则可以看到显示的内容明显增多了。“-l”是长格式（long list）的意思，也就是显示文件的详细信息。至于“-l”选项的具体含义，我们稍后再详细讲解。可以看到选项的作用是调整命令功能。如果没有选项，那么命令只能执行最基本的功能；而一旦有选项，则可以显示更加丰富的数据。\nLinux的选项又分为短格式选项（-l）和长格式选项（—all）。短格式选项是英文的简写，一般用一个减号调用，例如：\n[root@localhost ~]# ls -l\n而长格式选项是英文完整单词，一般用两个减号调用，例如：\n[root@localhost ~]# ls --all\n一般情况下，短格式选项是长格式选项的缩写，也就是一个短格式选项会有对应的长格式选项。当然也有例外，比如ls命令的短格式选项-l就没有对应的长格式选项。所以具体的命令选项可以通过后面我们要学习的帮助命令来进行查询。\n2）参数的作用\n参数是命令的操作对象，一般文件、目录、用户和进程等可以作为参数被命令操作。例如：\n [root@localhost ~]# ls -l anaconda-ks.cfg-rw-------. 1 root root 1207 1月  14 18:18 anaconda-ks.cfg\n但是为什么一开始ls命令可以省略参数？那是因为有默认参数。命令一般都需要加入参数，用于指定命令操作的对象是谁。如果可以省略参数，则一般都有默认参数。例如：\nroot@localhost ~]# lsanaconda-ks.cfg  install.log  install.log.syslog\n这个ls命令后面没有指定参数，默认参数是当前所在位置，所以会显示当前目录下的文件名。\n\n总结一下：命令的选项用于调整命令功能，而命令的参数是这个命令的操作对象。\n\n2.2 目录操作命令2.2.1 ls命令ls是最常见的目录操作命令，主要作用是显示目录下的内容。这个命令的基本信息如下：\n\n●命令名称：ls。●英文原意：list。●所在路径：/bin/ls。●执行权限：所有用户。●功能描述：显示目录下的内容。\n\n1）命令格式\n[root@localhost ~]#ls [选项] [文件名或目录名]选项：    -a:             显示所有文件    --color=when:   支持颜色输出，when的值默认是always（总显示颜色），也可以是                    never（从不显示颜色）和auto（自动）    -d:            显示目录信息，而不是目录下的文件    -h:            人性化显示，按照我们习惯的单位显示文件大小    -i:            显示文件的i节点号    -l:            长格式显示\n2）常见用法\n“-l”选项\n[root@localhost ~]# ls -l总用量 44-rw-------. 1 root root  1207 1月  14 18:18 anaconda-ks.cfg-rw-r--r--. 1 root root 24772 1月  14 18:17 install.log-rw-r--r--. 1 root root  7690 1月  14 18:17 install.log.syslog#权限   引用计数所有者所属组大小文件修改时间  文件名\n我们已经知道“-l”选项用于显示文件的详细信息，那么“-l”选项显示的这7列分别是什么含义？\n\n●第一列：权限。具体权限的含义将在4.5节中讲解。●第二列：引用计数。文件的引用计数代表该文件的硬链接个数，而目录的引用计数代表该目录有多少个一级子目录。●第三列：所有者，也就是这个文件属于哪个用户。默认所有者是文件的建立用户●第四列：所属组。默认所属组是文件建立用户的有效组，一般情况下就是建立用户的所在组。●第五列：大小。默认单位是字节。●第六列：文件修改时间。文件状态修改时间或文件数据修改时间都会更改这个时间，注意这个时间不是文件的创建时间。●第七列：文件名。\n\n2.2.2 cd命令cd是切换所在目录的命令，这个命令的基本信息如下。\n\n●命令名称：cd。●英文原意：change directory。●所在路径：Shell内置命令。●执行权限：所有用户。●功能描述：切换所在目录。\n\nLinux的命令按照来源方式分为两种：Shell内置命令和外部命令。所谓Shell内置命令，就是Shell自带的命令，这些命令是没有执行文件的；而外部命令就是由程序员单独开发的，是外来命令，所以会有命令的执行文件。Linux中的绝大多数命令是外部命令，而cd命令是一个典型的Shell内置命令，所以cd命令没有执行文件所在路径。1）命令格式\n[root@localhost ~]#cd [目录名]\n2）常见用法\n例1：基本用法\ncd命令切换目录只需在命令后加目录名称即可。例如：\n[root@localhost ~]# cd /usr/local/src/[root@localhost src]##进入/usr/local/src/目录\n通过命令提示符，我们可以确定当前所在目录已经切换。\n例2：简化用法\ncd命令可以识别一些特殊符号，用于快速切换所在目录，这些符号如表1所示。\n表1 cd命令简化用法\n\n\n    \n\n\n“cd ~”命令可以快速回到用户的家目录。\n[root@localhost src]# cd ~[root@localhost ~]#\ncd命令直接按回车键也是快速切换到家目录\n[root@localhost ~]# cd /etc/[root@localhost etc]# cd[root@localhost ~]##直接使用cd命令，也回到了家目录\n再试试“cd -”命令。\n[root@localhost ~]# cd /usr/local/src/#进入/usr/local/src/目录    [root@localhost src]# cd -/root[root@localhost ~]##“cd –”命令回到进入src目录之前的家目录    [root@localhost ~]# cd -/usr/local/src[root@localhost src]##再执行一遍“cd –”命令，又回到了/usr/local/src/目录\n再来试试“.”和“..”。\n[root@localhost ~]# cd /usr/local/src/#进入测试目录    [root@localhost src]# cd ..#进入上级目录    [root@localhost local]# pwd/usr/local#pwd是查看当前所在目录的命令，可以看到我们进入了上级目录/usr/local/    [root@localhost local]# cd .#进入当前目录    [root@localhost local]# pwd/usr/local#这个命令不会有目录的改变，只是告诉大家“.”代表当前目录\n2.2.3 mkdir命令mkdir是创建目录的命令，其基本信息如下。\n\n●命令名称：mkdir。●英文原意：make directories。●所在路径：/bin/mkdir。●执行权限：所有用户。●功能描述：创建空目录。\n\n1）命令格式\n[root@localhost ~]# mkdir [选项] 目录名选项：    -p:    递归建立所需目录\nmkdir也是一个非常简单的命令，其主要作用就是新建一个空目录。\n2）常见用法\n例1：建立目录\n[root@localhost ~]#mkdir cangls[root@localhost ~]# lsanaconda-ks.cfg  cangls install.log  install.log.syslog\n我们建立一个名为cangls的目录，通过ls命令可以查看到这个目录已经建立。\n例2：递归建立目录\n如果想建立一串空目录：\n[root@localhost ~]# mkdir lm/movie/jp/canglsmkdir: 无法创建目录&quot;lm/movie/jp/cangls&quot;: 没有那个文件或目录\n这条命令报错，没有正确执行。这是因为这4个目录都是不存在的，mkdir默认只能在已经存在的目录中建立新目录。而如果需要建立一系列的新目录，则需要加入“-p”选项，递归建立才可以。例如：\n[root@localhost ~]# mkdir -p lm/movie/jp/cangls[root@localhost ~]# lsanaconda-ks.cfg  cangls  install.log  install.log.syslog  lm[root@localhost ~]# ls lm/movie#这里只查看一级子目录，其实后续的jp目录、cangls目录都已经建立\n所谓的递归建立，就是一级一级地建立目录。\n2.2.4 rmdir命令既然有建立目录的命令，就一定会有删除目录的命令rmdir，其基本信息如下。\n\n●命令名称：rmdir。●英文原意：remove empty directories。●所在路径：/bin/rmdir。●执行权限：所有用户。●功能描述：删除空目录。\n\n1）命令格式\n[root@localhost ~]# rmdir [选项] 目录名选项：    -p:    递归删除目录\n2）常见用法\n[root@localhost ~]#rmdir cangls\n就这么简单，命令后面加目录名称即可。既然可以递归建立目录，当然也可以递归删除目录。例如：\n[root@localhost ~]# rmdir -p lm/movie/jp/cangls/\n但rmdir命令的作用十分有限，因为只能删除空目录，所以一旦目录中有内容，就会报错。例如：\n[root@localhost ~]# mkdir test#建立测试目录    [root@localhost ~]# touch test/boduo[root@localhost ~]# touch test/longze#在测试目录中建立两个文件    [root@localhost ~]# rmdir test/rmdir: 删除 &quot;test/&quot; 失败： 目录非空\n这个命令比较“笨”，所以我们不太常用。后续我们不论删除的是文件还是目录，都会使用rm命令。\n2.3 文件操作命令2.3.1 touch命令touch的意思是触摸，如果文件不存在，则会建立空文件；如果文件已经存在，则会修改文件的时间戳（访问时间、数据修改时间、状态修改时间都会改变）。千万不要把touch命令当成新建文件的命令，牢牢记住这是触摸的意思。这个命令的基本信息如下。\n\n●命令名称：touch。●英文原意：change file timestamps。●所在路径：/bin/touch。●执行权限：所有用户。●功能描述：修改文件的时间戳。\n\n1）命令格式\n[root@localhost ~]# touch [选项] 文件名或目录名选项：    -a:    只修改文件的访问时间（Access Time）    -c:    如果文件不存在，则不建立新文件    -d:    把文件的时间改为指定的时间    -m:    只修改文件的数据修改时间（Modify Time）\n2）常见用法\n[root@localhost ~]#touch bols#建立名为bols的空文件\n如果文件不存在，则会建立文件。\n[root@localhost ~]#touch bols[root@localhost ~]#touch bols#而如果文件已经存在，则也不会报错，只是会修改文件的访问时间\n2.3.2 cat命令cat命令用看来查看文件内容，这个命令地基本信息如下。\n\n●命令名称：cat。●英文原意：concatenate files and print on the standard output。●所在路径：/bin/cat。●执行权限：所有用户。●功能描述：合并文件并打印输出到标准输出。\n\n1）命令格式\n[root@localhost ~]# cat [选项] 文件名选项：    -A： 相当于-vET选项的整合，用于列出所有隐藏符号    -E： 列出每行结尾的回车符$    -n： 显示行号    -T： 把Tab键用^I显示出来    -v： 列出特殊字符\n2）常见用法\ncat命令用于查看文件内容，不论文件内容有多少，都会一次性显示。如果文件非常大，那么文件开头的内容就看不到了。不过Linux可以使用“PgUp+上箭头”向上翻页，但是这种翻页是有极限的，如果文件足够长，那么还是无法看全文件的内容。所以cat命令适合查看不太大的文件。当然，在Linux中是可以使用其他的命令或方法来查看大文件的，我们以后再来学习。cat命令本身非常简单，我们可以直接查看文件的内容。例如：\n[root@localhost ~]# cat anaconda-ks.cfg# Kickstart file automatically generated by anaconda.#version=DEVELinstallcdromlang zh_CN.UTF-8…省略部分内容…\n而如果使用“-n”选项，则会显示行号。例如：\n[root@localhost ~]# cat -n anaconda-ks.cfg    1  # Kickstart file automatically generated by anaconda.    2    3    4  #version=DEVEL    5  install    6  cdrom…省略部分内容…\n如果使用“-A”选项，则相当于使用了“-vET”选项，可以查看文本中的所有隐藏符号，包括回车符（$）、Tab键（^I）等。例如：\n[root@localhost ~]# cat -A anaconda-ks.cfg# Kickstart file automatically generated by anaconda.$$$#version=DEVEL$install$cdrom$…省略部分内容…\n2.4 目录和文件都能操作的命令2.4.1 rm命令rm是强大的删除命令，不仅可以删除文件，也可以删除目录。这个命令的基本信息如下。\n\n●命令名称：rm。●英文原意：remove files or directories。●所在路径：/bin/rm。●执行权限：所有用户。●功能描述：删除文件或目录。\n\n1）命令格式\n[root@localhost ~]# rm [选项] 文件或目录选项：    -f:    强制删除（force）    -i:    交互删除，在删除之前会询问用户    -r:    递归删除，可以删除目录（recursive）\n2）常见用法\n例1：基本用法\nrm命令如果任何选项都不加，则默认执行的是“rm -i文件名”，也就是在删除一个文件之前会先询问是否删除。例如：\n[root@localhost ~]# touch cangls[root@localhost ~]# rm canglsrm：是否删除普通空文件 &quot;cangls&quot;? y#删除前会询问是否删除\n例2：删除目录\n如果需要删除目录，则需要使用“-r”选项。例如：\n[root@localhost ~]# mkdir -p /test/lm/movie/jp/#递归建立测试目录    [root@localhost ~]# rm /test/rm: 无法删除&quot;/test/&quot;: 是一个目录#如果不加“-r”选项，则会报错    [root@localhost ~]# rm -r /test/rm：是否进入目录&quot;/test&quot;? yrm：是否进入目录&quot;/test/lm&quot;? yrm：是否进入目录&quot;/test/lm/movie&quot;? yrm：是否删除目录 &quot;/test/lm/movie/jp&quot;? yrm：是否删除目录 &quot;/test/lm/movie&quot;? yrm：是否删除目录 &quot;/test/lm&quot;? yrm：是否删除目录 &quot;/test&quot;? y#会分别询问是否进入子目录、是否删除子目录\n大家会发现，如果每级目录和每个文件都需要确认，那么在实际使用中简直是灾难！\n例3：强制删除\n如果要删除的目录中有1万个子目录或子文件，那么普通的rm删除最少需要确认1万次。所以，在真正删除文件的时候，我们会选择强制删除。例如：\n[root@localhost ~]# mkdir -p /test/lm/movie/jp/#重新建立测试目录    [root@localhost ~]# rm -rf /test/#强制删除\n加入了强制功能之后，删除就会变得很简单，但是需要注意：\n\n●数据强制删除之后无法恢复，除非依赖第三方的数据恢复工具，如extundelete等。但要注意，数据恢复很难恢复完整的数据，一般能恢复70%～80%就很难得了。所以，与其把宝压在数据恢复上，不如养成良好的操作习惯。\n●虽然“-rf”选项是用来删除目录的，但是删除文件也不会报错。所以，为了使用方便，一般不论是删除文件还是删除目录，都会直接使用“-rf”选项。\n\n2.4.2 cp命令cp是用于复制的命令，其基本信息如下：\n\n●命令名称：cp。●英文原意：copy files and directories。●所在路径：/bin/cp。●执行权限：所有用户。●功能描述：复制文件和目录。\n\n1）命令格式\n[root@localhost ~]# cp [选项] 源文件 目标文件选项：    -a:    相当于-dpr选项的集合    -d:    如果源文件为软链接（对硬链接无效），则复制出的目标文件也为软链接    -i:    询问，如果目标文件已经存在，则会询问是否覆盖    -l:    把目标文件建立为源文件的硬链接文件，而不是复制源文件    -s:    把目标文件建立为源文件的软链接文件，而不是复制源文件    -p:    复制后目标文件保留源文件的属性（包括所有者、所属组、权限和时间）    -r:    递归复制，用于复制目录\n2）常见用法\n例：基本用法\ncp命令既可以复制文件，也可以复制目录。我们先来看看如何复制文件，例如：\n[root@localhost ~]# touch cangls#建立源文件    [root@localhost ~]# cp cangls  /tmp/#把源文件不改名复制到/tmp/目录下\n如果需要改名复制，则命令如下：\n[root@localhost ~]# cp cangls  /tmp/bols#改名复制\n如果复制的目标位置已经存在同名的文件，则会提示是否覆盖，因为cp命令默认执行的是“cp -i”的别名，例如：\n[root@localhost ~]# cp cangls  /tmp/cp：是否覆盖&quot;/tmp/cangls&quot;? y#目标位置有同名文件，所以会提示是否覆盖\n接下来我们看看如何复制目录，其实复制目录只需使用“-r”选项即可，例如：\n[root@localhost ~]# mkdir movie#建立测试目录    [root@localhost ~]# cp -r /root/movie/ /tmp/#目录原名复制\n2.4.3 mv命令mv是用来剪切的命令，其基本信息如下。\n\n●命令名称：mv。●英文原意：move (rename) files。●所在路径：/bin/mv。●执行权限：所有用户。●功能描述：移动文件或改名。\n\n1）命令格式\n[root@localhost ~]# mv [选项] 源文件 目标文件选项：    -f:    强制覆盖，如果目标文件已经存在，则不询问，直接强制覆盖    -i:    交互移动，如果目标文件已经存在，则询问用户是否覆盖（默认选项）    -n:    如果目标文件已经存在，则不会覆盖移动，而且不询问用户    -v:    显示详细信息\n2）常见用法例1：移动文件或目录\n[root@localhost ~]# mv cangls /tmp/#移动之后，源文件会被删除，类似剪切[root@localhost ~]# mkdir movie[root@localhost ~]# mv movie/ /tmp/#也可以移动目录。和rm、cp不同的是，mv移动目录不需要加入“-r”选项\n如果移动的目标位置已经存在同名的文件，则同样会提示是否覆盖，因为mv命令默认执行的也是“mv -i”的别名，例如：\n[root@localhost ~]# touch cangls#重新建立文件    [root@localhost ~]# mv cangls /tmp/mv：是否覆盖&quot;/tmp/cangls&quot;? y#由于/tmp/目录下已经存在cangls文件，所以会提示是否覆盖，需要手工输入y覆盖移动\n例2：强制移动\n之前说过，如果目标目录下已经存在同名文件，则会提示是否覆盖，需要手工确认。这时如果移动的同名文件较多，则需要一个一个文件进行确认，很不方便。如果我们确认需要覆盖已经存在的同名文件，则可以使用“-f”选项进行强制移动，这就不再需要用户手工确认了。例如：\n[root@localhost ~]# touch cangls#重新建立文件[root@localhost ~]# mv -f cangls /tmp/#就算/tmp/目录下已经存在同名的文件，由于“-f”选项的作用，所以会强制覆盖\n例3：不覆盖移动\n既然可以强制覆盖移动，那也有可能需要不覆盖的移动。如果需要移动几百个同名文件，但是不想覆盖，这时就需要“-n”选项的帮助了。例如：\n[root@localhost ~]# ls /tmp/*ls/tmp/bols  /tmp/cangls#在/tmp/目录下已经存在bols、cangls文件了[root@localhost ~]# mv -vn bols cangls lmls  /tmp/&quot;lmls&quot; -&gt; &quot;/tmp/lmls&quot;#再向/tmp/目录中移动同名文件，如果使用了“-n”选项，则可以看到只移动了lmls，而同名的bols    和cangls并没有移动（“-v”选项用于显示移动过程）\n例4：改名\n如果源文件和目标文件在同一个目录中，那就是改名。例如：\n[root@localhost ~]# mv bols lmls#把bols改名为lmls\n目录也可以按照同样的方法改名。\n例5：显示移动过程\n如果我们想要知道在移动过程中到底有哪些文件进行了移动，则可以使用“-v”选项来查看详细的移动信息。例如：\n[root@localhost ~]# touch test1.txt test2.txt test3.txt#建立三个测试文件    [root@localhost ~]# mv -v *.txt /tmp/&quot;test1.txt&quot; -&gt; &quot;/tmp/test1.txt&quot;&quot;test2.txt&quot; -&gt; &quot;/tmp/test2.txt&quot;&quot;test3.txt&quot; -&gt; &quot;/tmp/test3.txt&quot;#加入“-v”选项，可以看到有哪些文件进行了移动\n2.5 帮助命令2.5.1 man命令man是最常见的帮助命令，也是Linux最主要的帮助命令，其基本信息如下。\n\n●命令名称：man。●英文原意：format and display the on-line manual pages。●所在路径：/usr/bin/man。●执行权限：所有用户。●功能描述：显示联机帮助手册。\n\n1）命令格式\n[root@localhost ~]# man [选项] 命令选项：    -f：    查看命令拥有哪个级别的帮助    -k：    查看和命令相关的所有帮助\nman命令比较简单，下面举个例子：\n[root@localhost ~]# man ls#获取ls命令的帮助信息\n这就是man命令的基本使用方法，非常简单。但是帮助命令的重点不是命令如何使用，而是帮助信息应该如何查询。这些信息较多，下面将详细讲解。2）man命令的使用方法\n还是查看ls命令的帮助，我们看看这个帮助信息的详细内容。\n[root@localhost ~]# man lsLS(1)                        User Commands                       LS(1)NAME      ls - list directory contents      #命令名称及英文原意   SYNOPSISls [OPTION]... [FILE]...      #命令的格式    DESCRIPTION#开始详细介绍命令选项的作用    \t  List  information  about  the  FILEs  (the current directory by default).  Sortentries alphabetically if none of -cftuvSUX nor --sort.      Mandatory arguments to long options are mandatory for short options too.      -a, --all\t\t\t do not ignore entries starting with .      -A, --almost-all\t\t     do not list implied . and ..…省略部分内容…AUTHORWritten by Richard M. Stallman and David MacKenzie.      #作者REPORTING BUGS#bug的报告地址 Report ls bugs to bug-coreutils@gnu.org      GNU coreutils home page: &lt;http://www.gnu.org/software/coreutils/&gt;      General help using GNU software: &lt;http://www.gnu.org/gethelp/&gt;      Report ls translation bugs to http://translationproject.org/team/COPYRIGHT#著作权受GPL规则保护      Copyright  2010 Free Software Foundation, Inc.  License GPLv3+: GNU GPLversion  3  or later&lt;http://gnu.org/licenses/gpl.html&gt;.      This  is  free  software: you are free to change and redistribute it.  There is NOWARRANTY, to the extent permitted by law.SEE ALSO#可以通过其他哪些命令查看到ls的相关信息      The full documentation for ls is maintained as a Texinfo manual.  If the  infoand  ls programs are properly installed at your site, the command info coreutils &#x27;ls invocation&#x27; should give you access to the complete manual.GNU coreutils 8.4                June 2012                         LS(1)\n虽然不同命令的man信息有一些区别，但是每个命令man信息的整体结构皆如演示这样。在帮助信息中，我们主要查看的就是命令的格式和选项的详细作用。\n不过大家请注意，在man信息的最后，可以看到还有哪些命令可以查看到此命令的相关信息。这是非常重要的提示，不同的帮助信息记录的侧重点是不太一样的。所以，如果在man信息中找不到想要的内容，则可以尝试查看其他相关帮助命令。\n3）man命令的快捷键man命令的快捷键可以参考表2。\n表2 man命令的快捷键\n\n\n    \n\n\n2.5.2 info命令info命令也可以获取命令的帮助。和man命令不同的是，info命令的帮助信息是一套完整的资料，每个单独命令的帮助信息只是这套完整资料中的某一个小章节。大家可以把info帮助信息看成一部独立的电子书，所以每个命令的帮助信息都会和书籍一样，拥有章节编号。例如：\n[root@localhost ~]# info lsFile: coreutils.info,  Node: ls invocation,  Next: dir invocation,  Up: Directorylisting10.1 &#x27;ls&#x27;: List directory contents==================================The &#x27;ls&#x27; program lists information about files (of any type, includingdirectories).  Options and file arguments can be intermixedarbitrarily, as usual.…省略部分内容…\n可以看到，ls命令的帮助只是整个info帮助信息中的第10.1节。在这个帮助信息中，如果标题的前面有“*”符号，则代表这是一个可以进入查看详细信息的子页面，只要按下回车键就可以进入。例如：\n[root@localhost ~]# info ls…省略部分内容…    Also see *note Common options::.  * Menu:  * Which files are listed::  * What information is listed::  * Sorting the output::  * Details about version sort::  * General output formatting::  * Formatting file timestamps::  * Formatting the file names::    …省略部分内容…\n这是ls命令的info帮助信息中可以查看详细的子页面的标题。info命令主要是靠快捷键来进行操作的，我们来看看常用的快捷键，如表3所示。\n表3 info命令的常用快捷键\n\n\n    \n\n\n2.6 压缩和解压缩命令在系统中，如果需要有大量的文件进行复制和保存，那么把它们打成压缩包是不错的选择。打包压缩作为常规操作，在Windows和Linux中都比较常见。Windows中常见的压缩包格式主要有“.zip”“.rar”和“.7z”等。、\n在Linux中也是一样的，可以识别的常见压缩格式有十几种，比如“.zip”“.gz”“.bz2”“.tar”“.tar.gz”“.tar.bz2”等。我们也不需要知道这些压缩格式的具体区别，只要对应的压缩包会解压缩、想要压缩的时候会操作即可。\n为什么还要区分是“.gz”还是“.bz2”的扩展名呢？这是因为，在Linux中，不同的压缩方法对应的解压缩方法也是不同的，这里的扩展名并不是Linux系统一定需要的（Linux不区分扩展名），而是用来给用户标识压缩格式的。只有知道了正确的压缩格式，才能采用正确的解压缩命令。\n2.6.1 “zip”格式“.zip”是Windows中最常用的压缩格式，Linux也可以正确识别“.zip”格式，这可以方便地和Windows系统通用压缩文件。1）“.zip”格式的压缩命令压缩命令就是zip，其基本信息如下。\n\n●命令名称：zip。●英文原意：package and compress (archive) files。●所在路径：/usr/bin/zip。●执行权限：所有用户。●功能描述：压缩文件或目录。\n\n命令格式如下：\n[root@localhost ~]# zip [选项] 压缩包名 源文件或源目录选项：    -r:    压缩目录\nzip压缩命令需要手工指定压缩之后的压缩包名，注意写清楚扩展名，以方便解压缩时使用。举个例子：\n[root@localhost ~]# zip ana.zip anaconda-ks.cfgadding: anaconda-ks.cfg (deflated 37%)#压缩    [root@localhost ~]# ll ana.zip-rw-r--r-- 1 root root 935 6月  17 16:00 ana.zip#压缩文件生成\n所有的压缩命令都可以同时压缩多个文件，例如：\n[root@localhost ~]# zip test.zip install.log install.log.syslogadding: install.log (deflated 72%)adding: install.log.syslog (deflated 85%)#同时压缩多个文件到test.zip压缩包中    [root@localhost ~]# ll test.zip-rw-r--r-- 1 root root 8368 6月  17 16:03 test.zip#压缩文件生成\n如果想要压缩目录，则需要使用“-r”选项，例如：\n[root@localhost ~]# mkdir dir1#建立测试目录    [root@localhost ~]# zip -r dir1.zip dir1adding: dir1/ (stored 0%)#压缩目录    [root@localhost ~]# ls -dl dir1.zip-rw-r--r-- 1 root root 160 6月  17 16:22 dir1.zip#压缩文件生成\n2）“.zip”格式的解压缩命令“.zip”格式的解压缩命令是unzip，其基本信息如下。\n\n●命令名称：unzip。●英文原意：list, test and extract compressed files in a ZIP archive。●所在路径：/usr/bin/unzip。●执行权限：所有用户。●功能描述：列表、测试和提取压缩文件中的文件。\n\n命令格式如下：\n[root@localhost ~]# unzip [选项] 压缩包名选项：    -d:    指定解压缩位置\n不论是文件压缩包，还是目录压缩包，都可以直接解压缩，例如：\n[root@localhost ~]# unzip dir1.zipArchive:  dir1.zipcreating: dir1/#解压缩\n也可以手工指定解压缩位置，例如：\n[root@localhost ~]# unzip -d /tmp/ ana.zipArchive:  ana.zipinflating: /tmp/anaconda-ks.cfg#把压缩包解压到指定位置\n2.6.2 “.gz”格式1）“.gz”格式的压缩命令“.gz”格式是Linux中最常用的压缩格式，使用gzip命令进行压缩，其基本信息如下。\n\n●命令名称：gzip。●英文原意：compress or expand files。●所在路径：/bin/gzip。●执行权限：所有用户。●功能描述：压缩文件或目录。\n\n这个命令的格式如下：\n[root@localhost ~]# gzip [选项] 源文件选项：    -c:    将压缩数据输出到标准输出中，可以用于保留源文件    -d:    解压缩    -r:    压缩目录    -v:    显示压缩文件的信息    -数字：  用于指定压缩等级，-1压缩等级最低，压缩比最差；-9压缩比最高。默认压缩比是-6\n例1：基本压缩\ngzip压缩命令非常简单，甚至不需要指定压缩之后的压缩包名，只需指定源文件名即可：\n[root@localhost ~]# gzip install.log#压缩install.log文件    [root@localhost ~]# lsanaconda-ks.cfg  install.log.gz  install.log.syslog#压缩文件生成，但是源文件也消失了\n例2：保留源文件压缩\n在使用gzip命令压缩文件时，源文件会消失，从而生成压缩文件。\n[root@localhost ~]# gzip -c anaconda-ks.cfg anaconda-ks.cfg.gz#使用-c选项，但是不让压缩数据输出到屏幕上，而是重定向到压缩文件中    #这样可以在压缩文件的同时不删除源文件    [root@localhost ~]# lsanaconda-ks.cfg  anaconda-ks.cfg.gz  install.log.gz  install.log.syslog#可以看到压缩文件和源文件都存在\n例子3：压缩目录\n我们可能会想当然地认为gzip命令可以压缩目录：\n[root@localhost ~]# mkdir test[root@localhost ~]# touch test/test1[root@localhost ~]# touch test/test2[root@localhost ~]# touch test/test3#建立测试目录，并在里面建立几个测试文件    [root@localhost ~]# gzip -r test/#压缩目录，并没有报错    [root@localhost ~]# lsanaconda-ks.cfg  anaconda-ks.cfg.gz  install.log.gz  install.log.syslog  test#但是查看发现test目录依然存在，并没有变为压缩文件    [root@localhost ~]# ls test/test1.gz  test2.gz  test3.gz#原来gzip命令不会打包目录，而是把目录下所有的子文件分别压缩\n在Linux中，打包和压缩是分开处理的。而gzip命令只会压缩，不能打包，所以才会出现没有打包目录，而只把目录下的文件进行压缩的情况。\n2）“.gz”格式的解压缩命令如果要解压缩“.gz”格式，那么使用“gzip -d压缩包”和“gunzip压缩包”命令都可以。我们先看看gunzip命令的基本信息。\n\n●命令名称：gunzip。●英文原意：compress or expand files。●所在路径：/bin/gunzip。●执行权限：所有用户。●功能描述：解压缩文件或目录。\n\n常规用法就是直接解压缩文件，例如：\n[root@localhost ~]# gunzip install.log.gz\n如果要解压缩目录下的内容，则依然使用“-r”选项，例如：\n[root@localhost ~]# gunzip -r test/\n当然，“gunzip -r”依然只会解压缩目录下的文件，而不会解打包。要想解压缩“.gz”格式，还可以使用“gzip -d”命令，例如：\n[root@localhost ~]# gzip -d anaconda-ks.cfg.gz\n3）查看“.gz”格式压缩的文本文件内容如果我们压缩的是一个纯文本文件，则可以直接使用zcat命令在不解压缩的情况下查看这个文本文件中的内容。例如：\n[root@localhost ~]# zcat anaconda-ks.cfg.gz\n2.6.3 “.bz2”格式1）“.bz2”格式的压缩命令“.bz2”格式是Linux的另一种压缩格式，从理论上来讲，“.bz2”格式的算法更先进、压缩比更好；而“.gz”格式相对来讲压缩的时间更快。“.bz2”格式的压缩命令是bzip2，这个命令的基本信息为：\n\n●命令名称：bzip2。●英文原意：a block-sorting file compressor。●所在路径：/usr/bin/bzip2。●执行权限：所有用户。●功能描述：.bz2格式的压缩命令。\n\n来看看bzip2命令的格式。\n[root@localhost ~]# bzip2 [选项] 源文件选项：    -d:    解压缩    -k:    压缩时，保留源文件    -v:    显示压缩的详细信息    -数字：  这个参数和gzip命令的作用一样，用于指定压缩等级，-1压缩等级最低，压缩比最差；-9压缩比最高\n值得注意的是，gzip只是不会打包目录，但是如果使用“-r”选项，则可以分别压缩目录下的每个文件；而bzip2命令则根本不支持压缩目录，也没有“-r”选项。\n例1：基本压缩命令\n在压缩文件命令后面直接指定源文件即可，例如：\n[root@localhost ~]# bzip2 anaconda-ks.cfg#压缩成“.bz2”格式\n这个压缩命令依然会在压缩的同时删除源文件。\n例2：压缩的同时保留源文件\nbzip2命令可以直接使用“-k”选项来保留源文件，而不用像gzip命令一样使用输出重定向来保留源文件。例如：\n[root@localhost ~]# bzip2 -k install.log.syslog#压缩    [root@localhost ~]# lsanaconda-ks.cfg.bz2install.loginstall.log.syslog  install.log.syslog.bz2    #压缩文件和源文件都存在\n2）“.bz2”格式的解压缩命令\n“.bz2”格式可以使用“bzip2 -d压缩包”命令来进行解压缩，也可以使用“bunzip2压缩包”命令来进行解压缩。先看看bunzip2命令的基本信息。\n\n●命令名称：bunzip2。●英文原意：a block-sorting file compressor。●所在路径：/usr/bin/bunzip2。●执行权限：所有用户。●功能描述：.bz2格式的解压缩命令。\n\n命令格式如下：\n[root@localhost ~]# bunzip2 [选项] 源文件选项：    -k:    解压缩时，保留源文件\n先试试使用gunzip2命令来进行解压缩，例如：\n[root@localhost ~]# bunzip2 anaconda-ks.cfg.bz2\n“.bz2”格式也可以使用“bzip2 -d压缩包”命令来进行解压缩，例如：\n[root@localhost ~]# bzip2 -d install.log.syslog.bz2\n3）查看“.bz2”格式压缩的文本文件内容\n和“.gz”格式一样，“.bz2”格式压缩的纯文本文件也可以不解压缩直接查看，使用的命令是bzcat。例如：\n[root@localhost ~]# bzcat install.log.syslog.bz2\n2.6.4 “.tar”格式通过前面的学习，我们发现，不论是gzip命令还是bzip2命令，好像都比较笨，gzip命令不能打包目录，而只能单独压缩目录下的子文件；bzip2命令干脆就不支持目录的压缩。\n在Linux中，对打包和压缩是区别对待的。也就是说，在Linux中，如果想把多个文件或目录打包到一个文件包中，则使用的是tar命令；而压缩才使用gzip或bzip2命令。\n1）“.tar”格式的打包命令“.tar”格式的打包和解打包都使用tar命令，区别只是选项不同。我们先看看tar命令的基本信息。\n\n●命令名称：tar。●英文原意：tar。●所在路径：/bin/tar。●执行权限：所有用户。●功能描述：打包与解打包命令。\n\n命令的基本格式如下：\n[root@localhost ~]# tar [选项] [-f 压缩包名] 源文件或目录选项：    -c:    打包    -f:    指定压缩包的文件名。压缩包的扩展名是用来给管理员识别格式的，所以一定            要正确指定扩展名    -v:    显示打包文件过程\n例1：基本使用\n现在先打包一个文件练练手。\n[root@localhost ~]# tar -cvf anaconda-ks.cfg.tar anaconda-ks.cfg#把anaconda-ks.cfg打包为anaconda-ks.cfg.tar文件\n选项“-cvf”一般是习惯用法，记住打包时需要指定打包之后的文件名，而且要用“.tar”作为扩展名。那打包目录呢？我们也试试：\n[root@localhost ~]# ll -d test/drwxr-xr-x 2 root root 4096 6月  17 21:09 test/#test是我们之前的测试目录    [root@localhost ~]# tar -cvf test.tar test/test/test/test3test/test2test/test1#把目录打包为test.tar文件\ntar命令也可以打包多个文件或目录，只要用空格分开即可。例如：\n[root@localhost ~]# tar -cvf ana.tar anaconda-ks.cfg /tmp/#把anaconda-ks.cfg文件和/tmp目录打包成ana.tar文件包\n例2：打包压缩目录\n我们已经解释过了，压缩命令不能直接压缩目录，我们就先用tar命令把目录打成数据包，然后再用gzip命令或bzip2命令压缩。例如：\n[root@localhost ~]# ll -d test test.tardrwxr-xr-x 2 root root  4096 6月  17 21:09 test-rw-r--r-- 1 root root 10240 6月  18 01:06 test.tar#我们之前已经把test目录打包成test.tar文件    [root@localhost ~]# gzip  test.tar[root@localhost ~]# ll test.tar.gz-rw-r--r-- 1 root root 176 6月  18 01:06 test.tar.gz#gzip命令会把test.tar压缩成test.tar.gz    [root@localhost ~]# gzip -d test.tar.gz#解压缩，把test.tar.gz解压缩为test.tar    [root@localhost ~]# bzip2 test.tar[root@localhost ~]# ll test.tar.bz2-rw-r--r-- 1 root root 164 6月  18 01:06 test.tar.bz2#bzip2命令会把test.tar压缩为test.tar.bz2格式\n2）“.tar”格式的解打包命令\n“.tar”格式的解打包也需要使用tar命令，但是选项不太一样。命令格式如下：\n[root@localhost ~]# tar [选项] 压缩包选项：    -x:        解打包    -f:        指定压缩包的文件名    -v:        显示打包文件过程    -t:        测试，就是不解打包，只是查看包中有哪些文件    -C 目录：    指定解打包位置\n其实解打包和打包相比，只是把打包选项“-cvf”更换为“-xvf”。我们来试试：\n[root@localhost ~]# tar -xvf anaconda-ks.cfg.tar#解打包到当前目录下\n如果使用“-xvf”选项，则会把包中的文件解压到当前目录下。如果想要指定解压位置，则需要使用“-C（大写）”选项。例如：\n[root@localhost ~]# tar -xvf test.tar -C /tmp#把文件包test.tar解打包到/tmp/目录下\n如果只想查看文件包中有哪些文件，则可以把解打包选项“-x”更换为测试选项“-t”。例如：\n[root@localhost ~]# tar -tvf test.tardrwxr-xr-x root/root        02016-06-17 21:09 test/-rw-r--r-- root/root        02016-06-17 17:51 test/test3-rw-r--r-- root/root        02016-06-17 17:51 test/test2-rw-r--r-- root/root        02016-06-17 17:51 test/test1#会用长格式显示test.tar文件包中文件的详细信息\n2.6.5 “.tar.gz”和“.tar.bz2”格式你可能会觉得Linux实在太不智能了，一个打包压缩，居然还要先打包成“.tar”格式，再压成“.tar.gz”或“.tar.bz2”格式。其实tar命令是可以同时打包压缩的，前面的讲解之所以把打包和压缩分开，是为了让大家了解在Linux中打包和压缩的不同。使用tar命令直接打包压缩。命令格式如下：\n[root@localhost ~]# tar [选项] 压缩包 源文件或目录选项：    -z:    压缩和解压缩“.tar.gz”格式    -j:    压缩和解压缩“.tar.bz2”格式\n例1：压缩与解压缩“.tar.gz”格式\n我们先来看看如何压缩“.tar.gz”格式。\n[root@localhost ~]# tar -zcvf tmp.tar.gz /tmp/#把/tmp/目录直接打包压缩为“.tar.gz”格式，通过“-z”来识别格式，“-cvf”和打包选项一致\n解压缩也只是在解打包选项“-xvf”前面加了一个“-z”选项。\n[root@localhost ~]# tar -zxvf tmp.tar.gz#解压缩与解打包“.tar.gz”格式\n前面讲的选项“-C”用于指定解压位置、“-t”用于查看压缩包内容，在这里同样适用。\n例2：压缩与解压缩“.tar.bz2”格式\n和“.tar.gz”格式唯一的不同就是“-zcvf”选项换成了“-jcvf”。\n[root@localhost ~]# tar -jcvf tmp.tar.bz2 /tmp/#打包压缩为“.tar.bz2”格式，注意压缩包文件名    [root@localhost ~]# tar -jxvf tmp.tar.bz2#解压缩与解打包“.tar.bz2”格式\n把文件直接压缩成“.tar.gz”和“.tar.bz2”格式，才是Linux中最常用的压缩方式，这是一定要掌握的压缩和解压缩方法。\n2.7 常用网络命令2.7.1 ifconfig命令ifconfig是Linux中查看和临时修改IP地址的命令，其基本信息如下。\n\n●命令名称：ifconfig。●英文原意：configure a network interface。●所在路径：/sbin/ifconfig。●执行权限：超级用户。●功能描述：配置网络接口。\n\n1）查看IP地址信息\nifconfig命令最主要的作用就是查看IP地址的信息，直接输入ifconfig命令即可。\n[root@localhost ~]# ifconfigeth0     Link encap:Ethernet  HWaddr 00:0C:29:C5:FB:AA#eth0网卡信息       网络类型为以太网      MAC地址    inet addr:192.168.44.3  Bcast:192.168.44.255  Mask:255.255.255.0        #IP地址                广播地址             子网掩码    inet6 addr: fe80::20c:29ff:fec5:fbaa/64 Scope:Link        #IPv6的地址（目前不生效）            UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1        #网络参数           最大传输单元 数据包转送次数     RX packets:881 errors:0 dropped:0 overruns:0 frame:0        #接收到的数据包情况            TX packets:853 errors:0 dropped:0 overruns:0 carrier:0        #发送的数据包情况    collisions:0 txqueuelen:1000        #数据包碰撞    数据缓冲区长度            RX bytes:82229 (80.3 KiB)  TX bytes:273463 (267.0 KiB)        #接收包的大小                   发送包的大小            Interrupt:19 Base address:0x2000        #IRQ中断          内存地址    lo       Link encap:Local Loopback        #本地回环网卡信息    inet addr:127.0.0.1  Mask:255.0.0.0inet6 addr: ::1/128 Scope:Host        UP LOOPBACK RUNNING  MTU:16436  Metric:1        RX packets:12 errors:0 dropped:0 overruns:0 frame:0        TX packets:12 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0        RX bytes:840 (840.0 b)  TX bytes:840 (840.0 b)\nifconfig命令主要用于查看IP地址、子网掩码和MAC地址这三类信息，其他信息我们有所了解即可。\nlo网卡是Loopback的缩写，也就是本地回环网卡，这个网卡的IP地址是127.0.0.1。它只代表我们的网络协议正常，就算不插入网线也可以ping通，所以基本没有实际使用价值，了解即可。\n2.7.2 ping命令ping是常用的网络命令，主要通过ICMP协议进行网络探测，测试网络中主机的通信情况。ping命令的基本信息如下。\n\n●命令名称：ping。●英文原意：send ICMP ECHO_REQUEST to network hosts。●所在路径：/bin/ping。●执行权限：所有用户。●功能描述：向网络主机发送ICMP请求。\n\n命令的基本格式如下：\n[root@localhost ~]# ping [选项] IP选项：    -b:        后面加入广播地址，用于对整个网段进行探测    -c 次数：    用于指定ping的次数    -s 字节：    指定探测包的大小\n例1：探测与指定主机通信\n[root@localhost ~]# ping 192.168.103.151PING 192.168.103.151 (192.168.103.151) 56(84) bytes of data.64 bytes from 192.168.103.151: icmp_seq=1 ttl=128 time=0.300 ms64 bytes from 192.168.103.151: icmp_seq=2 ttl=128 time=0.481 ms…省略部分内容…#探测与指定主机是否通信\nLinux是一个比较实在的操作系统，这个ping命令如果不使用“Ctrl+C”快捷键强行中止，就会一直ping下去，直到天荒地老……\n例2：指定ping的次数\n既然ping这么“实在”，如果不想一直ping下去，则可以使用“-c”选项指定ping的次数。例如：\n[root@localhost ~]# ping -c 3192.168.103.151#只探测3次，就中止ping命令\n例3：探测网段中的可用主机\n在ping命令中，可以使用“-b”选项，后面加入广播地址，探测整个网段。我们可以使用这个选项知道整个网络中有多少主机是可以和我们通信的，而不用一个一个IP地址地进行探测。例如：\n[root@localhost ~]# ping -b -c 3192.168.103.255WARNING: pinging broadcast addressPING 192.168.103.255 (192.168.103.255) 56(84) bytes of data.64 bytes from 192.168.103.199: icmp_seq=1 ttl=64 time=1.95 ms64 bytes from 192.168.103.168: icmp_seq=1 ttl=64 time=1.97 ms (DUP! )64 bytes from 192.168.103.252: icmp_seq=1 ttl=64 time=2.29 ms (DUP! )…省略部分内容…#探测192.168.103.0/24网段中有多少可以通信的主机\n","categories":["Learning Materials"]},{"title":"Neurosurgeon：Collaborative Intelligence Between the Cloud and Mobile Edge","url":"/2022/10/20/Neurosurgeon/","content":"\n 来        源：ASPLOS ‘17: Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems - April 2017 - Pages 615–629https://doi.org/10.1145/3037697.3037698\n 原文速递：Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge\n\n1. Background &amp; Motivation当前的智能应用，比如Apple Siri，Google Now和Microsoft Cortana，计算过程完全在云端进行。这种云计算范式通过网络传输用户原始到云数据中心，不仅对数据中心造成了很大的压力，还存在很高的通信时延。随着手机等移动设备的计算能力增强，能否将一部分数据计算从云端移至移动设备上进行值得研究。\n1.1 云中心计算现状\n    \n\n\n图1 AlexNet延迟分解\n\n\n    \n\n\n图2 AlextNet能耗分解\n\n\n1）对于3G和LTE网络，数据传输时延往往高于移动设备计算时延；\n2）云计算具备很大的计算优势，但经常由于数据传输时延高而失去优势；\n3）移动设备本地计算往往获得更低的时延和能耗，但在快速WiFi网络中，云计算的性能更佳。\n\n1.2 计算分配基于对云中心计算现状的研究，同时考虑到DNN的层结构很适合进行计算分割，该文考虑云端间的层级计算分配。\n\n    \n\n\n图3 AlexNet每层的执行时间和输出数据尺寸\n\n\n    \n\n\n图4 选择不同分割点时的时延和能耗\n\n\n1）由于每层的类型和在模型中所处的位置不同，具备不同的计算和数据需求；\n2）卷积和池化层的计算时延较小，而全连接层的时延较高；\n3）卷积和池化层往往在模型的前部，全连接层在后部；\n4）卷积层增加输出数据量，池化层减少输出数据量。前几层的数据量逐渐减小，后几层的数据量小于原始输入；\n5）“前几层的数据量逐渐减小，后几层的计算时延逐渐增加”的发现说明：在模型中部进行计算分割并由云端协同执行推理能够取得性能增益。\n\n2. Contributions\n1）深入的现状分析。对云中心计算方案进行分析，发现数据的无线网络传输时现有方法的瓶颈，在移动端进行DNN推理往往可以获得更低的时延和能耗；\n2）DNN计算和数据量研究。深入研究8种DNN的层级别时延和数据量特性，发现DNN的不同层具有不同的计算量和数据尺寸特性；\n3）云-端DNN计算分配。发现层级别的DNN分割能够获得性能增益，设计了一个系统方法在不同应用的DNN中寻找最优分割点来减小时延和能耗；\n4）设计Neurosurgeon运行时系统和层性能预测模型。\n\n3. Design DetailsNeurosurgeon由一个部署阶段和一个用来管理云-端分割的运行时系统组成，如图5。\n\n    \n\n\n图5 Neurosurgeon系统组成\n\n在部署阶段-Neurosurgeon通过分析移动设备和服务器生成各种DNN层类型的预测模型，该分析过程与DNN应用无关，对于移动设备和服务器平台只需进行一次，之后模型将被储存在移动设备和服务器上用于对每层的时延和能耗进行预测。\n在运行时-Neurosurgeon动态地决定DNN的最佳分割点，如图5所示，该过程分为以下步骤：1）Neurosurgeon分析并提取DNN每层的类型和配置；2）系统使用存储的层性能模型估计在云和移动设备上执行每一层的时延和能耗；3）综合考虑每个预测结果和当前无线连接带宽及数据中心负载情况，Neuro surgeon选择针对时延或能耗的最佳分割点；4）Neurosurgeon在云-端间协同执行DNN。\n3.1 性能预测模型对于每种层类型，Neurosurgeon基于该层的配置建立一个回归模型预测时延和能耗，使用GFLOPS（每秒千兆浮点运算）作为性能预测的量度。\n1）卷积层和池化层\n对卷积层的回归模型基于两个参数变量：输入特征图的特征数量和输入特征图每个像素的计算量（(filter\\ size / stride)^2 \\times(＃\\ of\\ filters)）。对于池化层，使用输入、输出特征图的尺寸作为回归模型的参数变量.\n2）全连接层\n对于全连接层，使用输入神经元和输出神经元的数量作为回归模型的参数。对于Softmax和argmax层的处理类似。\n3）激活层\n激活层的参数较少，只有输入输出数据的一对一映射，因此使用神经元的数量作为回归模型的参数变量。对于标准化层的操作类似。\n3.2 动态DNN分割Neurosurgeon的DNN分割算法如下，Neurosurgeon首先分析组成目标DNN的各层(L_i)，并使用性能预测器获得每层在移动设备和服务器上计算的时延(TM_i)、(TC_i)和能耗(PM_i)、(PU\\times TU_i)。由于线性模型是十分简单的，该DNN分割算法是轻量且高效的。\n\n    \n\n\n4. CommentsNeurosurgeon是一个云-端协同的协同推理系统，能够相较云计算减少传输数据量，相较单机边缘计算提升计算速度，从而提高DNN推理的响应速度。然而，该系统存在许多不足之处：\n\n1）性能预测器准确度无法保证。该系统的性能预测器仅仅针对对不同层参数构建简单的线性或指数回归模型，虽然该模型执行很快，但如此简单的预测方法难以获得高预测精度，而较低的预测精度势必影响DNN分割算法的效果；\n2）对多层网络的性能预测不准确。该文仅使用每层网络时延和能耗的加和表示多层的性能，而实验表明运行多层网络的时延和能耗往往低于每层网络的加和，由错误的性能预测数据确定DNN分割点很难得到最优方案；\n3）云-端协同的方案受到一些限制。首先，云-端协同仍然受到网络条件限制，在网络环境较差时即便转为单机推理也存在时延增加的问题。其次，该方案仅适用于一台移动设备和一台服务器的协同推理，参与节点较少，灵活性较差。\n\n","categories":["Paper Notes"]},{"title":"NeuralPower：Predict and Deploy Energy-Efficient Convolutional Neural Networks","url":"/2022/10/21/NeuralPower/","content":"\n来        源：Proceedings of The 9th Asian Conference on Machine Learning, ACML 2017, Seoul, Korea, November 15-17, 2017\n原文速递：NeuralPower: Predict and Deploy Energy-Efficient Convolutional Neural Networks\n\n1. Background &amp; Motivation具有相近性能的网络模型运行时可能存在高达40倍的能耗（图1），为了设计节能的CNN网络，进行准确的时延、能耗预测是至关重要的。然而该文发现现有的性能预测方法存在一些不足：1）通常使用的CNN复杂度指标（如FLOPs）太粗略以至于无法预测真实平台的能耗，运行CNN的能耗不仅与网络结构有关，还需要考虑软硬件平台；2）未能定量地捕捉到CNN架构变化对运行时间、功率和能耗地影响。\n\n    \n\n\n图1 多种CNN架构的测试误差和能耗\n\n该文提出一种CNN时延、功率、能耗预测框架——NeuralPower，不需要在目标平台上实际运行CNN网络就可以在部署阶段预测网络的时延、功率和能耗。\n\n    \n\n\n图2 NeuralPower预测过程\n\n2. Contributions\n1)提出基于学习的多项式回归方法NeuralPower，是第一个预测CNN能耗的框架；\n2）NeuralPower能够预测CNN运行时间，相比现有方法取得68.5%的准确度提升；\n3）除了网络总体运行时延和能耗的预测，NeuralPower能够分析网络不同部分（每层）的时延和能耗，帮助模型设计人员分析模型瓶颈，更好地平衡精度和能耗。\n\n3. Design Details3.1 单层时延和能耗模型与其他工作不同，该文的目标是在不需要了解平台详细信息的情况下，使预测模型能够灵活应用到各种软硬件平台。该文提出一个基于学习的多项式回归模型用于学习不同CNN层类型的系数。\n层级时延模型：CNN模型一层的运行时延可以被表示为：\n\n    \n\n\n该模型由两部分组成，第一部分是对输入向量X_T\\in \\mathbb{R}^{D_T}的多项式函数，x_i是X_T的第i项，q_{ij}是多项式第j项对于x_i的指数，c_j是要学习的系数。X_T包含的层配置参数（如batch尺寸、输入输出尺寸等），对不同层类型，X_T的维度D_T有所不同。第二部分是特殊的多项式项\\digamma，封装了每层的物理操作（如内存访问的总次数和浮点运算的总次数），特殊多项式的项数根据层类型变化，c_s^\\prime是需要学习的参数。\n层级能耗模型：层能耗预测同样使用基于多项式的方法：\n\n    \n\n\n该模型同样由两部分组成，第一部分是输入向量X_P的多项式函数，m_{ij}是多项式第j项对于x_i的指数，z_j是要学习的系数。在模型的第二部分，z_k^\\prime为第k项需要学习的系数。\n该文注意到能耗的固有限制（计算负载增加时，能耗不再线性增长，存在一定上限），该文引入输入特征的对数项应对这种趋势以提高准确度。\n3.2 完整模型时延和能耗模型完整模型时延模型：对于一个N层串行模型，模型总时延可通过n层时延预测值\\hat{T}_n的加和获得：\n\n    \n\n\n完整模型功耗模型：平均功耗\\hat{P}_{avg}可由下式获得：\n\n    \n\n\n完整模型能耗模型：整个网络的能耗\\hat{E}_{total}可由下式获得：\n\n    \n\n\n3.3 模型选择（1）运行时间模型\nNeuralPower通过改变多项式模型的次数调整特征空间的尺度，应用Lasso压缩估计为每个多项式模型选择最佳模型，选择具有最低交叉验证均方根误差(RMSE)的模型，如图3，此时选择次数为2的模型更加合理。\n\n    \n\n\n图3 针对全连接层的最优多项式模型和其他多项式次数模型对比\n\n卷积层：卷积层是最耗时间和能耗的CNN部件，该文使用3次的多项式模型对该层建模，选择的特征向量包括：batch尺寸、输入tensor尺寸、卷积和尺寸、步长、padding尺寸和输出tensor尺寸。对于特殊项，使用能表示总计算量和内存访问量的多项式。\n全连接层：该文使用2次多项式回归模型，特征向量为：batch尺寸、输入tensor尺寸和输出tensor尺寸。\n池化层：该文使用3次多项式回归模型，输入的特征向量为：输入tensor尺寸、步长、卷积核尺寸和输出tensor尺寸。\n（2）能耗模型\n该文使用原始特征的对数形式作为多项式模型的特殊特征，并发现使用2次多项式模型对三种CNN层类型能取得最优交叉验证误差。\n3.4 模型评估（1）层级运行时间模型\n表1 NeuralPower的层级时延模型与SOTA方法对比\n\n\n    \n\n\n（2）层级能耗模型\n表2 CNN通用层的能耗模型\n\n\n    \n\n\n（3）完整模型运行时间模型\n该文完整模型运行时间的计算建模为各层时延的加和：\n表3 完整模型时延预测性能对比\n\n\n    \n\n\n该文发现各层运行时延的加和与完整模型运行时延相近。\n（4）完整模型能耗模型\n表4 完整模型功耗预测性能\n\n\n    \n\n\n表5 完整模型能耗预测性能\n\n\n    \n\n\n4. Comments\n1）该文考虑到其他基于单网络复杂度指标（e.g. FLOPs）方法的局限性，该文提出基于多项式模型的模型性能预测方法；\n2）该文不仅进行了模型运行时延预测，还设计了能耗的预测方法；\n3）该文的完整模型时延预测是对各层时延的加和，实验发现各层运行时延的加和与完整模型运行时延相近，该说法是否可靠还待考究。\n\n","categories":["Paper Notes"]},{"title":"Numpy基础","url":"/2022/10/10/Numpy%E5%9F%BA%E7%A1%80/","content":"在机器学习和深度学习中，图像、声音、文本等输入数据最终都要转换为数组或矩阵。如何有效地进行数组和矩阵的运算？这就需要充分利用Numpy。Numpy（Numerical Python）是数据科学的通用语言，是科学计算、深度学习的基石。为什么是Numpy？实际上Python本身含有列表（list）和数组（array），但对于大数据来说，这些结构是有很多不足的。由于列表的元素可以是任何对象，因此列表中所保存的是对象的指针。例如为了保存一个简单的[1,2,3]，都需要有3个指针和3个整数对象。对于数值运算来说，这种结构显然比较浪费内存和CPU等宝贵资源。至于array对象，它可以直接保存数值，和C语言的一维数组比较类似。但是由于它不支持多维，在上面的函数也不多，因此也不适合做数值运算。Numpy（Numerical Python的简称）的诞生弥补了这些不足。Numpy提供了两种基本的对象：ndarray（N-dimensional Array Object）和ufunc（Universal Function Object）。ndarray是存储单一数据类型的多维数组，而ufunc则是能够对数组进行处理的函数。\n1. 创建Numpy数组在 Python 内建对象中，数组有三种形式：\n1）列表：[1, 2, 3]2）元组：(1, 2, 3, 4, 5)3）字典：{A:1, B:2}其中，元组与列表相似，不同之处在于元组的元素不能修改。而字典由键和值构成，上例中，A为键，1为值。python 标准类针对数组的处理局限于 1 维，并仅提供少量的功能。而 NumPy 最核心且最重要的一个特性就是其封装了一个新的数据类型ndarray（N-dimensional Array），它是一个多维数组对象。该对象封装了许多常用的数学运算函数，方便我们做数据处理、数据分析等。\n1.1 将列表转换成ndarraynp.array(object, dtype=None, copy=True, order=None, subok=False, ndmin=0)\n\nobject：列表、元组等。dtype：数据类型。如果未给出，则类型为被保存对象所需的最小类型。\n\n例：\nimport numpy as nplst = [[3.14, 2.17, 0, 1, 2], [1, 2, 3, 4, 5]]nd = np.array(lst)print(nd1) #[[3.14, 2.17, 0, 1, 2]# [1. 2. 3. 4. 5.]]print(type(nd))#&lt;class &#x27;numpy.ndarray&#x27;&gt;\n1.2 使用random模块生成数组在深度学习中，我们经常需要对一些参数进行初始化，因此为了更有效地训练模型，提高模型的性能，有些初始化还需要满足一定的条件，如满足正态分布或均匀分布等。这里介绍了几种常用的方法，表1列举了np.random模块常用的函数。\n表1 np.random模块常用函数\n\n\n    \n\n\n\n\n例：\nimport numpy as npnp.random.seed(123)nd = np.random.randn(2, 3)print(nd)#[[-1.0856306 0.99734545 0.2829785]#[-1.50629471 -0.57860025 1.65143654]]\n1.3 创建特定形状的多维数组表2 Numpy数组创建函数\n\n\n    \n\n\n1.4利用arange、linspace函数生成数组arange() 的功能是在给定区间内创建一系列均匀间隔的值。方法如下：\nnumpy.arange(start, stop, step, dtype=None)\n\nstart: 序列的起始值。\nstop: 序列的结束值。\nstep: 步长。\n\n例：\n# 在区间 [3, 7) 中以 0.5 为步长新建数组np.arange(3, 7, 0.5, dtype=&#x27;float32&#x27;)# array([3. , 3.5, 4. , 4.5, 5. , 5.5, 6. , 6.5], dtype=float32)\nlinspace 用于在指定的区间内返回间隔均匀的值。其方法如下：\nnumpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)\n\nstart：序列的起始值。stop：序列的结束值。num：生成的样本数。默认值为 50。endpoint：布尔值，如果为真，则最后一个样本包含在序列内。retstep：布尔值，如果为真，返回间距。dtype：数组的类型。\n\n例：\n# 在0～10之间生成等间隔的10个数np.linspace(0, 10, 10, endpoint=True)#array([ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,#        5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ])\n2. 数组维度和形状NumPy 数组又被称之为 ndarray 多维数组，n 就可以从1维依次递增。下图显示了 1 至 3 维的 NumPy 数组示例。\n\n    \n\n\n图1 1至3维Numpy数组\n\n通过下例了解数组的维度和形状。\none = np.array([7, 2, 9, 10])two = np.array([[5.2, 3.0, 4.5],                [9.1, 0.1, 0.3]])three = np.array([[[1, 1], [1, 1], [1, 1]],                  [[1, 1], [1, 1], [1, 1]],                  [[1, 1], [1, 1], [1, 1]],                  [[1, 1], [1, 1], [1, 1]]])one.shape, two.shape, three.shape#((4,), (2, 3), (4, 3, 2))\n3. Ndarray数组基本操作3.1 类型转换表3 Numpy中的类型转换函数\n\n\n    \n\n\n\na = np.arange(4).reshape(2, 2)np.asmatrix(a)  # 将二维数组转化为矩阵类型输出结果：matrix([[0, 1],        [2, 3]])\n3.2更改数组形状表4 Numpy中改变数组形状的一些函数\n\n\n    \n\n\n\n\n(1) reshape\narr =np.arange(10)print(arr)# 将向量 arr 维度变换为2行5列print(arr.reshape(2, 5))# 指定维度时可以只指定行数或列数, 其他用 -1 代替print(arr.reshape(5, -1))print(arr.reshape(-1, 5))输出结果：[0 1 2 3 4 5 6 7 8 9][[0 1 2 3 4] [5 6 7 8 9]][[0 1] [2 3] [4 5] [6 7] [8 9]][[0 1 2 3 4] [5 6 7 8 9]]\n(2) resize\narr =np.arange(10)print(arr)# 将向量 arr 维度变换为2行5列arr.resize(2, 5)print(arr)输出结果：[0 1 2 3 4 5 6 7 8 9][[0 1 2 3 4] [5 6 7 8 9]]\n(3) T\narr =np.arange(12).reshape(3,4)# 向量 arr 为3行4列print(arr)# 将向量 arr 进行转置为4行3列print(arr.T)输出结果：[[ 0  1  2  3] [ 4  5  6  7] [ 8  9 10 11]][[ 0  4  8] [ 1  5  9] [ 2  6 10] [ 3  7 11]]\n(4) ravel\narr =np.arange(6).reshape(2, -1)print(arr)# 按照列优先，展平print(&quot;按照列优先，展平&quot;)print(arr.ravel(&#x27;F&#x27;))# 按照行优先，展平print(&quot;按照行优先，展平&quot;)print(arr.ravel())输出结果：[[0 1 2] [3 4 5]]按照列优先，展平[0 3 1 4 2 5]按照行优先，展平[0 1 2 3 4 5]\n(5) flatten\na =np.floor(10*np.random.random((3,4)))print(a)print(a.flatten())输出结果：[[4. 0. 8. 5.] [1. 0. 4. 8.] [8. 2. 3. 7.]][4. 0. 8. 5. 1. 0. 4. 8. 8. 2. 3. 7.]\n(6)squeeze\n这是一个主要用来降维的函数，把矩阵中含1的维度去掉。\narr =np.arange(3).reshape(3, 1)print(arr.shape)  #(3,1)print(arr.squeeze().shape)  #(3,)arr1 =np.arange(6).reshape(3,1,2,1)print(arr1.shape) #(3, 1, 2, 1)print(arr1.squeeze().shape) #(3, 2)\n(7) transpose\n对高维矩阵进行轴对换，这个在深度学习中经常使用，比如把图片中表示颜色顺序的RGB改为GBR。\narr2 = np.arange(24).reshape(2,3,4)print(arr2.shape)  #(2, 3, 4)print(arr2.transpose(1,2,0).shape)  #(3, 4, 2)\n3.3 合并数组表5 Numpy数组合并方法\n\n\n    \n\n\n\n说明：\n1）append、concatenate以及stack都有一个axis参数，用于控制数组的合并方式是按行还是按列；2）对于append和concatenate，待合并的数组必须有相同的行数或列数（满足一个即可）；3）stack、hstack、dstack，要求待合并的数组必须具有相同的形状（shape）。\n\n(1) append\n合并一维数组：\na =np.array([1, 2, 3])b = np.array([4, 5, 6])c = np.append(a, b)print(c) # [1 2 3 4 5 6]\n合并多维数组：\na =np.arange(4).reshape(2, 2)b = np.arange(4).reshape(2, 2)# 按行合并c = np.append(a, b, axis=0)print(&#x27;按行合并后的结果&#x27;)print(c)print(&#x27;合并后数据维度&#x27;, c.shape)# 按列合并d = np.append(a, b, axis=1)print(&#x27;按列合并后的结果&#x27;)print(d)print(&#x27;合并后数据维度&#x27;, d.shape)输出结果：按行合并后的结果[[0 1] [2 3] [0 1] [2 3]]合并后数据维度 (4, 2)按列合并后的结果[[0 1 0 1] [2 3 2 3]]合并后数据维度 (2, 4)\n(2) concatenate\n沿指定轴连接数组或矩阵：\nimport numpy as npa =np.array([[1, 2], [3, 4]])b = np.array([[5, 6]])c = np.concatenate((a, b), axis=0)print(c)d = np.concatenate((a, b.T), axis=1)print(d)输出结果：[[1 2] [3 4] [5 6]][[1 2 5] [3 4 6]]\n(3) stack\n沿指定轴堆叠数组或矩阵：\na =np.array([[1, 2], [3, 4]])b = np.array([[5, 6], [7, 8]])print(np.stack((a, b), axis=0))输出结果：[[[1 2]  [3 4]] [[5 6]  [7 8]]]\n3.4数组索引和切片NumPy 中的索引和 Python 对list索引的方式非常相似，但又有所不同。\n(1) 数组索引\n一维数组索引\n取一维数据的索引和 list 一样\na = np.arange(10)a[1]输出结果：1\n#分别获取索引值为 1，2，3 的数据a[[1, 2, 3]] # 多加一层 [] 使用逗号隔开输出结果：array([1, 2, 3])\n二维数组索引\n#访问二维Ndarray中的单个元素：a = np.arange(20).reshape(4, 5)a[1, 2] # 不加 []，直接使用逗号隔开输出结果：7\nPython 中 list 索引 2 维数据的方法写法是 a[1][2]，可以看到还是有点区别的。\n访问二维Ndarray中的多个元素：a[[1, 2], [3, 4]]输出结果：array([8, 14])\n这里需要注意索引的对应关系。我们实际获取的是[1, 3]，也就是第2行和第4列对于的值8。以及[2, 4]，也就是第3行和第5列对应的值14。\n(2) 数组切片\nNumPy里面针对 Ndarray 的数组切片和Python里的list切片操作是一样的。其语法为：\n\nNdarray [start : stop : step]\n\n[start:stop:step] 分别代表 [起始索引:截至索引:步长]。\n一维数组切片\na = np.arange(10)a[:5] # 默认左边从 0 开始，左闭右开a[5:] # 默认右边以最后一个结尾a[0:10:2] # 步长为2输出结果：array([0, 1, 2, 3, 4])array([5, 6, 7, 8, 9])array([0, 2, 4, 6, 8])\n多维数组切片\na = np.arange(20).reshape(4, 5)a[0:3, 2:4] #先取第 1，2，3 行（第一个维度），再取第 3，4 列（第二个维度）输出结果：array([[ 2,  3],       [ 7,  8],       [12, 13]])\n3.5 搜索和计数表6 Numpy搜索和计数函数\n\n\n    \n\n\na = np.random.randint(0, 10, 20) # 在 [0,10) 之间生成 20 个随机数anp.argmax(a)np.nonzero(a)np.count_nonzero(a)输出结果：array([9, 2, 0, 5, 2, 4, 2, 0, 0, 0, 0, 7, 3, 6, 8, 1, 9, 1, 0, 3])0(array([ 0,  1,  3,  4,  5,  6, 11, 12, 13, 14, 15, 16, 17, 19]),)14\n4. 通用函数Numpy提供了两种基本的对象，即ndarray和ufunc对象。前文已经介绍了ndarray，本节将介绍Numpy的另一个对象通用函数（ufunc）。ufunc是universal function的缩写，它是一种能对数组的每个元素进行操作的函数。许多ufunc函数都是用C语言级别实现的，因此它们的计算速度非常快。此外，它们比math模块中的函数更灵活。math模块的输入一般是标量，但Numpy中的函数可以是向量或矩阵，而利用向量或矩阵可以避免使用循环语句，这点在机器学习、深度学习中非常重要。\n表7 Numpy中的几个常用通用函数\n\n\n    \n\n\n5. 广播机制Numpy的Universal functions中要求输入的数组shape是一致的，当数组的shape不相等时，则会使用广播机制。不过，调整数组使得shape一样，需要满足一定的规则，否则将出错。这些规则可归纳为以下4条。\n\n1）让所有输入数组都向其中shape最长的数组看齐，不足的部分则通过在前面加1补齐，如：a：2×3×2b：3×2则b向a看齐，在b的前面加1，变为：1×3×22）输出数组的shape是输入数组shape的各个轴上的最大值；3）如果输入数组的某个轴和输出数组的对应轴的长度相同或者某个轴的长度为1时，这个数组能被用来计算，否则出错；4）当输入数组的某个轴的长度为1时，沿着此轴运算时都用（或复制）此轴上的第一组值。\n\n广播在整个Numpy中用于决定如何处理形状迥异的数组，涉及的算术运算包括（+，-，*，/…）。这些规则说得很严谨，但不直观，下面我们结合图形与代码来进一步说明。  目的：A+B，其中A为4×1矩阵，B为一维向量（3,）。要相加，需要做如下处理：  ·根据规则1，B需要向看齐，把B变为（1,3）  ·根据规则2，输出的结果为各个轴上的最大值，即输出结果应该为（4,3）矩阵，那么A如何由（4,1）变为（4,3）矩阵？B又如何由（1,3）变为（4,3）矩阵？  ·根据规则4，用此轴上的第一组值（要主要区分是哪个轴），进行复制（但在实际处理中不是真正复制，否则太耗内存，而是采用其他对象如ogrid对象，进行网格处理）即可，详细处理过程如图2所示。\n\n    \n\n\n图2 Numpy广播规则示意图\n\nA = np.arange(0, 40,10).reshape(4, 1)B = np.arange(0, 3)print(&quot;A矩阵的形状:&#123;&#125;,B矩阵的形状:&#123;&#125;&quot;.format(A.shape,B.shape))C=A+Bprint(&quot;C矩阵的形状:&#123;&#125;&quot;.format(C.shape))print(C)运行结果：A矩阵的形状:(4, 1),B矩阵的形状:(3,)C矩阵的形状:(4, 3)[[ 0  1  2] [10 11 12] [20 21 22] [30 31 32]]\n附录：dtype 数据类型\n    \n\n\n\n\n\n更多Numpy相关内容，请查看：http://www.Numpy.org/\n\n","categories":["Learning Materials"]},{"title":"Topic1：Is Accurate and Light-weight DNN Multi-layer Latency Prediction Possible?","url":"/2022/10/22/Topic1/","content":"\n\n\n    \n    \n    \n        .head{\n            position: relative;\n            padding: 10px;\n        }\n        .bl{\n            box-shadow: 0 4px 8px 0 #1D2088;\n            font-size: 20px;\n            text-align: left;\n            background-color: #fff;\n            border-radius: 7px;\n            position: relative;\n            margin: 0px;\n            padding: 20px;\n            line-height: 1.5em;\n            text-decoration: none;\n            a{ text-decoration: none};\n        }\n\n        .button {\n          display: inline-block;\n          padding: 5px 15px;\n          font-size: 14px;\n          cursor: pointer;\n          text-align: center;   \n          text-decoration: none;\n          outline: none;\n          color: #000000;\n          background-color: #C9D7F3;\n          border: 0px solid #ffffff;\n          border-radius: 8px;\n          box-shadow: 0 4px 4px  #1D2088;\n          a { text-decoration: none};\n        }\n        .button:hover {background-color: #B4C7EE\n        text-decoration: none;\n        a { text-decoration: none}; \n        }\n\n        .button:active {\n          background-color: #ffffff;\n          box-shadow: 0 3px 4px #1D2088;\n          transform: translateY(2px);\n          text-decoration: none;\n          a { text-decoration: none};\n        }\n        p {font-size: 14px;};\n        a { text-decoration: none;} ;\n        a: hover{ text-decoration: none;};\n    \n\n\n    \n        \n        \n    \n    \n        CoDL：Efficient CPU-GPU Co-execution for Deep Learning Inference on Mobile Devices\n        \n        本文提出了一个在移动设备的CPU和GPU上协同执行模型推理的系统架构和方法，许多方法的提出基于作者对大量实验现象的观察。通过联系实验现象和系统基本特性，该文实现了针对于移动设备的准确且轻量的时延预测。同时，该文数据分割和算子链的设计也具有启发意义。\n\n​        \n\n        \n        Paper Note\n        &nbsp\n        \n        Paper Link\n    \n    \n    \n        Neurosurgeon：Collaborative Intelligence Between the Cloud and Mobile Edge\n        \n        Neurosurgeon是一个云-端协同的协同推理系统，能够相较云计算减少传输数据量，相较单机边缘计算提升计算速度，从而提高DNN推理的响应速度。然而，该系统存在许多不足之处：\n        1）性能预测器准确度无法保证。该系统的性能预测器仅仅针对对不同层参数构建简单的线性或指数回归模型，虽然该模型执行很快，但如此简单的预测方法难以获得高预测精度，而较低的预测精度势必影响DNN分割算法的效果；\n\n2）对多层网络的性能预测不准确。该文仅使用每层网络时延和能耗的加和表示多层的性能，而实验表明运行多层网络的时延和能耗往往低于每层网络的加和，由错误的性能预测数据确定DNN分割点很难得到最优方案。\n\n        \n        Paper Note\n        &nbsp\n        \n        Paper Link\n    \n\n    \n    \n        µLayer：Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization\n        \n        该文致力于通过CPU-GPU协同执行的方式进行高效神经网络推理，设计了一种层级协同推理方法，能够取得一定的性能提升，但仍存在一些问题：\n        1）该系统中的时延预测器采用机器学习拟合函数，与Neurosurgeon相同，存在精度低的问题；\n\n2）该CPU-GPU协同方法是层粒度的，存在高频同步的问题，而由于时延预测器的不准确性会造成CPU，GPU的运行差异，同步开销可能会成为时延的主体部分。\n\n        \n        Paper Note\n        &nbsp\n        \n        Paper Link\n    \n\n    \n    \n        NeuralPower：Predict and Deploy Energy-Efficient Convolutional Neural Networks\n        \n        1）该文考虑到其他基于单网络复杂度指标（e.g. FLOPs）方法的局限性，该文提出基于多项式模型的模型性能预测方法；\n\n2）该文不仅进行了模型运行时延预测，还设计了能耗的预测方法；\n\n3）该文的完整模型时延预测是对各层时延的加和，实验发现各层运行时延的加和与完整模型运行时延相近，该说法是否可靠还待考究。\n\n        \n        Paper Note\n        &nbsp\n        \n        Paper Link\n    \n\n    \n    \n        nn-Meter：Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices\n        \n        1）与部分工作采用模型各层时延之和预测完整模型时延的方法不同，该工作考虑了神经网络算子融合的问题，设计了针对不同设备类型的网络级时延预测方法；2)但部分工作中提到nn-meter存在内存占用量大的问题，难以在移动设备上装载。\n\n        \n        Paper Note\n        &nbsp\n        \n        Paper Link\n    \n\n    \n    \n        BRP-NAS: Prediction-based NAS using GCNs\n        \n        与层级时延预测不同，该工作将模型的结构信息也作为时延预测的一个参量，使用图卷积网络（GCN）直接预测完整模型的时延。1）考虑整个网络的特性，而非是对单层时延预测值的简单加和，准确度更高；\n    2）未考虑设备特性，是完全黑盒的预测，模型尺寸可能比较大；\n    3）泛化能力差，对新模型的支持性差。\n\n        \n        Paper Note\n        &nbsp\n        \n        Paper Link\n    \n\n    \n    \n        Learned TPU Cost Model for XLA Tensor Programs\n        \n        该工作设计了一个基于学习的神经网络性能评估模型，用于预测每个kernel的执行时间y'。 \n        1）该工作考虑了框架在进行神经网络推理时算子间的影响，采用图嵌入的方式进行时延预测，比单层时延预测加和的模型时延预测准确；2）该工作是基于学习的神经网络预测方法，计算和训练开销可能较大。\n\n        \n        Paper Note\n        &nbsp\n        \n        Paper Link\n    \n\n    \n    \n        FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search\n        \n        使用一个标定表模型预测每个算子的时延，通过计算算子时延之和预测模型总时延\n        1）该方法考虑到时延预测的效率，采用标定表代替预测模型，是一种轻量级的方案；2）该方法在设备转换时可能存在需要重新标定的问题，工作量较大；3）该方法未考虑框架在运行时的算子融合，存在精度低的问题。\n        \n\n        \n        Paper Note\n        &nbsp\n        \n        Paper Link\n    \n\n    \n    \n        PALEO：A Performance Model for Deep Neural Networks\n        \n        \n        该工作综合考虑了网络结构和硬件配置，通过分析DNN在设备上进行计算的过程对神经网络的推理时延进行预测。\n        1）该工作分析了在设备上进行推理需要的开销，能在一定程度上降低基于FLOPs时延预测的误差；\n2）基于FLOPs的时延预测误差较大；\n3）该工作对完整模型的时延预测方法采用多层时延的加和，未考虑算子融合机制，误差较大。\n        \n\n        \n        Paper Note\n        &nbsp\n        \n        Paper Link\n    \n\n    \n    \n        Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression\n        \n        该工作侧重于对电脑应用程序运行时间的预测，与直接进行模型训练不同，其通过分析代码特性提取出多个程序的特征，并使用稀疏多项式回归模型预测程序运行时间。1）该方法使用多项式模型预测程序运行时延，准确性优于基于FLOPs的线性模型；2）该方法通过代码分析的方式预测电脑程序的运行时间，但对于神经网络的泛化性较差，许多代码并不开源，无法获得代码中的详细信息。\n        \n\n        \n        Paper Note\n        &nbsp\n        \n        Paper Link\n    \n\n\n    \n    \n        TVM：An Automated End-to-End Optimizing Compiler for Deep Learning\n        \n        该工作使用基于机器学习的代价模型，在多轮预测的过程中测量模型实际运行时间，周期性地对模型进行更新，逐渐提高模型准确度。\n        1）该方法通过运行时对模型的逐渐优化提高模型在不同设备上的可迁移性；\n        2）该方法完全基于机器学习，训练和计算开销较大。\n        \n\n        \n        Paper Note\n        &nbsp\n        \n        Paper Link\n    \n\n\n\n    Learning to Optimize Halide with Tree Search and Random Programs\n    \n    该工作的代价模型使用人工设计的项向量和小神经网络预测的系数向量点乘获得程序的运行时间。\n    1）该方法考虑了预测器的预测效率，通过人工设计的多项式将预测网络变成小神经网络；\n    2）该方法需要考虑程序的代码特征，而对于非开源程序，该方法的泛化性较差。\n    \n\n\n    \n    Paper Note\n    &nbsp\n    \n    Paper Link\n\n\n\n\n    HELP：Hardware-Adaptive Efficient Latency Prediction for NAS via Meta-Learning\n    \n    该工作注意到许多工作的时延预测模型在不同设备上的泛化开销大的问题，提出了一种基于元学习的硬件设备自适应时延预测器，在预测器部署到新设备时，只需要采集少量数据就可以实现模型迁移。\n    1）该工作考虑了预测模型在不同设备上的重新训练问题，使用元学习减少在不同设备上训练模型的代价；\n    2）该工作采用黑盒函数预测推理时延，要获得准确的预测结果需要复杂的函数，预测器效率不高。\n    \n\n\n    \n    Paper Note\n    &nbsp\n    \n    Paper Link\n\n\n\n\n    Reducing energy consumption of Neural Architecture Search: An inference latency prediction framework\n    \n    工作的时延预测器使用LSTM网络进行特征提取，使用LightGBM进行时延回归。\n    1）该文参考了多种特征嵌入、预测模型，能够获得较高的预测精度；2）该工作使用神经网络预测网络在设备上的时延，训练和预测的开销大，效率较低。\n    \n\n\n    \n    Paper Note\n    &nbsp\n    \n    Paper Link\n\n\n\n","categories":["Topic Corner"]},{"title":"group：DNN Latency Prediction","url":"/2022/10/26/group1/","content":"BRP-NAS: Prediction-based NAS using GCNs\n 来        源：34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada\n原文速递：BRP-NAS：Prediction-based NAS using GCNs\n\n与层级时延预测不同，该工作将模型的结构信息也作为时延预测的一个参量，直接预测完整模型的时延。对于图g=(V,E)，（V是N个具有D个特征的节点集，E是边集），将一个特征描述X\\in \\mathbb{R}^{N\\times D}通过一个邻接矩阵A\\in \\mathbb{R}^{N\\times N}进行编码作为图卷积网络（GCN）的输入。对于一个L层GCN，层间的传播规则为：\n\nH^{l+1}=f(H^l,A)=\\sigma(AH^lW^l)其中，H^l和W^l分别是第l层的特征图和权重矩阵，\\sigma(\\bullet)是一个非线性激活函数。\n该文使用一个4层GCN模型，每层有600个隐藏单元，引入一个全局节点（连接所有节点）汇总所有节点级信息进行图嵌入。\n\n1）考虑整个网络的特性，而非是对单层时延预测值的简单加和，准确度更高；\n2）未考虑设备特性，是完全黑盒的预测，模型尺寸可能比较大；\n3）泛化能力差，对新模型的支持性差。\n\n\n\nLearned TPU Cost Model for XLA Tensor Programs\n来        源：33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada\n原文速递：Learned TPU Cost Model for XLA Tensor Programs\n\n该工作设计了一个基于学习的神经网络性能评估模型，用于预测每个kernel的执行时间y'，该模型如下图所示。\n\n    \n\n\n图1 神经网络性能评估模型\n\n第i个操作的操作码x^o通过嵌入查找表嵌入为一个向量，一个操作的特征X^f包含输出tensor形状、striding、padding、卷积核尺寸、tensor重叠等。然后使用一个前馈层f_1接一个GraphSAGE合并操作码和操作特征信息，得到节点嵌入\\varepsilon ^k。之后，通过计算\\varepsilon每一行的均值和最大值生成kernel嵌入，将均值和最大值向量的合并结果传入最终的前馈层生成kernel预测结果。\n\n1）该工作考虑了框架在进行神经网络推理时算子间的影响，采用图嵌入的方式进行时延预测，比单层时延预测加和的模型时延预测准确；\n2）该工作是基于学习的神经网络预测方法，计算和训练开销可能较大。\n\n\n\nFBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search\n来        源：Computer Vision and Pattern Recognition 2019\n原文速递：FBNet：Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search\n\n该工作致力于进行准确高效的神经网络搜索（NAS），注意到FLOP数并不总能准确地反映神经网络在设备上的推理时延。该工作测量了搜索空间内每个算子在设备上的运行时延，并使用一个标定表模型预测每个算子的时延，通过计算算子时延之和预测模型总时延。\n\n    \n\n\n\n1）该方法考虑到时延预测的效率，采用标定表代替预测模型，是一种轻量级的方案；\n2）该方法在设备转换时可能存在需要重新标定的问题，工作量较大；\n3）该方法未考虑框架在运行时的算子融合，存在精度低的问题。\n\n\n\nPALEO：A Performance Model for Deep Neural Networks\n来        源：The International Conference on Learning Representations (ICLR) 2017\n原文速递：PALEO：A Performance Model for Deep Neural Networks\n\n该工作通过分析DNN在设备上进行计算的过程对神经网络的推理时延进行预测，其建模方法如下图所示，分为计算时延和通信时延两部分，并综合考虑了网络结构和硬件配置。\n\n    \n\n\n图2 PALEO模型方法\n\n该工作将每个神经网络节点u的计算时延建模为三部分：获取父节点输入的时间R(Pa(u))、在设备d上执行计算f的时间、将模型输出写入内存的时延W(f,d)。\n\nT(u)=R(Pa(u))+C(f,d)+W(f,d)计算时延基于FLOPs计数和设备运行速度计算：C(f,d)=FLOPs(f)/speed(d)，IO的时延通过计算内存占用/IO带宽获得。对于完整网络的时延预测，该工作通过计算所有层的时延和获得：T(N)=\\sum_{i=1}^nT(u^{(i)})。\n\n1）该工作分析了在设备上进行推理需要的开销，能在一定程度上降低基于FLOPs时延预测的误差；\n2）基于FLOPs的时延预测误差较大；\n3）该工作对完整模型的时延预测方法采用多层时延的加和，未考虑算子融合机制，误差较大。\n\n\n\nPredicting Execution Time of Computer Programs Using Sparse Polynomial Regression\n来        源：NIPS’10: Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1December 2010 Pages 883–891\n原文速递：Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression\n\n该工作侧重于对电脑应用程序运行时间的预测，与直接进行模型训练不同，其通过分析代码特性提取出多个程序的特征，并使用稀疏多项式回归模型预测程序运行时间。\n稀疏多项式回归参考了LASSO的工作，从程序中可以分析出许多特征，如循环数、分支数、变量值等，而这些特征并不都对预测结果产生较大贡献，但大量的特征会使得多项式项数增多，降低预测效率。因此，可以将对结果影响小的特征的系数置零，只留下对结果影响大的特征子集。\n\n1）该方法使用多项式模型预测程序运行时延，准确性优于基于FLOPs的线性模型；\n2）该方法通过代码分析的方式预测电脑程序的运行时间，但对于神经网络的泛化性较差，许多代码并不开源，无法获得代码中的详细信息。\n\nTVM：An Automated End-to-End Optimizing Compiler for Deep Learning\n来        源：the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI ’18). October 8–10, 2018 • Carlsbad, CA, USA\n原文速递：TVM：An Automated End-to-End Optimizing Compiler for Deep Learning\n\n该工作使用基于机器学习的代价模型，考虑到一个好的代价模型需要考虑影响性能的全部因素：内存访问模式、数据重用、管道依赖性等，现代硬件的复杂性高，并且针对每个新的目标硬件都需要新的代价模型，将是一个十分繁琐的过程。该方法在多轮预测的过程中测量模型实际运行时间，周期性地对模型进行更新，逐渐提高模型准确度。该方法在经过对比后使用梯度提升树模型（Gradient Tree Boosting Model），涉及到集中学习地范畴。\n\n1）该方法通过运行时对模型的逐渐优化提高模型在不同设备上的可迁移性；\n2）该方法完全基于机器学习，训练和计算开销较大。\n\n\n\nLearning to Optimize Halide with Tree Search and Random Programs\n来        源：ACM Transactions on Graphics, Volume 38, Issue 4, August 2019, Article No.: 121, pp 1–12 https://doi.org/10.1145/3306346.3322967\n原文速递：Learning to Optimize Halide with Tree Search and Random Programs\n\n该工作采用通过神经网络预测程序运行时间的方法，同时考虑到完全用网络进行黑盒预测需要一个很大的网络，并且很难训练，该工作的代价模型使用人工设计的项向量和小神经网络预测的系数向量点乘获得程序的运行时间。这些人工设计的项是特征的非线性组合，作者希望通过这样的设计方式使这些项与运行时间成比例地扩展。神经网络只需要调整每一项的权重，对于对最终表现无关的项，可将其系数置为0。\n\n1）该方法考虑了预测器的预测效率，通过人工设计的多项式将预测网络变成小神经网络；\n2）该方法需要考虑程序的代码特征，而对于非开源程序，该方法的泛化性较差。\n\n\n\nHELP：Hardware-Adaptive Efficient Latency Prediction for NAS via Meta-Learning\n来        源：35th Conference on Neural Information Processing Systems (NeurIPS 2021)\n原文速递：HELP：Hardware-Adaptive Efficient Latency Prediction for Nas via Meta-Learning\n\n该工作注意到许多工作的时延预测只针对特定设备进行设计，应用在每个新设备时都需要对模型重新训练，而每次训练都需要大量的测量数据，影响了NAS工作的效率。因此，该工作提出了一种基于元学习的硬件设备自适应时延预测器，在预测器部署到新设备时，只需要采集少量数据就可以实现模型迁移。\n\n1）该工作考虑了预测模型在不同设备上的重新训练问题，使用元学习减少在不同设备上训练模型的代价；\n2）该工作采用黑盒函数预测推理时延，要获得准确的预测结果需要复杂的函数，预测器效率不高。\n\n\n\nReducing energy consumption of Neural Architecture Search: An inference latency prediction framework\n来        源：Sustainable Cities and Society Volume 67, April 2021, 102747\n原文速递：Reducing energy consumption of Neural Architecture Search：An inference latency prediction framework\n\n该工作关注于更好地进行NAS，避免在实际设备上多次运行神经网络造成的碳排放和能源消耗，通过时延预测器直接获得神经网络在目标设备上的性能情况。具体地，该工作使用LSTM网络进行特征提取，使用LightGBM进行时延回归。\n\n1）该文参考了多种特征嵌入、预测模型，能够获得较高的预测精度；\n2）该工作使用神经网络预测神经网络在设备上的时延，训练和预测的开销大，效率较低。\n\n","categories":["Paper Notes"]},{"title":"nn-Meter：Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices","url":"/2022/10/26/nn-meter/","content":"\n来        源：MobiSys ‘21: Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and ServicesJune 2021 Pages 81–93https://doi.org/10.1145/3458864.3467882\n原文速递：nn-Meter：Towards Accurate Latency Prediction of Deep-Learning Model\n\n（2021 MobiSys Best Paper）\n\n1. Background &amp; Motivation设计一个高准确度、高效的DNN模型，需要得到准确的不同模型在部署中的推理时延。然而，测量DNN模型的推理时延是繁琐且昂贵的，推理时延可能会受到设备类型、模型框架的影响，模型推理时延的预测成为了一种重要的手段。但直接进行模型级别的预测，其搜索空间又太大：假设一个具有N个节点的模型，每个节点有D个特征，在模型DAG图中最多存在N×(N-1)条边，则整个预测空间中一共有多达DN×N×(N-1)个模型，这就需要寻找更细粒度的方法。尽管DNN网络大量出现，但基础算子类型仍然很少，比如卷积、激活函数、全连接层、BatchNorm和element-wise算子。虽然算子级预测是一种细粒度方法，但它并不能捕捉不同设备上的图级别（graph level）优化，如图1所示。这些较稳定的基础算子又进一步根据推理平台的优化构成了内核（kernel），任何DNN的都是由这些基础内核构成的。\n\n \n\n\n图1 DNN框架的图优化\n\n2. Contributions\n1）提出了nn-Meter，一种在边缘设备上准确预测CNN模型推理时延的系统；\n2）设计了nn-Meter的两个关键技术：内核检测器，用于捕获边缘设备上的各种算子融合行为。自适应采样，用于减少构建时延预测器的计算量；\n3）使用nn-Meter在手机CPU、手机GPU和Intel VPU上进行实验，结果显示，nn-Meter在多种设备和模型上都取得了相比现有方法更好的预测准确度。\n\n3. Design Details图2为nn-Meter的系统结构，包括内核检测和自适应数据采样，它离线检测了算子融合规则并且构建了内核级的预测器。\n\n \n\n\n图2 nn-Meter系统结构\n\n3.1 内核检测内核检测可以自动识别部署平台的算子融合规则，从而基于这些规则检测模型中的所有内核。nn-Meter 会离线收集所有融合规则，进行在线模型预测时，内核搜索算法则会将这些规则递归地应用于目标模型来找到所有内核，图3为内核检测的一个示例。\n\n \n\n\n图3 ResNet18内核检测示例\n\n3.2 自适应数据采样自适应数据采样用于训练内核级时延预测器，由于某些内核所具有的数据采样空间巨大，例如 Conv+bn+relu 的采样空间已达到千万数量级，该方法通过迭代采样过程对最有益的配置进行采样，离线为目标设备上的所有内核构建机器学习预测器。\n\n \n\n\n图4 多种输出通道数下Conv+bn+relu内核的推理时延\n\n如图4所示，随机采样会丢失很多真实的细节信息，该方法采用自适应采样在误差较大的点附近进行更细粒度的数据采样。最后，nn-Meter采用了随机森林回归来预测非线性的内核级延迟，使用内核的延迟总和计算整个模型的延迟\n\n \n\n\n其中，f_o是kernelo的机器学习预测器，x_o是提取的特征。\n4. Comments该文章考虑了内核级别的时延估计，相比之前的工作更加深入研究了模型在实际设备上的推理机制，考虑了框架和设备对模型的加速优化问题，该方法不仅能够用来预测整个模型的推理时延，也可以预测任意几层的推理时延。但部分工作中提到nn-meter存在内存占用量大的问题，难以在移动设备上装载。\n","categories":["Paper Notes"]},{"title":"PyTorch基础","url":"/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/","content":"1. 引言PyTorch是Facebook团队于2017年1月发布的一个深度学习框架，虽然晚于TensorFlow、Keras等框架，但自发布之日起，其关注度就在不断上升，目前在GitHub上的热度已超过Theano、Caffe、MXNet等框架。\nPyTorch是一个建立在Torch库之上的Python包，旨在加速深度学习应用。它提供一种类似Numpy的抽象方法来表征张量（或多维数组），可以利用GPU来加速训练。由于PyTorch采用了动态计算图（Dynamic Computational Graph）结构，且基于tape的Autograd系统的深度神经网络。其他很多框架，比如TensorFlow（TensorFlow2.0也加入了动态网络的支持）、Caffe、CNTK、Theano等，采用静态计算图。使用PyTorch，通过一种称为Reverse-mode auto-differentiation（反向模式自动微分）的技术，可以零延迟或零成本地任意改变你的网络的行为。Torch是PyTorch中的一个重要包，它包含了多维张量的数据结构以及基于其上的多种数学操作。自2015年谷歌开源TensorFlow以来，深度学习框架之争越来越激烈，全球多个看重AI研究与应用的科技巨头均在加大这方面的投入。PyTorch从2017年年初发布以来，可谓是异军突起，短时间内取得了一系列成果，成为明星框架。\nPyTorch由4个主要的包组成：\n\\cdottorch：类似于Numpy的通用数组库，可将张量类型转换为torch.cuda.TensorFloat，并在GPU上进行计算；\n\\cdottorch.autograd：用于构建计算图形并自动获取梯度的包；\n\\cdottorch.nn：具有共享层和损失函数的神经网络库；\n\\cdottorch.optim：具有通用优化算法（如SGD、Adam等）的优化包。\n2. Numpy与Tensor上一内容介绍了Numpy，了解到其存取数据非常方便，而且还拥有大量的函数，所以深得数据处理、机器学习者喜爱。这节内容将介绍PyTorch的Tensor，它可以是零维（又称为标量或一个数）、一维、二维及多维的数组。Tensor自称为神经网络界的Numpy，它与Numpy相似，二者可以共享内存，且之间的转换非常方便和高效。不过它们也有不同之处，最大的区别就是Numpy会把ndarray放在CPU中进行加速运算，而由Torch产生的Tensor会放在GPU中进行加速运算（假设当前环境有GPU）。\n2.1 创建Tensor创建Tensor的方法有很多，可以从列表或ndarray等类型进行构建，也可根据指定的形状构建。常见的创建Tensor的方法可参考表1。\n表1 常见的创建Tensor的方法\n\n\n    \n\n\n\nimport torch#根据list数据生成Tensortorch.Tensor([1,2,3,4,5,6])#根据指定形状生成Tensortorch.Tensor(2,3)#根据给定的数组生成Tensort=torch.Tensor([[1,2,3],[4,5,6]])#查看Tensor的形状t.size()#shape与size()等价t.shape#根据已有形状创建Tensortorch.Tensor(t.size())\n#生成一个单位矩阵torch.eye(2,2)#自动生成全是0的矩阵torch.zeros(2,3)#根据规则生成数据torch.linspace(1,10,4)#生成满足均匀分布随机数torch.rand(2,3)#生成满足标准分布随机数torch.randn(2,3)#返回所给数据形状相同，值全为0的张量torch.zeros_like(torch.rand(2,3))\n2.2 修改Tensor形状在处理数据、构建网络层等过程中，经常需要了解Tensor的形状、修改Tensor的形状。与修改Numpy的形状类似，修改Tenor的形状也有很多类似函数，具体可参考表2。\n表2 常用修改Tensor形状的函数\n\n\n    \n\n\n#生成一个形状为2x3的矩阵x = torch.randn(2, 3)#查看矩阵的形状x.size()   #结果为torch.Size([2, 3])#查看x的维度x.dim()    #结果为2#把x变为3x2的矩阵x.view(3,2)#把x展平为1维向量y=x.view(-1)  y.shape#添加一个维度z=torch.unsqueeze(y,0)#查看z的形状z.size()   #结果为torch.Size([1, 6])#计算Z的元素个数z.numel()   #结果为6\n\ntorch.view与torch.reshpae的异同1）reshape()可以由torch.reshape()，也可由torch.Tensor.reshape()调用。但view()只可由torch.Tensor.view()来调用；2）对于一个将要被view的Tensor，新的size必须与原来的size与stride兼容。否则，在view之前必须调用contiguous()方法；3）同样也是返回与input数据量相同，但形状不同的Tensor。若满足view的条件，则不会copy，若不满足，则会copy；4）如果你只想重塑张量，请使用torch.reshape。如果你还关注内存使用情况并希望确保两个张量共享相同的数据，请使用torch.view。\n\n2.3 索引操作Tensor的索引操作与Numpy类似，一般情况下索引结果与源数据共享内存。从Tensor获取元素除了可以通过索引，也可以借助一些函数，常用的选择函数可参考表3。\n表3 常用索引操作函数\n\n\n    \n\n\n设置一个随机种子torch.manual_seed(100) #生成一个形状为2x3的矩阵x = torch.randn(2, 3)#根据索引获取第1行，所有数据x[0,:]#获取最后一列数据x[:,-1]#生成是否大于0的Byter张量mask=x&gt;0#获取大于0的值torch.masked_select(x,mask)#获取非0下标,即行，列索引torch.nonzero(mask)#获取指定索引对应的值,输出根据以下规则得到#out[i][j] = input[index[i][j]][j]  # if dim == 0#out[i][j] = input[i][index[i][j]]  # if dim == 1index=torch.LongTensor([[0,1,1]])torch.gather(x,0,index)index=torch.LongTensor([[0,1,1],[1,1,1]])a=torch.gather(x,1,index)#把a的值返回到一个2x3的0矩阵中z=torch.zeros(2,3)z.scatter_(1,index,a)\n2.4 广播机制广播机制是向量运算的重要技巧。除了Numpy支持广播机制外，PyTorch也支持广播机制。\nimport torchimport numpy as npA = np.arange(0, 40, 10).reshape(4, 1)B = np.arange(0, 3)#把ndarray转换为TensorA1=torch.from_numpy(A)  #形状为4x1B1=torch.from_numpy(B)  #形状为3#Tensor自动实现广播C=A1+B1#我们可以根据广播机制，手工进行配置#根据规则1，B1需要向A1看齐，把B变为（1, 3）B2=B1.unsqueeze(0)  #B2的形状为1x3#使用expand函数重复数组，分别的4x3的矩阵A2=A1.expand(4, 3)B3=B2.expand(4, 3)#然后进行相加,C1与C结果一致C1=A2+B3\n2.5 逐元素操作与Numpy一样，Tensor也有逐元素操作（Element-Wise），且操作内容相似，但使用函数可能不尽相同。大部分数学运算都属于逐元素操作，其输入与输出的形状相同。常见的逐元素操作可参考表4。\n表4 常见逐元素操作\n\n\n    \n\n\n\n这些操作均会创建新的Tensor，如果需要就地操作，可以使用这些方法的下划线版本，例如abs_。\n\nt = torch.randn(1, 3)t1 = torch.randn(3, 1)t2 = torch.randn(1, 3)#t+0.1*(t1/t2)torch.addcdiv(t, 0.1, t1, t2)#计算sigmoidtorch.sigmoid(t)#将t限制在[0,1]之间torch.clamp(t,0,1)#t+2进行就地运算t.add_(2)\n2.6 归并操作归并操作顾名思义，就是对输入进行归并或合计等操作，这类操作的输入输出形状一般并不相同，而且往往是输入大于输出形状。归并操作可以对整个Tensor，也可以沿着某个维度进行归并。常见的归并操作可参考表5。\n表5 常见归并操作\n\n\n    \n\n\n\n归并操作一般涉及一个dim参数，指定沿哪个维进行归并。另一个参数是keepdim，说明输出结果中是否保留维度1，缺省情况是False，即不保留。\n\n#生成一个含6个数的向量a=torch.linspace(0,10,6)#使用view方法，把a变为2x3矩阵a=a.view((2,3))#沿y轴方向累加，即dim=0b=a.sum(dim=0)   #b的形状为[3]#沿y轴方向累加，即dim=0,并保留含1的维度b=a.sum(dim=0,keepdim=True) #b的形状为[1,3]\n2.7 比较操作比较操作一般是进行逐元素比较，有些是按指定方向比较。常用的比较函数可参考表6。\n表6 常用的比较函数\n\n\n    \n\n\nx=torch.linspace(0,10,6).view(2,3)#求所有元素的最大值torch.max(x)   #结果为10#求y轴方向的最大值torch.max(x,dim=0)  #结果为[6,8,10]#求最大的2个元素torch.topk(x,1,dim=0)  #结果为[6,8,10],对应索引为tensor([[1, 1, 1])\n2.8 矩阵操作机器学习和深度学习中存在大量的矩阵运算，常用的算法有两种：一种是逐元素乘法，另外一种是点积乘法。PyTorch中常用的矩阵函数可参考表7。\n表7 常用矩阵函数\n\n\n    \n\n\n\n1）Torch的dot与Numpy的dot有点不同，Torch中的dot是对两个为1D张量进行点积运算，Numpy中的dot无此限制；2）mm是对2D的矩阵进行点积，bmm对含batch的3D矩阵进行点积运算；3）转置运算会导致存储空间不连续，需要调用contiguous方法转为连续。\n\na=torch.tensor([2, 3])b=torch.tensor([3, 4])torch.dot(a,b)  #运行结果为18x=torch.randint(10,(2,3))y=torch.randint(6,(3,4))torch.mm(x,y)x=torch.randint(10,(2,2,3))y=torch.randint(6,(2,3,4))torch.bmm(x,y)\n2.9 PyTorch与Numpy比较PyTorch与Numpy有很多类似的地方，并且有很多相同的操作函数名称，或虽然函数名称不同但含义相同；当然也有一些虽然函数名称相同，但含义不尽相同。有些很容易混淆，Numpy与Pytorch函数的一些主要区别可参考表8。\n表8 Pytorch与Numpy函数对照表\n\n\n    \n\n\n3. 使用Tensor及Autograd实现机器学习在神经网络中，一个重要内容就是进行参数学习，而参数学习离不开求导，那么PyTorch是如何进行求导的呢？现在大部分深度学习架构都有自动求导的功能，PyTorch也不例外，torch.autograd包就是用来自动求导的。Autograd包为张量上所有的操作提供了自动求导功能，而torch.Tensor和torch.Function为Autograd的两个核心类，它们相互连接并生成一个有向无环图（DAG）。\n本节将使用PyTorch的autograd自动求导包及对应的Tensor，利用自动反向传播来求梯度，无须手工计算梯度。\n1）导入需要的库。\nimport torchimport numpy as npfrom matplotlib import pyplot as plt\n2）生成训练数据，并可视化数据分布情况。\nt.manual_seed(100) dtype = t.float#生成x坐标数据，x为tenor，需要把x的形状转换为100x1x = t.unsqueeze(torch.linspace(-1, 1, 100), dim=1) #生成y坐标数据，y为tenor，形状为100x1，另加上一些噪声y = 3*x.pow(2) +2+ 0.2*torch.rand(x.size())   # 画图，把tensor数据转换为numpy数据plt.scatter(x.numpy(), y.numpy())plt.show()\n\n    \n\n\n3）初始化权重参数。\n# 随机初始化参数，参数w、b为需要学习的，故需requires_grad=Truew = t.randn(1,1, dtype=dtype,requires_grad=True)b = t.zeros(1,1, dtype=dtype, requires_grad=True) \n4）训练模型。\nlr =0.001 # 学习率for ii in range(800):    # 前向传播，并定义损失函数loss    y_pred = x.pow(2).mm(w) + b    loss = 0.5 * (y_pred - y) ** 2    loss = loss.sum()    # 自动计算梯度，梯度存放在grad属性中    loss.backward()    # 手动更新参数，需要用torch.no_grad()，使上下文环境中切断自动求导的计算    with t.no_grad():        w -= lr * w.grad        b -= lr * b.grad    # 梯度清零        w.grad.zero_()        b.grad.zero_()\n5）可视化训练结果。\nplt.plot(x.numpy(), y_pred.detach().numpy(),&#x27;r-&#x27;,label=&#x27;predict&#x27;)#predictplt.scatter(x.numpy(), y.numpy(),color=&#x27;blue&#x27;,marker=&#x27;o&#x27;,label=&#x27;true&#x27;) # true dataplt.xlim(-1,1)plt.ylim(2,6)  plt.legend()plt.show()print(w, b)\n\n    \n\n\n运行结果：tensor([[2.9648]], requires_grad=True) tensor([[2.1145]], requires_grad=True)\n4. PyTorch神经网络工具箱4.1 神经网络核心组件神经网络看起来很复杂，节点很多，层数多，参数更多。但核心部分或组件不多，把这些组件确定后，这个神经网络基本就确定了。这些核心组件包括：1）层：神经网络的基本结构，将输入张量转换为输出张量。2）模型：层构成的网络。3）损失函数：参数学习的目标函数，通过最小化损失函数来学习各种参数。4）优化器：如何使损失函数最小，这就涉及优化器。当然这些核心组件不是独立的，它们之间，以及它们与神经网络其他组件之间有密切关系。为便于读者理解，我们可以把这些关键组件及相互关系，用图1表示。\n\n    \n\n\n图1 神经网络关键组件及相互关系示意图\n\n多个层链接在一起构成一个模型或网络，输入数据通过这个模型转换为预测值，然后损失函数把预测值与真实值进行比较，得到损失值（损失值可以是距离、概率值等），该损失值用于衡量预测值与目标结果的匹配或相似程度，优化器利用损失值更新权重参数，从而使损失值越来越小。这是一个循环过程，当损失值达到一个阀值或循环次数到达指定次数，循环结束。\n4.2 神经网络工具箱nn前面我们使用Autograd及Tensor实现机器学习实例时，需要做不少设置，如对叶子节点的参数requires_grad设置为True，然后调用backward，再从grad属性中提取梯度。对于大规模的网络，Autograd太过于底层和烦琐。为了简单、有效解决这个问题，nn是一个有效工具。在nn工具箱中有两个重要模块：nn.Model、nn.functional。\nnn中的大多数层（Layer）在functional中都有与之对应的函数。nn.functional中函数与nn.Module中的Layer的主要区别是后者继承Module类，会自动提取可学习的参数。而nn.functional更像是纯函数。两者功能相同，且性能也没有很大区别，那么如何选择呢？像卷积层、全连接层、Dropout层等因含有可学习参数，一般使用nn.Module，而激活函数、池化层不含可学习参数，可以使用nn.functional中对应的函数。\n4.2.1 nn.Modulenn.Module是nn的一个核心数据结构，它可以是神经网络的某个层（Layer），也可以是包含多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，生成自己的网络/层。nn中已实现了绝大多数层，包括全连接层、损失层、激活层、卷积层、循环层等，这些层都是nn.Module的子类，能够自动检测到自己的Parameter，并将其作为学习参数，且针对GPU运行进行了cuDNN优化。\n4.2.2 nn.functionalnn中的层，一类是继承了nn.Module，其命名一般为nn.Xxx（第一个是大写），如nn.Linear、nn.Conv2d、nn.CrossEntropyLoss等。另一类是nn.functional中的函数，其名称一般为nn.funtional.xxx，如nn.funtional.linear、nn.funtional.conv2d、nn.funtional.cross_entropy等。从功能来说两者相当，基于nn.Moudle能实现的层，使用nn.funtional也可实现，反之亦然，而且性能方面两者也没有太大差异。不过在具体使用时，两者还是有区别，主要区别如下：\n\n1）nn.Xxx继承于nn.Module，nn.Xxx需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。它能够很好地与nn.Sequential结合使用，而nn.functional.xxx无法与nn.Sequential结合使用。2）nn.Xxx不需要自己定义和管理weight、bias参数；而nn.functional.xxx需要自己定义weight、bias参数，每次调用的时候都需要手动传入weight、bias等参数，不利于代码复用。3）Dropout操作在训练和测试阶段是有区别的，使用nn.Xxx方式定义Dropout，在调用model.eval()之后，自动实现状态的转换，而使用nn.functional.xxx却无此功能。\n\n总的来说，两种功能都是相同的，但PyTorch官方推荐：具有学习参数的（例如，conv2d,linear,batch_norm)采用nn.Xxx方式。没有学习参数的（例如，maxpool、loss func、activation func）等根据个人选择使用nn.functional.xxx或者nn.Xxx方式。\n4.3 优化器PyTorch常用的优化方法都封装在torch.optim里面，其设计很灵活，可以扩展为自定义的优化方法。所有的优化方法都是继承了基类optim.Optimizer，并实现了自己的优化步骤。最常用的优化算法就是梯度下降法及其各种变种，这类优化算法通过使用参数的梯度值更新参数。\n使用优化器的一般步骤为：\n（1）建立优化器实例导入optim模块，实例化SGD优化器，这里使用动量参数momentum（该值一般在（0,1）之间），是SGD的改进版，效果一般比不使用动量规则的要好。\nimport torch.optim as optimoptimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n\n 以下步骤在训练模型的for循环中。\n\n（2）向前传播把输入数据传入神经网络Net实例化对象model中，自动执行forward函数，得到out输出值，然后用out与标记label计算损失值loss。\nout = model(img)loss = criterion(out, label)\n（3）清空梯度缺省情况梯度是累加的，在梯度反向传播前，先需把梯度清零。\noptimizer.zero_grad()\n（4）反向传播基于损失值，把梯度进行反向传播。\nloss.backward()\n（5）更新参数基于当前梯度（存储在参数的.grad属性中）更新参数。\noptimizer.step()\n动态修改学习率参数\n修改参数的方式可以通过修改参数optimizer.params_groups或新建optimizer。新建optimizer比较简单，optimizer十分轻量级，所以开销很小。但是新的优化器会初始化动量等状态信息，这对于使用动量的优化器（momentum参数的SGD）可能会造成收敛中的震荡。所以，这里直接采用修改参数optimizer.params_groups。optimizer.param_groups：长度1的list，optimizer.param_groups[0]：长度为6的字典，包括权重参数、lr、momentum等参数。\nlen(optimizer.param_groups[0])运行结果：6\n动态修改学习率参数示例：\nfor epoch in range(num_epoches):#动态修改参数学习率    if epoch%5==0:        optimizer.param_groups[0][&#x27;lr&#x27;]*=0.1        print(optimizer.param_groups[0][&#x27;lr&#x27;])    for img, label in train_loader:######\n4.4 神经网络构建实例使用PyTorch构建神经网络使用的主要工具（或类）及相互关系如图2所示。如4.2节所述，构建网络层可基于Module类或函数（nn.functional）。\n\n    \n\n\n图2 PyTorch实现神经网络的主要工具及相互关系\n\n下面将利用神经网络完成对手写数字进行识别的实例，来说明如何借助nn工具箱来实现一个神经网络，并对神经网络有个直观了解。主要步骤有：\n\n1）利用PyTorch内置函数mnist下载数据集；2）利用torchvision对数据进行预处理，调用torch.utils建立一个数据迭代器；3）可视化源数据；4）利用nn工具箱构建神经网络模型；5）实例化模型，并定义损失函数及优化器；6）训练模型；7）可视化结果。\n\n神经网络的结构如图3所示，使用两个隐含层，每层激活函数为ReLU，最后使用torch.max(out,1)找出张量out最大值对应索引作为预测值。\n\n    \n\n\n图3 实例使用的神经网络结构图\n\n4.4.1 数据准备（1）导入模块\nimport numpy as npimport torch# 导入 PyTorch 内置的 mnist 数据from torchvision.datasets import mnist #导入预处理模块import torchvision.transforms as transformsfrom torch.utils.data import DataLoader#导入nn及优化器import torch.nn.functional as Fimport torch.optim as optimfrom torch import nn#导入画图库import matplotlib.pyplot as plt\n（2）定义超参数\ntrain_batch_size = 64test_batch_size = 128learning_rate = 0.01num_epoches = 20lr = 0.01momentum = 0.5\n（3）下载数据并对数据进行预处理\n#定义预处理函数，这些预处理依次放在Compose函数中。transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5], [0.5])])#下载数据，并对数据进行预处理train_dataset = mnist.MNIST(&#x27;./data&#x27;, train=True, transform=transform, download=True)test_dataset = mnist.MNIST(&#x27;./data&#x27;, train=False, transform=transform)#dataloader是一个可迭代对象，可以想迭代器一样使用。train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n\n1）transforms.Compose可以把一些转换函数组合在一起；2）Normalize([0.5],[0.5])对张量进行归一化，这里两个0.5分别表示对张量进行归一化的全局平均值和方差。因图像是灰色的只有一个通道，如果有多个通道，需要有多个数字，如3个通道，应该是Normalize([m1,m2,m3],[n1,n2,n3])；3）download参数控制是否需要下载，如果./data目录下已有MNIST，可选择False；4）用DataLoader得到生成器，这可节省内存；5）shuffle=True表示对数据进行打乱操作。\n\n4.4.2 可视化源数据examples = enumerate(test_loader)batch_idx, (example_data, example_targets) = next(examples)fig = plt.figure()for i in range(6):  plt.subplot(2,3,i+1)  plt.tight_layout()  plt.imshow(example_data[i][0], cmap=&#x27;gray&#x27;, interpolation=&#x27;none&#x27;)  plt.title(&quot;Ground Truth: &#123;&#125;&quot;.format(example_targets[i]))  plt.xticks([])  plt.yticks([])plt.show()\n\n    \n\n\n4.4.3 模型构建（1）构建网络\nclass Net(nn.Module):    &quot;&quot;&quot;    使用sequential构建网络，Sequential()函数的功能是将网络的层组合到一起    &quot;&quot;&quot;    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):        super(Net, self).__init__()        self.layer1 = nn.Sequential(nn.Linear(in_dim, n_hidden_1),nn.BatchNorm1d(n_hidden_1))        self.layer2 = nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2),nn.BatchNorm1d (n_hidden_2))        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2, out_dim))    def forward(self, x):        x = F.relu(self.layer1(x))        x = F.relu(self.layer2(x))        x = self.layer3(x)        return x\n（2）实例化网络\n#检测是否有可用的GPU，有则使用，否则使用CPUdevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)#实例化网络model = Net(28 * 28, 300, 100, 10)model.to(device)# 定义损失函数和优化器criterion = nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n4.4.4 模型训练这里使用for循环进行迭代，其中包括对训练数据的训练模型，然后用测试数据的验证模型。\n（1）模型训练\n# 开始训练losses = []acces = []eval_losses = []eval_acces = []for epoch in range(num_epoches):    train_loss = 0    train_acc = 0    #将module设置为训练模式    model.train()\t#动态修改参数学习率    if epoch%5==0:        optimizer.param_groups[0][&#x27;lr&#x27;]*=0.1    for img, label in train_loader:        img=img.to(device)        label = label.to(device)        img = img.view(img.size(0), -1)        # 前向传播        out = model(img)        loss = criterion(out, label)        # 反向传播        optimizer.zero_grad()        loss.backward()        optimizer.step()        # 记录误差        train_loss += loss.item()        # 计算分类的准确率        _, pred = out.max(1)        num_correct = (pred == label).sum().item()        acc = num_correct / img.shape[0]        train_acc += acc    losses.append(train_loss / len(train_loader))    acces.append(train_acc / len(train_loader))    # 在测试集上检验效果    eval_loss = 0    eval_acc = 0\t# 将模型改为预测模式（会将所有的training属性设置为False）    model.eval()    for img, label in test_loader:        img=img.to(device)        label = label.to(device)        img = img.view(img.size(0), -1)        out = model(img)        loss = criterion(out, label)        # 记录误差        eval_loss += loss.item()        # 记录准确率        _, pred = out.max(1)        num_correct = (pred == label).sum().item()        acc = num_correct / img.shape[0]        eval_acc += acc    eval_losses.append(eval_loss / len(test_loader))    eval_acces.append(eval_acc / len(test_loader))    print(&#x27;epoch: &#123;&#125;, Train Loss: &#123;:.4f&#125;, Train Acc: &#123;:.4f&#125;, Test Loss: &#123;:.4f&#125;, Test Acc: &#123;:.4f&#125;&#x27;          .format(epoch, train_loss / len(train_loader), train_acc / len(train_loader),                      eval_loss / len(test_loader), eval_acc / len(test_loader)))\n迭代结果：\nepoch:0, Train Loss:0.9885, Train Acc:0.7987, Test Loss:0.5235, Test Acc:0.9073epoch:1, Train Loss:0.4622, Train Acc:0.9034, Test Loss:0.3412, Test Acc:0.9277epoch:2, Train Loss:0.3403, Train Acc:0.9208, Test Loss:0.2666, Test Acc:0.9376epoch:3, Train Loss:0.2761, Train Acc:0.9335, Test Loss:0.2217, Test Acc:0.9458epoch:4, Train Loss:0.2379, Train Acc:0.9410, Test Loss:0.1946, Test Acc:0.9511epoch:5, Train Loss:0.2174, Train Acc:0.9467, Test Loss:0.1922, Test Acc:0.9510epoch:6, Train Loss:0.2137, Train Acc:0.9482, Test Loss:0.1891, Test Acc:0.9517epoch:7, Train Loss:0.2120, Train Acc:0.9475, Test Loss:0.1872, Test Acc:0.9529epoch:8, Train Loss:0.2091, Train Acc:0.9487, Test Loss:0.1843, Test Acc:0.9530epoch:9, Train Loss:0.2067, Train Acc:0.9491, Test Loss:0.1831, Test Acc:0.9530epoch:10, Train Loss:0.2043, Train Acc:0.9498, Test Loss:0.1841, Test Acc:0.9540epoch:11, Train Loss:0.2046, Train Acc:0.9498, Test Loss:0.1826, Test Acc:0.9545epoch:12, Train Loss:0.2037, Train Acc:0.9497, Test Loss:0.1814, Test Acc:0.9542epoch:13, Train Loss:0.2038, Train Acc:0.9494, Test Loss:0.1813, Test Acc:0.9537epoch:14, Train Loss:0.2044, Train Acc:0.9500, Test Loss:0.1805, Test Acc:0.9547epoch:15, Train Loss:0.2034, Train Acc:0.9497, Test Loss:0.1828, Test Acc:0.9536epoch:16, Train Loss:0.2026, Train Acc:0.9508, Test Loss:0.1815, Test Acc:0.9533epoch:17, Train Loss:0.2025, Train Acc:0.9504, Test Loss:0.1838, Test Acc:0.9536epoch:18, Train Loss:0.2034, Train Acc:0.9501, Test Loss:0.1827, Test Acc:0.9542epoch:19, Train Loss:0.2034, Train Acc:0.9505, Test Loss:0.1804, Test Acc:0.9528\n该神经网络的结构比较简单，只用了两层，也没有使用Dropout层，迭代20次，测试准确率达到98%左右，效果还可以。不过，还是有提升空间，如果采用cnn、Dropout等层，应该还可以提升模型性能。\n（2）可视化训练及测试损失值\nplt.title(&#x27;trainloss&#x27;)plt.plot(np.arange(len(losses)), losses)plt.legend([&#x27;Train Loss&#x27;], loc=&#x27;upper right&#x27;)plt.show()\n\n    \n\n\n5. PyTorch数据处理工具箱上节利用PyTorch的torchvision、data等包，下载及预处理MNIST数据集。数据下载和预处理是机器学习、深度学习实际项目中耗时又重要的任务，尤其是数据预处理，关系到数据质量和模型性能，往往要占据项目的大部分时间。好在PyTorch为此提供了专门的数据下载、数据处理包，使用这些包，可极大地提高我们的开发效率及数据质量。\nPyTorch涉及数据处理（数据装载、数据预处理、数据增强等）主要工具包及相互关系如图4所示。\n\n    \n\n\n图4 PyTorch主要数据处理工具\n\n图4的左边是torch.utils.data工具包，它包括以下4个类。\n\n1）Dataset：是一个抽象类，其他数据集需要继承这个类，并且覆写其中的两个方法（_getitem、len__）；2）DataLoader：定义一个新的迭代器，实现批量（batch）读取，打乱数据（shuffle）并提供并行加速等功能；3）random_split：把数据集随机拆分为给定长度的非重叠的新数据集；4）*sampler：多种采样函数。\n\n图4中间是PyTorch可视化处理工具（Torchvision），其是PyTorch的一个视觉处理工具包，独立于PyTorch，需要另外安装。它包括4个类，各类的主要功能如下。\n\n1）datasets：提供常用的数据集加载，设计上都是继承自torch.utils.data.Dataset，主要包括MMIST、CIFAR10/100、ImageNet和COCO等；2）models：提供深度学习中各种经典的网络结构以及训练好的模型（如果选择pretrained=True），包括AlexNet、VGG系列、ResNet系列、Inception系列等；3）transforms：常用的数据预处理操作，主要包括对Tensor及PIL Image对象的操作；4）utils：含两个函数，一个是make_grid，它能将多张图片拼接在一个网格中；另一个是save_img，它能将Tensor保存成图片。\n\n5.1 utils.datautils.data包括Dataset和DataLoader。torch.utils.data.Dataset为抽象类，自定义数据集需要继承这个类，并实现两个函数，一个是__len__，另一个是__getitem，前者提供数据的大小（size），后者通过给定索引获取数据和标签。\\getitem__一次只能获取一个数据，所以需要通过torch.utils.data.DataLoader来定义一个新的迭代器，实现batch读取。下面定义一个简单的数据集，然后通过具体使用Dataset及DataLoader，获得一个直观的认识。\n（1）导入需要的模块。\nimport torchfrom torch.utils import dataimport numpy as np\n（2）定义获取数据集的类。\n该类继承基类Dataset，自定义一个数据集及对应标签。\nclass TestDataset(data.Dataset):#继承Dataset    def __init__(self):        self.Data=np.asarray([[1,2],[3,4],[2,1],[3,4],[4,5]])#一些由2维向量表示的数据集        self.Label=np.asarray([0,1,0,1,2])#这是数据集对应的标签    def __getitem__(self, index):        #把numpy转换为Tensor        txt=torch.from_numpy(self.Data[index])        label=torch.tensor(self.Label[index])        return txt,label     def __len__(self):        return len(self.Data)\n（3）获取数据集中数据。\nTest=TestDataset()print(Test[2])  #相当于调用__getitem__(2)print(Test.__len__())\n输出结果：\n(tensor([2, 1]), tensor(0))5\n以上数据以tuple返回，每次只返回一个样本。实际上，Dateset只负责数据的抽取，调用一次__getitem__只返回一个样本。如果希望批量处理（batch），还要同时进行shuffle和并行加速等操作，可选择DataLoader。DataLoader的格式为：\ndata.DataLoader(    dataset,    batch_size=1,    shuffle=False,    sampler=None,    batch_sampler=None,    num_workers=0,    collate_fn=&lt;function default_collate at 0x7f108ee01620&gt;,    pin_memory=False,    drop_last=False,    timeout=0,    worker_init_fn=None,)\n主要参数说明：\n\ndataset：加载的数据集；batch_size：批大小；shuffle：是否将数据打乱；sampler：样本抽样；num_workers：使用多进程加载的进程数，0代表不使用多进程；collate_fn：如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可；pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些；drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃。\n\ntest_loader = data.DataLoader(Test,batch_size=2,shuffle=False,num_workers=0)for i,traindata in enumerate(test_loader):    print(&#x27;i:&#x27;,i)    Data,Label=traindata    print(&#x27;data:&#x27;,Data)    print(&#x27;Label:&#x27;,Label)\n运行结果：\ni: 0data: tensor([[1, 2],        [3, 4]])Label: tensor([0, 1])i: 1data: tensor([[2, 1],        [3, 4]])Label: tensor([0, 1])i: 2data: tensor([[4, 5]])Label: tensor([2])\n可以看出，这是批量读取，可以像使用迭代器一样使用它，比如对它进行循环操作。不过由于它不是迭代器，我们可以通过iter命令将其转换为迭代器。\ndataiter=iter(test_loader)imgs,labels=next(dataiter)\n5.2 torchvision如图4所示，torchvision有4个功能模块：model、datasets、transforms和utils，上文中使用datasets下载了一些经典数据集，本节介绍如何使用datasets的ImageFolder处理数据集，以及如何使用transforms对数据进行预处理、增强等。\n5.2.1 ImageFolder当文件依据标签处于不同文件下时，如：─── data    ├── zhangliu    │   ├── 001.jpg    │   └── 002.jpg    ├── wuhua    │   ├── 001.jpg    │   └── 002.jpg    ……………..可以利用torchvision.datasets.ImageFolder来直接构造出dataset：\nloader = datasets.ImageFolder(path, transform=None)loader = data.DataLoader(dataset)\nImageFolder会将目录中的文件夹名自动转化成序列，当DataLoader载入时，标签自动就是整数序列了。\n5.2.2 transformstransforms提供了对PIL Image对象和Tensor对象的常用操作。\n（1）对PIL Image的常见操作\n\nScale/Resize：调整尺寸，长宽比保持不变；CenterCrop、RandomCrop、RandomSizedCrop：裁剪图片，CenterCrop和RandomCrop在crop时是固定size，RandomResizedCrop则是random size的crop；Pad：填充；ToTensor：把一个取值范围是[0,255]的PIL.Image转换成Tensor。形状为（H,W,C）的Numpy.ndarray转换成形状为[C,H,W]，取值范围是[0,1.0]的torch.FloatTensor；RandomHorizontalFlip：图像随机水平翻转，翻转概率为0.5；RandomVerticalFlip：图像随机垂直翻转；ColorJitter：修改亮度、对比度和饱和度。\n\n（2）对Tensor的常见操作\n\nNormalize：标准化，即，减均值，除以标准差；ToPILImage：将Tensor转为PIL Image。\n\n如果要对数据集进行多个操作，可通过Compose将这些操作像管道一样拼接起来，类似于nn.Sequential。\ntransforms.Compose([    #将给定的 PIL.Image 进行中心切割，得到给定的 size，    #size 可以是 tuple，(target_height, target_width)。    #size 也可以是一个 Integer，在这种情况下，切出来的图片形状是正方形。                transforms.CenterCrop(10),    #切割中心点的位置随机选取    transforms.RandomCrop(20, padding=0),    #把一个取值范围是 [0, 255] 的 PIL.Image 或者 shape 为 (H, W, C) 的 numpy.ndarray，    #转换为形状为 (C, H, W)，取值范围是 [0, 1] 的 torch.FloatTensor    transforms.ToTensor(),    #规范化到[-1,1]    transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))])\n\n本内容旨在为读者提供PyTorch的一些主要基础内容，仅包含PyTorch完整内容的一部分，欲学习更多PyTorch相关内容，请查看：https://pytorch.org/docs/stable/index.html\n\n","categories":["Learning Materials"]},{"title":"µLayer：Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization","url":"/2022/10/20/%CE%BCLayer/","content":"\n 来        源：EuroSys ‘19: Proceedings of the Fourteenth EuroSys Conference 2019 - March 2019 - Article No.: 45 - Pages 1–15 https://doi.org/10.1145/3302424.3303950\n原文速递：µLayer: Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization\n\n1. Background &amp; Motivation随着神经网络（NN）的发展，移动服务供应商在移动设备上部署了许多服务，比如Google的手写转文字功能、Google的即时翻译、YouTube的视频分割以及Google、Apple等公司的虚拟助手。而为满足这些应用的时延要求，传统的做法是依赖于丰富的云端资源实现的。然而，随着高性能的移动端SoC的出现，完全依赖移动端设备进行神经网络推理成为可能。由于强响应性对移动服务至关重要，高效利用移动设备资源是十分必要的。\n通过更好地利用移动设备硬件资源，对轻量神经网络的几乎实时响应成为可能，但由于时延问题，大部分移动服务仍然依赖云端资源，比如所有实际的虚拟助手都将工作卸载到云端进行语音识别。而现在的Soc上具有多种资源，实现快速准确的设备上推理是可行的。\n2. Contributions\n1）发现协同实现神经网络单层加速的潜力。该文发现现代SoC上的CPU和GPU具有平衡的吞吐量，使CPU-GPU协同神经网络加速成为可能；\n2）提出新的协同单层加速机制。通过从多维度对神经网络执行进行优化，获得相比先进方法的时延降低；\n3）设计μLayer系统。提出一个优化的神经网络架构μLayer用于减小神经网络的执行时延。\n\n3. Design Details3.1 协同单层加速如图1(a)、(b)所示，现有移动神经网络框架由于只利用单一处理器限制了性能，而本文提出的框架能够在CPU和GPU上协同执行神经网络中的层。\n\n    \n\n\n图1 设备上推理机制对比\n\n该文提出通道级工作负载分配方法如图2所示，其可以避免引入CPU和GPU间的冗余计算。\n\n    \n\n\n图2 一层的通道级工作负载分配\n\n对于卷积层和全连接层，filter被以p:(1-p)的比例分配给各处理器，完整的输入数据将被分配给各处理器进行计算，最后计算出的输出通道被合并生成输出数据。对于池化层，输入数据被分配给各处理器，每个处理器对自己的数据部分使用全局函数进行计算，最后的结果将被合并。\n3.2 针对处理器的量化为了优化每个处理器的性能表现，该文考虑了2种量化方案：16位半精度浮点数（F16）和8位线性量化（QUInt8）。因为GPU在实现高吞吐量的浮点运算方面有本地硬件支持，对GPU使用16位半精度浮点数量化。CPU更擅长处理QUInt8类型的数据，因此对其使用8位线性量化。\n3.3 分支分配现有的一些神经网络（如GoogLeNet）包含分支结构，如图3。这些分支可能会影响通道级工作负载分配的性能。而多分支结构的时延可通过并行运行多个分支来隐藏，即同一分支上的多个层将在一个处理器上运行。\n\n    \n\n\n图3 多分支结构神经网络示例\n\n3.4 μLayer运行时架构如图4，μLayer由3个部件组成：一个神经网络分割器、一个时延预测器和一个神经网络执行器。\n\n    \n\n\n图4 μLayer运行时架构\n\n神经网络分割器负责协同执行计划的生成，对于每一层，神经网络分割器决定通道分割的比例p(0\\leq p \\leq 1)，神经网络分割器根据时延预测器找出最优的p值。\n时延预测器能够根据每层神经网络的参数（如输入尺寸和filter尺寸）以及p值预测该层的执行时延。该文在Neurosurgeon(Paper Link)设计的时延预测器基础上引入参数p，首先对CPU和GPU独立执行运用指数回归方法拟合模型，然后通过给定的p值调整时延预测值。时延预测器将时延预测值反馈给神经网络分割器选出分割方案，最后神经网络执行器根据最优分割方案在CPU和GPU间协同进行神经网络推理。\n4. Comments该文致力于通过CPU-GPU协同执行的方式进行高效神经网络推理，设计了一种层级协同推理方法，能够取得一定的性能提升，但仍存在一些问题：\n\n1）该系统中的时延预测器采用机器学习拟合函数，与Neurosurgeon相同，存在精度低的问题；\n2）该CPU-GPU协同方法是层粒度的，存在高频同步的问题，而由于时延预测器的不准确性会造成CPU，GPU的运行差异，同步开销可能会成为时延的主体部分。\n\n","categories":["Paper Notes"]},{"title":"深度学习方法探析——卷积神经网络","url":"/2022/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%8E%A2%E6%9E%90%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","content":"1. 卷积神经网络的组成1.1 卷积 (Convolution)1.1.1 卷积运算在数字图像处理中，我们用到了一种边缘检测算子，图1为一种垂直边缘检测的过程。依次在图像上圈出9个单元，将检测算子中的每个对应单元与圈出的单元相乘，最后将所有乘积结果相加，作为一个输出。依次移动框选区域，直至对整个图像进行运算。从输出图像可以看出，该运算过程检测出了图像中的垂直边缘，这个运算的过程就可以理解为卷积神经网络中最基本的卷积运算。使用不同的算子可以对图像中的不同边缘进行检测，甚至是45°、75°等特殊边缘，在边缘检测领域也曾提出过Sobel、Scharr等算子，其性能根据应用场景有所差别。\n\n    \n\n\n图1 数字图像处理中垂直边缘检测过程\n\n卷积神经网络中的卷积运算，与上述操作的不同之处在于：卷积神经网络将算子中的9个固定的值当做9个参数来进行训练，如此便能得到任一种需要的算子，而不是只有固定的一种。卷积神经网络通常将这个算子称为过滤器，或卷积核。\n1.1.2 Padding如果我们有一个n×n的图像，使用f×f的过滤器进行卷积，得到的输出图像维度为(n-f+1)×(n-f+1)。图1中的例子是对6×6的图像使用3×3的过滤器进行卷积，得到了4×4的输出。这会导致两个缺点：1）每次对图像进行卷积操作后，图像都会缩小；2）图像边缘的像素只在少数卷积运算中被使用，这导致了图像边缘的许多信息丢失。为了解决这些缺点，Padding操作在图像的边缘填充一些数据，此时，输出的图像维度变为(n+2p-f+1)×(n+2p-f+1)，其中，p为padding操作增加的层数。例如，在图1中的原始图像上加上1层数据，变为8×8的图像，经过卷积运算后，输出为6×6的图像，与原始图像大小相同。Padding操作通常有两种选择：Valid卷积和Same卷积。Valid卷积指不对原始图像进行填充，输出维度为(n-f+1)×(n-f+1)的图像；Same卷积指对图像进行填充，输出维度为(n+2p-f+1)×(n+2p-f+1)的图像，并使其与原始图像大小相等。\n1.1.3 卷积步长在进行卷积操作时，还有一个重要的参数要进行选择：卷积步长(stride)。卷积步长指过滤器每次在图像上移动的格数。设置卷积步长后，输出的图像维度为\\lfloor\\frac{n+2p-f}{s}+1\\rfloor\\times\\lfloor\\frac{n+2p-f}{s}+1\\rfloor ，其中，s为卷积的步长。\n1.1.4 三维卷积彩色图像有R、G、B三个通道，它的维度不再是之前二维的n×n，变为三维的n×n×3，之前的卷积操作也应相应地变为三维卷积。三维卷积采用一与原图像通道数相同的三维过滤器进行卷积，如图2所示。\n\n    \n\n\n图2 三维卷积过程\n\n三维卷积的过程为：将三维过滤器与原图像进行逐层二维卷积，并将得到的三层数据按位置相加，得到最终的二维输出。可能有一个过滤器是用来检测水平边缘的，但还想同时检测图像的其他特征。这时就可以增加过滤器的数量，每个过滤器负责检测一个特征，如图3所示，最终输出的通道数与过滤器的个数相等。也就是说，要检测多少特征，就会有多少个通道输出。\n\n    \n\n\n图3 多特征三维卷积\n\n1.1.5 卷积网络单层卷积神经网络的前向传播过程如图4所示，可与普通的神经网络前项传播类比。卷积操作可以看做原来的特征权重W所做的操作，对每个滤波器卷积后得到的输出加上相应的偏差b后，通过一个非线性激活函数g(z)输出。\n\n    \n\n\n图4 卷积神经网络前向传播过程\n\n现将卷积神经网络中l层各阶段的维度总结到表1中。\n表1 卷积神经网络l层各阶段的维度\n\n\n    \n\n\n其中，n_H^{[l]}=\\lfloor\\frac{n_H^{[l-1]}+2p^{[l]}-f^{l}}{s^{[l]}}+1\\rfloor,n_H^{[l]}=\\lfloor\\frac{n_W^{[l-1]}+2p^{[l]}-f^{l}}{s^{[l]}}+1\\rfloor。若有m个数据输入，则输出A^{[l]}的维度为m\\times n_H^{[l]}\\times n_W^{[l]}\\times n_C^{[l]} 。\n1.1.6 卷积的优势与普通神经网络相比，采用卷积的神经网络主要有两个优势：参数共享和稀疏连接。1) 参数共享在使用普通的神经网络处理大量的特征时，由于两层的节点全部紧密相连，特征矩阵W中的参数个数为两层节点数的乘积，这可能有上千万的数量级。而使用卷积运算，无论前后两层有多少数据，需要训练的参数都只有过滤器的大小。这是由于，一个特征检测器如果适用于图片的某个区域，那么它也可能适用其他区域，而不需要额外训练其他特征检测器，做到参数的共享。2) 稀疏连接稀疏连接指每个输出都只与过滤器包含的范围有关，而与其他输入特征无关。神经网络通过参数共享和稀疏连接两种机制减少参数，以此预防过拟合。同时，这使得卷积神经网络具有平移不变的属性。即使图像移动了几个像素，但其仍然有相似的特征，通过参数共享可以获得同样的输出标记，从而增强网络的鲁棒性。\n1.2 池化 (Pooling)卷积神经网络除了有卷积层外，通常还使用池化层来缩减模型的大小，提高计算速度，同时提高所提取特征的鲁棒性。池化操作通常可分为两种类型：最大池化和平均池化。池化操作和卷积操作类似，可以使用一个过滤器，根据设定的步长和padding值对原图像进行运算。最大池化进行的操作是将该过滤器范围内的数据最大值进行输出，平均池化进行的操作是对过滤器范围内的数据求平均值作为输出。对最大池化可以如下理解：若在图像的某个区域检测到了某个特征，池化输出在该部分会有一个较大的值。虽然，池化层作为卷积神经网络的一部分，但它没有任何需要调整的参数，一旦设定好f和s，其就是一个固定操作。池化的常用参数为f = s = 2，这可以使上一层的长和宽都减小一倍。\n1.3 全连接层 (Fully Connected)全连接层的操作如图5所示，FC1将前面卷积池化后得到的数据进行展平，之后就与普通的深层神经网络相同，通过参数W和b进行前向传播，最后通过softmax层或sigmoid函数输出。由于两个全连接层中的节点紧密相连，这就是所谓的“全连接”。\n\n    \n\n\n图5 全连接层\n\n2. 经典卷积神经网络2.1 LeNet-5LeNet-5是在1998年被提出的，一个LeNet的示例如图6所示。由于LeNet提出时主要用于处理灰度图像，这里采用一32×32×1的图像作为输入。LeNet的网络形式为：先卷积层和池化层交替，最后由几个全连接层输出预测结果。\n\n    \n\n\n图6 LeNet-5示例\n\n2.2 Alexnet在一张227×227像素的图片上采用Alexnet网络的示例如7所示。Alexnet和LeNet非常相似，但Alexnet要大得多，LeNet有大约6万个参数，AlexNet包含约6000万个参数。因此，Alexnet表现得比LeNet更出色。\n\n    \n\n\n图7 Alexnet示例\n\n2.3 VGG-16VGG-16是一种专注于构建卷积层的网络，它的一大优点是简化了神经网络结构，其示例如图8所示。VGG-16中的16指在这个网络中共包含16个卷积层和全连接层，虽然它规模很大，包含约1.38亿个参数，但其具有相当规整的结构：几个卷积层后接一个池化层，并且每次卷积的过滤器数量成倍增长，每次池化操作都将图像的大小缩减一倍。随着网络的加深，图像大小逐渐减小，深度逐渐增加。\n\n    \n\n\n图8 VGG-16示例\n\n3. 先进的卷积神经网络结构3.1 ResNetResNet全称为Residual Networks，意为残差网络。ResNet是由残差块组成的网络，一个残差块如图9所示，残差块将a^{[l]}跨过计算a^{[l+1]}送至下一层，此时，a^{[l+2]}就变成\n\n    \n\n\n\n    \n\n\n图9 残差块\n\n普通网络（Plain Network）在比较深时可能会出现梯度消失和梯度爆炸的问题，影响网络的性能，而残差网络可以很好地应对深度较大的网络，残差网络所做的事就是将浅层的信息传递到更深层的网络中去，可能远比一两层多。当网络层数很多时，如果为了应对梯度消失和梯度爆炸，对网络使用正则化，在深层的W^{[l]}会十分接近0，为了方便起见，b^{[l]}设为0，将(1)式展开为\n\n    \n\n\n在很深的层l+2上，将W^{[l+2]}当做0看待，(2)式变为\n\n    \n\n\n其中，由于激活函数采用ReLu，故经过激活函数后得到a^{[l]}，实现了信息从浅层到深层的传递，因此，可以训练更深层的网络，而不降低网络的效率。值得注意的是，残差网络因为采用网络的跳跃连接，为了保证维度合适，采用了很多same卷积。\n3.2 Google Inception网络在构建卷积神经网络时，我们常常需要决定很多超参数，比如过滤器的大小、是否需要池化层等等。Inception网络所做的事就是，让网络代替人去做这些选择，虽然网络变得复杂，但网络表现可以得到提高。Inception模块如图10所示，它将多种操作得到的结果堆叠起来，为了保证维度相符，全部采用same卷积。\n\n    \n\n\n图10 Inception模块\n\n这里采用了一种1×1的卷积形式，现对其进行分析。对一28×28×192的输入数据直接应用5×5的same卷积，如图11(a)所示。其总共要进行28×28×32×5×5×192≈1.2亿次乘法运算。\n\n    \n\n\n图11 有无瓶颈层卷积对比\n\n图11(b)为采用了瓶颈层的卷积，第一次卷积操作进行了28×28×16×192≈240万次乘法运算，第二次卷积操作进行了28×28×32×5×5×16≈1000万次乘法运算，加起来共约1240万，仅为原来的十分之一。采用1×1卷积的原因，我认为是1×1卷积不会对原图像的特征起到影响，但却起到了压缩规模的作用，通过后续的卷积运算还可以保持原来的效果。将许多Inception模块组合起来就形成了GoogLeNet网络，如图12所示。网络中存在一些分支，它们起到一些调整的作用，使隐藏层也参与特征计算，防止网络发生过拟合。\n\n    \n\n\n图12 GoogLeNet网络\n\n4. 目标检测4.1 目标定位目标分类只需要输出一个预测值即可，但目标定位不仅要完成分类，还要确定目标在图像中的位置，并标记出来，如图13所示。\n\n    \n\n\n图13 目标定位\n\n目标分类只需要softmax层输出每个类别的概率即可，目标定位问题需要对该层的输出进行更改，增加一些辅助定位的参数，修改后的输出为\n\n    \n\n\n其中，p_c用来标记图片中是否包含要检测的对象，b_x、b_y表示目标的中心点坐标，b_h、b_w表示目标的高度和宽度，c为目标的类别，0表示未检测到该目标。当p_c为1时，代价函数将所有参数考虑进去；当p_c为0时，仅考虑p_c的准确度。\n4.2 基于滑动窗口的目标检测比较简单的滑动窗口目标检测过程为：选择一种大小的窗口放在图像的初始位置，裁切下窗口中的图片，送入卷积神经网络中进行预测。依次移动滑动窗口，直至遍历整张图片。然后再改变窗口大小，重复上述过程，总有一个窗口可以识别到目标。但这种方法滑动窗口很慢，并且运算量巨大。为了解决这一问题，可以使用滑动窗口的卷积实现。首先，需要将卷积神经网络中的所有层都转化为卷积操作，也就是要将全连接层转化为卷积层，图14 (a)为一个卷积层转化为全连接层的过程，将其转化为卷积层只需采用大小相同的过滤器对其进行卷积操作，得到1×1的维度，如图14 (b)所示。\n\n    \n\n\n图14 全连接层转化为卷积层\n\n假设一个模型的滑动窗口大小为14×14×3，每次滑动窗口裁切后进行运算的过程如图15 (a)所示。在如图15(b)所示的16×16图片运用14×14的窗口进行卷积运算要进行4次，可以发现产生了许多重复的计算。滑动窗口的卷积实现直接对16×16×3的图片应用原卷积操作，第一个滑动窗口涉及到的计算区域在图15(b)中用绿色标出，可以看到，对第一个滑动窗口的运算只体现在了输出的2×2×4模块的左上角。同理，其余三次运算体现为另外三个角落上。因此，只需对原图进行一次卷积神经运算就可以得到运算4次滑动窗口的效果，减少了很多运算。\n\n    \n\n\n(a)对滑动窗口大小区域运行卷积神经网络\n\n\n    \n\n\n(b)滑动窗口的卷积实现\n\n图15 普通滑动窗口和滑动窗口的卷积实现\n\n\n\n4.3 YOLO (You Only Look Once)算法4.3.1 Bounding Box预测采用滑动窗口的目标检测虽然能够检测到目标，但那些边框没办法完美匹配目标的位置。Bounding Box预测是YOLO算法中用于目标分类和定位的方法，它可以精确预测目标的位置。如图16所示，Bounding Box将图片分割成若干部分，每个部分对应一个与(4)式相同的label，图中的输出为9×9×k (k为单个label的长度)。在训练时，喂给卷积神经网络图像，并将对应的label作为监督学习目标进行学习，目标仅属于中心点所在的格子。由于采用了滑动窗口的卷积实现，不用对每个格子单独进行一次运算，而是对整张图片进行一次卷积就可以输出所有预测值，减少了计算量，使得YOLO可以用于实时目标检测。\n\n    \n\n\n图16 Bounding Box预测\n\n4.3.2 非极大值抑制 (Non-Max Suppression)当运行目标检测算法时，可能目标位于不同格子中的部分被各格子识别为中心点在其中，这将导致一个目标的多次识别，非极大值抑制做的就是防止目标的多次识别。它首先找出概率最大的矩形，视其为检测到目标，然后将与其交并比较大的矩形抑制掉。然后再找剩下概率最大的矩形，抑制掉与其交并比大的矩形，如此重复，直到遍历所有矩形。\n4.3.3 Anchor Boxes前述操作只能在每个格子里检测一个目标，但如图17所示的情况会出现多个目标归属于同一格的情况，使用Anchor Boxes可以实现在一个格子里检测多个目标。\n\n    \n\n\n图17 一格包含多个目标示意\n\n\n    \n\n\n图18 Anchor boxes\n\nAnchor boxes的思路是这样的：在图17的情况中，我们可以预先设定两个Anchor boxes，如图18所示，实际情况可能需要设定更多的Anchor boxes。这种情况下输出的label就不能和 (4)式相同了，应更改为\n\n    \n\n\n其将 (4)式重复了2遍，如果设定更多Anchor boxes，则需要重复更多次数。在训练时，需要将目标分配到所在格子内与之交并比更大的Anchor box项内，每个Anchor box项应是独立的，不受其他项的影响。YOLO算法的流程为：使用卷积神经网络对图像进行推理后，每个格子里都会得到与Anchor boxes数量相等的项。我们需要先去除神经网络预测不存在对象的格子中的方框，此时剩下的方框中的多个Anchor boxes项可能都会识别到同一目标，再对剩下的格子应用非极大值抑制，得到真正的目标位置。\n","categories":["Learning Materials"]},{"title":"深度学习方法探析——优化算法","url":"/2022/10/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%8E%A2%E6%9E%90%E2%80%94%E2%80%94%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/","content":"1.引言深层学习在应用于大量数据时需要花费大量的训练时间，这成为了阻碍深度学习发展的一个困难。本文总结了几种可以加速深度学习模型训练过程的优化算法，采用Mini-batch优化算法可以有效缓解迭代缓慢的问题，在部分数据处理的过程中持续更新参数；Momentum算法、RMSprop算法和Adam算法能够加速梯度的下降过程，减小梯度在不必要方向上的抖动。\n2. Mini-batch梯度下降法之前我们在反向传播中所用的梯度下降算法，称为batch梯度下降法，在每次计算完毕所有样本对应的dW和db后，再将W和b的值进行更新。这对于数据量小的数据集没有问题，应用向量化的方式也可以很快地进行迭代，但当数据量巨大时，对每个矩阵的处理时间同样是巨大的，这意味着经过很长一段时间才能进行一次梯度下降，这会严重影响模型训练的速度。Mini-batch的方法将很大的数据集分解为多个子训练集，如图1所示，将原来的大数据集随机打乱排序，再划分为具有相同数据量的子集，以保证数据的均匀性，本文将第t个子集表示为X^{\\{t\\}}，对应的标签为Y^{\\{t\\}}。\n\n    \n\n\n图1 mini-batch子集划分\n\nMini-batch方法对每个子集逐一进行梯度下降，每将所有子集遍历一次称为一代(epoch)，与原来的batch方法相比，mini-batch梯度下降法的梯度下降更新参数次数提高了子集数倍数，梯度下降算法也能更快执行。Mini-batch梯度下降法的代价函数随迭代次数的变化与batch法是不同的，图2为某测试数据集，分别对其使用mini-batch和batch梯度下降，得到的总代价曲线如图3所示（本文中的程序均使用3层模型，1到3层的节点数分别为5、2、1）。可以看到，mini-batch梯度下降法在梯度下降的过程中存在很多类似噪声的起伏，但总体趋势还是下降的；batch梯度下降法的总代价函数在学习率适中的情况下可以单调下降。经过相同的迭代次数，mini-batch梯度下降算法得到的总代价更小，说明与batch梯度下降法相比，mini-batch在未完整遍历整个训练集时就开始不断梯度下降更新参数，具有更快的训练速度。\n\n    \n\n\n图2 训练数据集\n\n\n    \n\n\n图3 mini-batch梯度下降法和batch梯度下降法总代价随迭代次数的变化\n\n对Mini-batch梯度下降法得到的模型采用不同的颜色进行分割得到图4，对训练集的准确率如图5所示，为79.67%。\n\n    \n\n\n图4 mini-batch梯度下降法得到的模型\n\n\n    \n\n\n图5 mini-batch模型对训练集的准确度\n\n上文中出现的总代价抖动的情况是由于每次训练的数据子集不同，会存在某些数据子集将模型的训练偏离总代价小的方向，而batch梯度下降法每次训练的数据集相同，总代价不会向着增大的方向移动，但mini-batch梯度下降法能更持续地靠近最小值的方向，使用三维图形表示如图7所示。因此，mini-batch梯度下降法不会使最终代价收敛到某一点上，而是在最优点附近。\n\n    \n\n\n图7 mini-batch和batch梯度下降法在梯度下降过程示意\n\nMini-batch梯度下降法的优点是能更持续地优化模型，但每个子集分割得越小，向量化带来的速度增益就消失得越严重。当每个子集为1个数据时，就完全失去了向量化的加速。因此，合理地选择mini-batch子集的大小是十分重要的。\n3. Momentum梯度下降法3.1 指数加权平均指数加权平均在统计中也叫指数加权移动平均，可以用于对数据进行平滑，可总结为方程\n\n    \n\n\n其中，v_t为t时刻指数加权平均后的值，\\theta_t为t时刻的原始数据，\\beta为权重系数，\\beta越小，平均的数据范围越大。图8为用指数加权平均进行平滑的示意图，各曲线按\\beta从小到大排序为黄、蓝、紫、红，当权重较小时，只对小范围数据进行了平滑，所以数据抖动明显，当权重较大时，红色曲线与理想曲线有一定的偏移，这是由于平滑了过多的数据。因此，合理选择权重可以做到对抖动数据的有效平滑，而权重过小会导致数据的偏移。\n\n    \n\n\n图8 不同权重系数的指数加权平均\n\n3.2 指数加权平均的偏差修正在指数加权的初始阶段，v_0被设置为0。经过第一次运算v_1=\\beta v_0+(1-\\beta)\\theta_1后，v_1仅为\\theta_1的1-\\beta倍，这会比真实值相差很多，因为一般\\beta会取得很大。为了修正这个偏差，可以将v_t修改为\\frac{v_t}{1-\\beta^t}，当t很小时，可以有效弥补指数加权的偏差。而当t很大时， 就接近于1，基本不会产生效果，这使得修正后的值与真实值很相近。当然，如果不关注初始训练时的偏差，也可以不做任何处理，等过了初始阶段就会有比较准确的数据。\n3.3 Momentum梯度下降法Momentum梯度下降法可以加速学习的过程，其思想为：对梯度进行指数加权平均，并利用指数加权平均后的梯度更新权重。算法流程如下：\n\n    \n\n\n假设有一代价函数如图9所示，由于是一椭圆状，绿色箭头表示的梯度下降过程呈抖动下降的趋势。这种情况下的梯度下降只能采用较小的学习率，如果学习率过大会使结果偏离函数的范围，这会导致学习速度缓慢。\n\n    \n\n\n图9 某一代价函数的梯度下降\n\n当对梯度进行指数加权平均后，纵轴上的方向相反的梯度相互抵消使得平均值很小，而指向最优点的横轴方向由于梯度方向一致，仍会有一个较大的平均值。因此，该情况下可以使用较大的学习率进行梯度下降，加速了学习的过程。另外，人们经常将 的值设置为0.9，实践表明该数值具有很好的鲁棒性。\n3.4 编程实现Momentum梯度下降采用Momentum算法对相同数据集采用mini-batch的方法进行训练，得到的模型用颜色进行分割如图10所示，对训练集的准确度如图11所示，为79.67%。\n\n    \n\n\n图10 Momentum梯度下降法得到的模型\n\n\n    \n\n\n图11 Momentum模型对训练集的准确度\n\n这和单纯用mini-batch梯度下降法进行计算得到的结果相同，原因在于该模型过于简单，momentum梯度下降法在训练过程中没有起到过多作用，但当模型很复杂时，就可以看出差别。\n4.RMSprop算法RMSprop算法的全称是Root Mean Square Prop，意为采用均方根的方式加快学习进程。其算法流程如下：\n\n    \n\n\n该算法仍可采用上节的例子进行理解，S_{dW}、S_{db}分别对dW、db的平方进行指数加权平均。当某一维度存在震荡时，其导数会很大，计算得到的S值也会很大，使得对该维度的更新更慢；当指向最低代价的维度出现梯度下降慢的情况时，其导数会较小，相应的S值也会更小，该维度更新得就会加快，缓解了学习的瓶颈。该算法中的\\beta值推荐使用0.999，算法中用到的\\varepsilon主要是为了防止0成为分母导致程序出错，一般使用10^{-8}即可。\n5. Adam优化算法Adam算法可以简单理解为是将Momentum算法和RMSprop算法结合起来的优化算法，其算法流程如下：\n\n    \n\n\n可以看到，Adam算法既对dW和db进行了指数加权平均以减轻抖动，又对他们进行了均方根的运算以缓解梯度下降过程中的瓶颈。另外，运用Adam算法时一般会进行偏差修正，所以对v_{dW}、v_{db}、S_{dW}、S_{db} 都进行了\\frac {1}{1-\\beta}的偏差修正。这里，根据Momentum梯度下降算法和RMSprop算法的内容对不同的\\beta进行了角标区分，\\beta_1取0.9，\\beta_2取0.999，\\varepsilon取10^{-8}。仍采用上面章节中使用的模型和数据集，得到的总代价随代数变化的曲线如图12所示，对训练得到的模型按照颜色进行划分如图13所示，该模型对训练集的准确度如图14所示。\n\n    \n\n\n图12 Adam优化算法总代价随迭代次数的变化\n\n\n    \n\n\n图13 Adam优化算法得到的模型\n\n\n    \n\n\n图14 Adam优化算法模型对训练集的准确度\n\n可以发现，与单纯使用mini-batch算法和叠加momentum算法的模型相比，相同的迭代次数下，Adam算法很快就到达了很小的代价，其精确度也提高了许多，说明Adam确实能在很大程度上提高训练的速度。\n6.  学习率衰减在前面应用mini-batch进行模型训练的过程中，由于数据子集的不统一性，会产生许多噪声，导致总代价无法收敛到最优点而在附近徘徊。学习率衰减指将学习率随着迭代的次数不断减小，该情况下会使梯度下降的步伐不断减少，从而使得总代价最终十分接近最优点。对学习率衰减形象地用图形表示如图15所示。\n\n    \n\n\n图15 学习率衰减对模型训练的影响示意图\n\n对于学习率衰减，有几种常用的方法：一种是采用下式进行学习率衰减\n\n    \n\n\n其中，\\alpha_0为初始学习率，\\gamma为衰减率，n为代数。也可以使用如下称为指数学习率衰减的公式\n\n    \n\n\n还有一些公式，如\\alpha=\\frac {k}{\\sqrt{n}}\\alpha_0等，都具有一定的效果，甚至还可以在模型训练的过程中手动调节学习率。虽然学习率衰减确实有时能加快训练，但学习率衰减通常不是考虑的第一位，选择一个合适的超参数\\alpha往往会更有用。\n","categories":["Learning Materials"]},{"title":"深度学习方法探析——浅层神经网络","url":"/2022/10/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%8E%A2%E6%9E%90%E2%80%94%E2%80%94%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","content":"1.逻辑回归逻辑回归可以理解为不具备hidden layer(隐藏层)的神经网络，其模型如图1所示。逻辑回归能通过线性计算预测输入特征值对应的输出，其中的特征权重w和偏差b决定着预测的准确度，因此获得合适的w和b是此模型的重点。使用输入加权的原因可以从人的神经元中找到答案，图2是人的神经元模型，可能同时有多个电信号作用于神经元，当它们的加权和大于神经元的阈值时，其就会在轴突产生输出电信号，神经网络就是由此发展而来的。\n\n    \n\n\n\n\n图1 逻辑回归模型\n\n\n    \n\n\n图2 神经元模型\n\n1.1 梯度下降法逻辑回归利用给定的一组带标记的训练集进行代数运算获取w和b是一项艰巨的工作，甚至是难以完成的，人工智能的发展也因此一度停滞不前。梯度下降法的提出解决了这一问题，虽然其获得的可能不是最优解，但至少让这一过程有解。梯度下降法的思路是：初始随机选择参数，根据预测值与期望值的偏差反向传播误差，进而调整参数的大小。就像从山的任意位置沿着梯度的方向朝着谷底前进，谷底就是预测偏差最小的位置，也就是模型训练的目标点。表1列出了本文会用到的符号解释。\n 表1 本文用到的符号及解释 \n\n\n    \n\n\n\n当输入值多于一个时，需要程序循环多次以计算对应的预测值，但在编程时，大量使用for循环会极大影响程序执行速度。为了解决这个问题，我们尽量采用向量的形式表示各个值，应用python内嵌函数可以轻松地处理向量的运算，经过比较，向量形式比for循环形式快数百倍。因此，本文均采用向量形式进行计算。逻辑回归的过程可以分为forward propagation (向前传播)和backward propagation(向后传播)两个过程。向前传播通过上面所述的过程计算当前参数对应预测值的代价(或损失)，向后传播采用链式法则计算总代价(或总损失)对各参数的偏导数，根据设定的学习率更新参数。之后重复前述操作，直到达到理想的代价或者迭代次数为止。具体的逻辑回归模型训练过程如下：\n\n    \n\n\n\n1.2 编程实现逻辑回归现通过python编程搭建逻辑回归模型实现“猫识别”，程序采用模块化方式编写，用到的函数如表2所示。本文采用可下载的h5文件数据集进行模型训练，也可采用CSV文件数据集。\n 表2 “猫识别”程序用到的函数\n\n\n    \n\n\n本文模型训练迭代10000次，学习率采用0.005。运行程序，可以看到总代价随着训练次数逐渐减小，如图3所示。\n下面随意下载一张猫的图片交给程序进行预测，图4为交给程序的猫图片，程序对图片预测后输出该图片是猫的概率，若概率大于0.5则判定该图片有猫，并显示预测结果，如图5所示。可以看到，程序预测该图片有99.66%的概率为猫。\n\n    \n\n\n图3 逻辑回归程序不同迭代次数的总代价\n\n\n    \n\n\n\n\n图4 预测图片（含猫）\n\n\n    \n\n\n图5 预测结果（含猫）\n\n为了测试程序不会把所有图片认定为猫，再对一不含猫的图片进行预测，如图6所示。预测结果如图6所示，程序预测该图片有33.74%的概率为猫，认为这不是一张含有猫的图片。可以看出，程序可以正确分辨有无猫在图片中。\n\n    \n\n\n图6 预测图片（不含猫） \n\n\n    \n\n\n\n图7 预测结果（不含猫）\n\n经过多次测试，发现程序对正面较清晰的猫识别概率很高，有些情况下存在识别错误。另外，对于一些与猫相近的狗的正面照也存在识别为猫的情况。\n2.浅层神经网络逻辑回归可以看为只有一层的神经网络，也可以理解为人脑的一个神经元，将多个逻辑回归中的单元叠起来就变成了神经网络，如图8所示。其中，输入层也称为第0层，因此下图的神经网络被称为2层神经网络。\n\n    \n\n\n图8 神经网络模型\n\n2.1 神经网络的表示神经网络的节点数比逻辑回归多，需要对逻辑回归的向量形式做一定的调整。本文采用上标 [1]表示第1层神经网络，第0层的X可用A^{[0]}表示，w从之前的1维向量变为矩阵W，其中，为方便后续计算，已将对应的w进行转置，如(5)式所示。\n\n    \n\n\n每层的输出Z^{[i]}可通过(6)式计算\n\n    \n\n\n其中，A^{[0]}相当于第0层的输入，即X。下面以第1层为例推导该过程的维度对应关系：\n\nW^{[1]}X=\\begin{bmatrix}w_1^{[1]T}\\\\w_2^{[1]T}\\\\...\\\\w_{n_{hidden}}^{[1]T}\\end{bmatrix}\\cdot\\begin{bmatrix}\\vdots&\\vdots&\\vdots&\\vdots\\\\x_1&x_2&x_3&\\vdots\\\\\\vdots&\\vdots&\\vdots&\\vdots\\end{bmatrix}=\\begin{bmatrix}\\vdots&\\vdots&\\vdots&\\vdots\\\\w_1^{[1]T}&w_2^{[1]T}&w_3^{[1]T}&\\vdots\\\\\\vdots&\\vdots&\\vdots&\\vdots\\end{bmatrix}=\\begin{bmatrix}\\vdots&\\vdots&\\vdots&\\vdots\\\\z_1^{[1]}&z_2^{[1]}&z_3^{[1]}&\\vdots\\\\\\vdots&\\vdots&\\vdots&\\vdots\\end{bmatrix}=Z^{[1]}2.1.1 forward propagation(正向传播)只有2层的浅层神经网络的正向传播可由以下4个式子求得\n\n    \n\n\n2.1.2 backward propagation(反向传播)反向传播可总结为以下6个公式 (输出层采用sigmoid函数作为激活函数)\n\n    \n\n\n其中，(11)、(12)、(13)、(15)和(16)式原理同(2)-(4)式，现对(14)式进行适当推导。运用链式法则，可得\n\n    \n\n\n其中，\n\n    \n\n\n综合(17)-(19)式以上各式可得\n\ndZ^{[1]}=W^{[2]T}*g^{[1]'}(Z^{[1]})2.1.3 浅层神经网络训练过程具体的浅层神经网络训练过程如下(以2层为例)：\n \n2.1.4 激活函数的选择神经网络需要非线性的激活函数，因为如果激活函数是线性的，那么无论有多少隐藏层，都可以看作是一组线性组合，那么都会浓缩为1层，这显然是毫无意义的。在逻辑回归中用到的是sigmoid函数，如图9(a)所示，然而另外一种非线性函数tanh在大多数情况是更受欢迎的，如图9(b)所示，它覆盖了-1到1的值域，这在神经网络中会取得更好的效果，但如果模型所解决的是一个二分类问题，期望得到的是0或1，可以考虑在输出层使用sigmoid函数。这两个函数都存在一个问题，当z太大或太小时斜率接近于0，这会导致学习饱和，所以可以采用ReLu函数或Leaky ReLu函数代替，如图9(c)、(d)所示，他们都可以保证z较大时不错的收敛速度。\n\n    \n\n\n图9 几种激活函数 \n\n前述计算过程中，激活函数g(z)的导数经常被用到，因此本文将几种激活函数的导数整理到表3中。\n\n    \n\n\n\n\n2.1.5 随机初始化训练神经网络时，权重随机初始化是十分重要的。当把权重W全部初始化为0时，同层的隐藏单元计算着同一个函数，他们对输出的影响也全部相同，无论经过多少次迭代他们都计算着相同的函数。也就是说，这和只有一个隐藏单元没有任何差别。为了解决这一问题，我们应当将W随机初始化，而b没有这个问题，可以将其初始化为0。\n2.2 编程实现浅层神经网络现编程搭建只有一个隐藏层的浅层神经网络，隐藏层设置5个节点，用到的函数如表4所示。\n表4 浅层神经网络用到的函数 \n\n\n    \n\n\n\n为了方便比较，本文采用与逻辑回归程序相同的数据集进行训练，迭代次数同为10000次，总代价的变化如图10所示。可以发现，相同的迭代次数下，浅层神经网络比逻辑回归单节点网络的代价0.035704要小。\n\n    \n\n\n图10 浅层神经网络程序不同迭代次数的总代价\n\n\n    \n \n\n图11 预测图片（含猫）\n\n\n    \n\n\n图12 预测结果（含猫）\n\n现使用与逻辑回归程序相同的两张图片对浅层神经网络进行测试。如图11所示，程序预测该图片有99.51%的概率为猫图片，相比逻辑回归单节点99.66%的概率略有降低，但两种情况下预测的概率都接近100%，可以认为识别效果都十分不错，并不能看出优劣。为了更加清晰地比较两种模型，我们选取一张不太标准的猫图片进行识别，如图13所示。\n\n    \n\n\n图13 非标准含猫图片\n\n逻辑回归的单节点模型预测的结果如图14所示，浅层神经网络预测的结果如图15所示。\n\n    \n\n\n图14 逻辑回归单节点模型预测结果图 \n\n\n    \n\n\n图15 浅层神经网络预测结果 \n\n可以看到，浅层神经网络的预测结果比单节点模型概率高8.86%，浅层神经网络的预测能力更强。\n","categories":["Learning Materials"]},{"title":"深度学习方法探析——深层神经网络","url":"/2022/10/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%8E%A2%E6%9E%90%E2%80%94%E2%80%94%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","content":"1. 深层神经网络深层神经网络与之前的浅层神经网络的不同之处在于网络的层数更多，或者说隐藏层的层数更多，网络更深，深度学习也是基于此命名的。在过去的几年中，DLI（Deep Learning Institute）已经意识到有一些函数，只有非常深的神经网络能学会，而更浅的模型则办不到，因此深度神经网络是十分重要的。但网络的深是深在网络层数上，而不在网络节点的数量多。\n\n    \n\n\n图1 深层神经网络模型\n\n图1是一个深层神经网络的模型，它有5层，输入层算作第0层。本文用L表示层数，该模型的L=5，用n^{[l]}表示每层的节点数，这里n^{[1]}=n^{[2]}=n^{[3]}=4, n^{[4]}=3, n^{[5]}=1。\n1.1 Forward Propagation (正向传播)正向传播和浅层神经网络差别不大，也比较简单，只需不断向前迭代即可，可用(1)、(2)式表示。\n\n    \n\n\n当对多个样本进行训练时，可使用(3)、(4)式的向量形式。\n\n    \n\n\n该过程需要提供给系统A^{[0]} =X进行初始化。\n1.2Backward Propagation (反向传播)反向传播通过对总代价求导得到的da^{[l]}计算dw^{[l]}、db^{[l]}和da^{[l-1]} ，依次向前传递，计算出各dw^{[l]}和db^{[l]}，便于运用梯度下降法对参数进行优化。反向传播可由(5)-(8)式进行计算。\n\n    \n\n\n向量化形式可写成(9)-(12)式。\n\n    \n\n\n对整个正向传播和反向传播的过程进行总结可得到图2，其中系统将正向传播得到的z^{[l]}进行缓存，并传递给反向传播各模块进行计算。\n\n    \n\n\n图2 深层神经网络正反向传播过程\n\n在计算过程中，确保各参数的维数正确是避免bug产生的重要手段，故将各参数的维数总结到表1中。\n表1 深层神经网络参数维数\n\n\n\n\n    \n\n\n1.3 深层神经网络计算过程\n    \n\n\n图3 深层神经网络计算过程\n\n当运行人脸识别程序时，输入给程序一张人脸照片，如图3所示，第一层神经网络可以看做在识别图像的边缘，每个节点负责不同方向的边缘，如第一张图所示。识别到边缘后交给第二层网络进行计算，它将各个边缘组合起来变成人体的各个部位，每个结点负责不同的部位，如第二张图所示，后面层的结点可能会将各部位再进行组合变成一张人脸，对此进行识别，这就是深度神经网络在进行计算时所经历的大致过程。总的来说，这是一个由小的区域到大的区域，由简单到复杂的过程。\n1.4 编程实现深层神经网络由上面几节已经知道了深层神经网络中的几种重要计算，现将训练整个深层神经网络的过程归纳为图4所示的流程，并由此编程搭建深层神经网络。本文设计了一个4层深层神经网络作为编程对象，第0-4层分别有12288、20、7、5和1个节点。\n\n    \n\n\n图4 深层神经网络训练流程\n\n程序用到的函数及功能说明如表2所示。\n表2 深层神经网络用到的函数\n\n\n\n\n    \n\n\n现采用与浅层神经网络相同的训练集进行模型训练，迭代次数为3000次，仅为浅层神经网络程序的十分之三，学习率\\alpha同为0.005。总代价随着迭代次数的变化如图5所示，可以发现相同迭代次数下，深层神经网络的总代价略高于浅层神经网络。\n\n    \n\n\n图5 总代价随迭代次数的变化\n\n2. 深层神经网络与浅层神经网络比较对浅层神经网络进行测试发现，只可识别正面较标准的猫图片，对于角度较偏或姿势奇特的图片不可识别。现采用深层神经网络对这些难以识别的图片进行对比测试，得到的测试结果如表3所示。\n表3 浅层神经网络和深层神经网络对非标准猫图片的预测\n\n\n\n\n    \n\n\n从上表可以看出，尽管深层神经网络迭代次数仅为浅层神经网络的四分之一，深层神经网络比浅层神经网络识别正确率高得多。并且可以发现，对于这些猫图片，深层神经网络预测的概率几乎全部为1或近似于1，比浅层神经网络概率高很多，可见其性能的优异。\n3. 总结从测试结果中看，神经网络凭借较少的训练次数就可拥有远超浅层神经网络的性能，这是十分令人兴奋的。同时，深层神经网络模型对表3中的第4张图片预测错误，这时，将学习率\\alpha从0.005改为0.0075，再次对模型进行训练，得到的总代价如图6所示，比学习率为0.005时小。再次对该图片进行预测得到了如图7所示的结果，预测正确，且概率达到了1。这说明学习率的选择对模型的性能有很大影响，这也引起了我们对超参数优化的兴趣，关于模型优化方面的内容将在后续文章中阐述。\n\n    \n\n\n图6 α=0.0075时总代价随迭代次数的变化\n\n\n    \n\n\n图7 α=0.0075时的预测结果","categories":["Learning Materials"]},{"title":"深度学习方法探析——超参数调试和多分类模型","url":"/2022/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%8E%A2%E6%9E%90%E2%80%94%E2%80%94%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E5%92%8C%E5%A4%9A%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/","content":"1. 超参数调试前面应用深层神经网络进行模型训练和推理的过程中遇到了这样一个情况：对一张猫的图片识别错误，概率仅为30%多，但在改变学习率后，概率变为了1，识别正确。这反应了超参数对神经网络模型训练的重要性，我们在训练神经网络的过程中常常需要不断改变各个超参数的值来对比前后性能的变化。因此，制定一套系统的超参数调试流程是十分有必要的。由于神经网络有众多超参数，在调试过程中要有一个优先级。通常，学习率\\alpha决定着模型训练的质量，需要优先考虑。其次，mini-batch的大小和隐藏单元的数目也会影响算法的运行，也需要考虑。对于Momentum梯度下降法和Adam算法中的参数\\beta、模型的层数、学习率衰减率等超参数有时也会产生影响，但优先级较低。对于Adam算法中的β_1、β_2和ε，一般选用固定值0.9、0.999和10^{-8}，不对其进行调试。如果在超参数的选择范围内均匀取值，只能覆盖少量数据，因此需要在划定的范围内随机取值。但仅仅均匀随机取样是不行的，对某些超参数来说，标尺的正确选择是必要的。对于模型层数和节点数这类超参数，在给定范围内均匀取值不会出现什么问题。但对于学习率\\alpha、Adam算法中的β，例如在0.0001到1之间随机取值，那么90%的数据会落到0.1到1之间，0.0001到0.1之间只有10%的数据，这是不均衡的。这时应该将数据范围换成对数形式，从10-4到100，再在指数上进行平均随机选取，就可以兼顾每一个数量级的数据。超参数的搜索过程可以分成两种：一种是在没有足够计算资源的情况下，可以一次只训练一个模型，并在训练的过程中不断修改超参数的值，看代价函数如何变化，进而选出最优超参数；另一种是在计算资源充足的情况下，可以同时训练多个模型，查看各模型的性能，选择代价函数最理想的模型对应的超参数。\n2. Batch标准化 (Batch Normalization)在前面训练过程优化中有提到输入标准化的问题，输入标准化可以加快学习的过程。深层神经网络中有很多隐藏层，对每个隐藏层都有前一层的A^{[l-1]}输入，要想加快W^{[l]}和b^{[l]}的训练过程，就需要对每一层的输出Z^{[l]}进行标准化，这就是Batch归一化。Batch标准化的公式为\n\n    \n\n\n有时我们可能希望隐藏层具有一些特别的分布，不希望是均值为0，方差为1的标准分布，这时可在Batch标准化中做如下操作\n\n    \n\n\n由于会对每层的输出进行标准化，任何增加的常数都会被抵消，在使用Batch标准化时可以不考虑b^{[l]}这个参数。使用Batch标准化算法配合Mini-batch进行梯度下降的算法如下。\n\n    \n\n\n除了可以加快学习的过程外，Batch标准化还可以使深层节点更好地抵抗输入数据分布的变化。在遇到训练集和测试集分布不同的情况时，对模型的推理将产生影响，这种情况被称为“Covariate shift”。Batch标准化使每层的输出保持恒定的均值和方差，当输入的分布改变时，每层的输出的变化程度就会减小，特别是对深层的节点来说，这有助于模型抵抗数据分布的改变。Batch标准化还有一个附加作用：正则化，使用不同的mini-batch计算均值和方法时，由于数据的不统一会产生一些噪声，因此在标准化和从z^{[l]}到\\tilde{z}^{[l]}的过程中都会产生一些噪声.这和Dropout正则化很像，区别在于Dropout的噪声使其对每个隐藏单元乘0或1，所以Batch标准化也会存在轻微的正则化效果。在训练中运用Batch标准化时，\\mu和σ^2是通过整个mini-batch计算出的。然而，在进行模型预测时，很多时候要逐一处理样本，这时对\\mu和σ^2的计算就没有意义了。这时就需要采用指数加权平均的方法对每个mini-batch计算出的\\mu^{[l]}和σ^{2[l]}进行运算，估计出每层的\\mu^{[l]}和σ^{2[l]}，运用到单一样本的预测中。\n3. Softmax回归 (Softmax regression)3.1 Softmax回归概念之前我们训练的全部为二分类模型，Softmax回归可以用来训练具有多个输出的模型。Softmax回归模型如图1所示，模型要对数据集分成几类就设置几个输出节点，类别的数目用C表示，每个输出层节点输出的值为该类别的概率。Softmax回归的关键是Softmax激活函数，其对输出层的输出Z[l]起到的作用为：先对Z[l]进行指数变换\n\n    \n\n\n\n然后对其进行归一化，使各节点的输出总和为1：\n\n    \n\n\n\n此时的输出即为各类别的概率。\n\n    \n\n\n图1 Softmax模型\n\nSoftmax回归的损失函数定义为\n\n    \n\n\n\n当试图减小训练集的损失时，它可以让对应的 值尽可能大。整个训练集的总代价函数为\n\n    \n\n\n\n3.2 编程实现Softmax回归本文采用Tensorflow深度学习框架，使用softmax回归实现了手部动作从0到5的识别。Tensorflow是一个不错的深度学习框架，它可以帮助我们省去很多程序的编写，特别是对于后向传播，Tensorflow可以根据编写的代价函数自动进行梯度下降，帮助我们解决了后向传播这一最复杂的问题。Tensorflow作为一种用于深度学习的编程框架，其主要包含两种目标类：Tensors和Operators，编程中应该遵守的步骤如下：\n\n    \n\n\n本文训练集采用1080张64*64 pixels的有0到5手势的图片，如图2所示，测试集为相同规格的图片120张。本文采用三层模型，0到3层的节点数分别为12288、25、12、6，学习率0.0001，采用mini-batch迭代1500代，每个mini-batch大小为32。\n\n    \n\n\n图2 训练数据集示例\n\n训练得到的总代价随迭代次数的变化如图3所示，图中的抖动是由于使用了mini-batch的方法，但总体逐渐下降，效果不错。\n\n    \n\n\n图3 总代价随迭代次数的变化\n\n该模型对训练集和测试集的准确度如图4所示，测试集比训练集准确度低很多，说明模型存在高偏差，这是由于模型过于简单造成的，这里由于训练时间较长，选择了这个适中的模型。\n\n    \n\n\n图4 训练集和测试集的准确度\n\n使用自己拍摄的0到5手势进行预测，得到的结果如表1所示，该模型具有多个输出，能够预测0到5的手势。但注意到，对于下表的测试数据，准确率不高，说明该模型确实如测试集的结果显示一样，具有较大的偏差，对新环境的适应能力不强，还有很大改进空间。\n表1 模型对0到5手势的预测结果\n\n\n    \n","categories":["Learning Materials"]},{"title":"深度学习方法探析——训练过程优化","url":"/2022/10/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%8E%A2%E6%9E%90%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%BC%98%E5%8C%96/","content":"1. 引言深层神经网络的训练过程往往需要花费很多时间，特别对于很深的网络，往往需要花费巨大的时间和算力，而且如果网络的结构选择不当，会造成数据欠拟合或过拟合的后果。为了尽可能减少训练深层神经网络的代价，提高模型准确度，本文从训练过程角度讨论了几种可以加速训练速度和精度的方法，包括欠拟合和过拟合的判断方法、解决过拟合的正则化方法、提高训练速度的归一化输入、减缓梯度消失和梯度爆炸的初始化方法。另外，本文还介绍了一种用于检验网络梯度下降正确性的方法——梯度检验。\n2.  训练集、验证集和测试集在深层神经网络模型训练的过程中，需要提供许多数据用于计算最优参数，这些训练用的数据被称为训练集 (train set)。可以注意到模型中还存在许多超参数，这些超参数在模型训练开始都是根据经验设置的，可以使用交叉验证集，也称验证集 (dev set)来挑选超参数，它是参与模型选择过程的。而测试集 (test set)不以任何形式参与到模型的选择中，它用于在选择好模型后对模型的运行状况进行无偏估计。我们要做的是，在训练集上对模型进行训练；然后尝试不同的模型框架，在验证集上评估这些模型，迭代选出适合的模型；最后在测试集上对该模型的运行状况进行无偏估计，通过不断的循环迭代训练出优秀的模型，该过程如图1所示。\n\n    \n\n\n图1 深层神经网络模型训练迭代流程\n\n在数据量比较小的时候，可以按照60%训练集，20%验证集，20%测试集的比例进行划分。但是在大数据时代，数据量可能是百万级别，验证集和测试集占数据总量的比例会趋向于变得更小。有时，如果不需要无偏估计，也可以只设置训练集和验证集，人们通常把这种情况称为只有训练集和测试集，实际上是将验证集过度拟合到了验证集，从而把测试集当做验证集使用。训练集、验证集和测试集的对比如表1所示。\n表1 训练集、验证集、测试集对比\n\n\n    \n\n\n3. 偏差和方差3.1 偏差和方差估计的偏差 (bias)被定义为\n\n    \n\n\n其中，期望作用在所有数据上，\\theta是用于定义数据生成分布的\\theta的真实值。估计量的方差 (variance)为\n\n    \n\n\n其中，随机变量是训练集。估计量的方差告诉我们，当独立地从潜在的数据生成过程中重采样数据集时，如何期望估计的变化。正如我们希望估计的偏差较小，我们也希望其方差较小。\n3.2 欠拟合和过拟合当讨论偏差和方差时，就不得不研究欠拟合和过拟合的问题。如图2所示，图中的绿点为一组数据集，图2 (a)对数据集拟合了一条直线，得到了一个逻辑回归拟合，它并不能很好地拟合该数据，这就是欠拟合 (underfitting)的情况，该情况下偏差高；如果我们使用一个复杂的网络模型，比如深度神经网络或含有隐藏单元的神经网络，可能就非常适用于这个数据集，如图2 (c)所示，但这显然也不是一种很好的拟合方式，它过度地拟合了训练集中的特殊情况，这就是过拟合 (overfitting)，该情况下方差高。图2 (b)就是一种很好的拟合方式，其复杂程度适中，拟合数据更加合理，可以称这种情况为适度拟合 (just right)，这种情况下偏差和方差均较小。\n\n    \n\n\n图2 三种拟合情况\n\n3.3 欠拟合和过拟合的判断现假设训练集和验证集的最优化误差为0，我们可以通过比较训练集和验证集的误差大小判断拟合情况。若某情况下训练集的误差小，而验证集的误差大，说明过度拟合了训练集的数据，为高方差；若训练集和验证集的误差都很大且二者相当，则训练集数据没有得到很好的拟合，为高偏差；若训练集和验证集的误差都很大且验证集误差更大一些，则为高偏差高方差的情况，在某些维度上的数据区域偏差高，某些维度上的数据方差高，是最坏的一种情况；若训练集和验证集误差都很小，该情况就为适度拟合。现将欠拟合和过拟合判断方法总结为表2。\n表2 欠拟合和过拟合判断方法\n\n\n    \n\n\n3.4 降低偏差和方差的方法训练网络的最低标准就是能够拟合数据，因此在训练完模型后应该首先评估网络的偏差是否很大。如果发现网络的偏差很大，可以常识几个方法：选择一个新的网络，比如含有更多隐藏层或者隐藏单元的网络；花费更多时间来训练网络；或者尝试更先进的优化算法。其中，使用规模更大的网络通常是有用的。如果检测到网络的方差很高，最好的办法就是采用更多的数据。但有时我们无法获得更多数据，这时可以尝试使用正则化的方式降低网络的方差，这将在下一节中讲述。\n4. 正则化4.1 L^2正则化现从逻辑回归模型着手，对L^2正则化进行研究。对逻辑回归模型实施正则化后，其总代价为\n\n    \n\n\n其中涉及到了范数的概念，L^p范数定义如下\n\n    \n\n\n其中p∈R ,p≥1。由于该过程用到了w的L^2范数，因此称其为称为L^2正则化，‖w‖_2^2的计算过程为\n\n    \n\n\n这里只将参数w正则化的原因是，w几乎包含了所有参数，而b只是一个参数，可以忽略不计。将L^2正则化推广到神经网络中，(3)式变为\n\n    \n\n\n其中，‖W^{[l] } ‖_F^2的计算过程为\n\n    \n\n\n该矩阵范数被称作“弗罗贝尼乌斯范数 (Frobenius norm)”，用下标F标注。正则化后的dW只需在原有基础上减去正则项的导数即可：\n\n    \n\n\n其中，(back prop)代表未正则化前的dW^{[l]} 。更新后的W^{[l]}为\n\n    \n\n\n可见，L^2正则化每次更新都在原有的W^{[l]}上先乘一个小于1点的数再进行梯度下降，因此L^2正则化也被称为“权重衰减”。\n4.2 正则化减少过拟合的原理如前所述，过拟合情况的发生是由于网络过于复杂，拟合了过多的训练集数据，我们希望简化网络模型，正则化减少过拟合的原理可以从两方面直观理解。一方面，正则化朝着减小权重的方向更新W^{[l]} ，可以假设某些W^{[l]}被更新到接近于0，这样对应的节点就相当于断开了，网络得到简化。实际上权重的衰减并不会这么大，但可以一定程度简化网络，达到减缓过拟合的效果。另一方面，我们所用的激活函数，如sigmoid函数和tanh函数，在0附近近似于线性，正则化将参数往减小的方向调整，假设调整幅度很大，正则化参数λ足够大，就会使得z^{[l]}落在线性区域。而线性的激活函数无论有多少层，对输出而言都只有一层，这就使得复杂的网络得到了简化。虽然在实际情况下每层输出不会变为线性，但可以一定程度地减缓过拟合。\n4.3 Dropout正则化除L^2正则化外，还有一种十分有效的正则化方法——Dropout正则化，可以理解为随机失活。Dropout的原理是：对每一层的节点设置失活概率，该层中的每个节点都以该概率随机被删除，这在一定程度上简化了网络，可以达到减缓过拟合的效果，如图3所示。\n\n    \n\n\n图3 实施Dropout前后的网络\n\nDropout正则化可以减缓过拟合的原因可以理解为：每个节点的输入都可能随机消失，所以在训练过程中不会过分依赖任何一个节点，这会产生收缩权重的平方范数的效果，这和L^2正则化相类似。值得注意的是，为了不对预测结果在范围上产生变化，需要在每层激活函数的输出数据上除以 (1-失活概率)，这起到了归一化的作用。Dropout的一大缺点就是代价函数 J不再被明确定义，或者说在某种程度上很难计算，每次迭代，都会随机移除一些节点，这使得梯度下降的性能很难得到检验。因此，通常可以先关闭Dropout，检查代价函数 J单调递减后再打开Dropout进行迭代训练，这有助于减少程序中的错误。\n4.4 正则化实际效果测试通过理论推理，可以得出正则化可以减缓过拟合的结论，下面将使用python编程实现L^2正则化和Dropout正则化，比较实际结果与理论推理是否一致。首先不适用正则化，对一组数据进行拟合，该组数据有两个特征向量，通过图形化展示如图4所示，不同的y以不同的颜色表示。\n\n    \n\n\n图4 待分类数据\n\n现使用一个三层网络对其进行训练，每层节点数分别为2、20、3、1，迭代30000次，得到的总代价曲线如图5所示，训练集和验证集准确度如图6所示，验证集误差比训练集误差大。将训练出的分类器不同部分以不同颜色填图得到图7，可以发现有部分特例也被分类器拟合了，存在过拟合的情况。\n\n    \n\n\n图5 未正则化时的总代价曲线\n\n\n    \n\n\n图6 未正则化时的测试验证集准确度\n\n\n    \n\n\n图7 未正则化时训练得到的模型\n\n4.4.1 L^2正则化对上述模型实施L^2正则化，设置参数λ为0.7，其余不变，这时总代价曲线如图8所示，训练集和验证集准确度如图9所示。\n\n    \n\n\n图8 L2正则化时的总代价曲线\n\n\n    \n\n\n图9 L2正则化时的测试验证集准确度\n\n可以看到，此时验证机误差和训练集误差十分接近，过拟合情况得到了改善。对模型进行颜色填图，得到的分类器模型如图10所示。该模型没有将特殊点拟合进去，实现了不错的二分类，可见L^2正则化的效果是不错的。\n\n    \n\n\n图10 L2正则化时训练得到的模型\n\n4.4.2 Dropout正则化现对最初的模型进行Dropout正则化，将所有节点的失活概率都设置为0.14，其余不变，得到的总代价曲线如图11所示，训练集和验证集准确度如图12所示。\n\n    \n\n\n图11 Dropout正则化时的总代价曲线\n\n\n    \n\n\n图12 Dropout正则化时的测试验证集准确度\n\n使用Dropout正则化后，验证集的准确度超过了训练集，说明适当简化网络后得到了更佳的性能。得到的分类器模型如图13所示，该模型也没有将特殊点拟合进去，并且对分界线上得数据进行了合理分配，训练得到的模型性能优于前者。可见，Dropout正则化在一定程度上能缓解数据的过拟合。\n\n    \n\n\n图13 Dropout正则化时训练得到的模型\n\n4.5 Early Stopping除了L^2正则化和Dropout正则化外，Early Stopping也是一种避免数据过拟合的有效方法。在未进行多少次迭代时，参数W^{[l]}的值很小，随着迭代次数的增加，W^{[l]}的值越来越大，此时就可能会产生过拟合。Early Stopping要做的就是在方差恶化的时候及时停止迭代，如图14所示，后半段验证集误差随着迭代次数增加而增大，方差进而增大，在最优迭代次数处停止迭代可以得到较好的偏差和方差的权衡。\n\n    \n\n\n图14 训练验证集误差随迭代次数的变化\n\nEarly Stopping的优点是：不必像L^2正则化一样不断测试λ的值以便找到合适的超参数，它只需运行一次梯度下降即可。但它在停止迭代的同时也停止了对总代价的优化，因此它可能影响偏差的大小。换句话说，它是在进行偏差和方差的权衡，降低了方差可能以提高偏差为代价，而不能用两种方法分别解决这两个问题，但在一定程度上这种方法是可行的。\n5.  归一化输入归一化输入是一个很好的加速训练过程的方法，归一化的过程可以分为两步：均值归零化和方差归一化。均值归零化的公式为\n\n    \n\n\n其中，\\mu为数据的均值\n\n    \n\n\n方差归一化的公式为\n\n    \n\n\n其中，σ^2为数据的方差\n\n    \n\n\n归一化输入使输入全部变为均值为0，方差为1的数据，该过程可形象地表现为图15的形式，为便于展示，假定该组数据只有两个特征向量。\n\n    \n\n\n图15 归一化输入\n\n下面说明归一化输入可以加速训练过程的原因，当使用未归一化的数据时，其不同特征向量的均值和方差可能有所不同，这将导致不同的特征向量对应的W_i的均值和方差不同，他们的范围可能相差数十甚至上百倍。现以一个二维的W为例，绘制其总代价函数，如图16(a)所示，这是未输入归一化的情形，两个维度存在较大范围差。当在其上进行梯度下降时，必须取很小的步长，而且是迂回下降的，因为这个函数是狭长的。而通过如图16(b)的归一化后的数据训练出的W对应的总代价函数为一个碗形，可以很容易地沿一条线进行梯度下降直至谷底，有效节省了训练时间。\n\n    \n\n\n图16 归一化前后总代价函数梯度下降\n\n6. 权重初始化6.1 梯度消失和梯度爆炸当训练深度较深的深层神经网络时，通过反向传播计算出的梯度有时会变得特别大或特别小，甚至是以指数形式变化，这会极大地加大训练的难度，这就是梯度爆炸和梯度消失。当梯度变得特别小时，每次梯度下降的步长都变得很小，增加了达到谷底的迭代次数；当梯度变得特别大时，梯度下降法只能在谷底两次徘徊，而无法到达谷底，这都十分影响训练的速度和精度。造成梯度消失和梯度爆炸的原因是：在深层神经网络的训练过程中中一般都会存在这一项 ∏_{(l=1)}^LW^{[l]} X，深层网络中的L一般很大，只要W^{[l]} 略微大于1，该项将会变得非常大，反之亦然。这就导致了激活函数的输出很大或很小，同理，对应的梯度也会变得非常大或非常小。\n6.2 权重初始化缓解梯度消失和梯度爆炸的一种方法就是更谨慎地选择随机初始化参数，虽然这不能从根本上解决问题，但会有所帮助。对于一个神经元来说，其输出和输入有如下关系\n\n    \n\n\n其中，n表示神经网络输入的特征数量。为了防止Z过大或过小，我们希望在n较大时减小W_i，在n较小时增大W_i，比较好的方法就是将W_i的方差Var(W_i )设置为1⁄n，我们要做的就是在对每层的权重随机初始化时，在其上乘1/\\sqrt{n^{[l-1]}}。特别地，若使用ReLu函数作为激活函数，将Var(W_i )设置为1⁄n通常是更佳的，也就是在随机初始化的权重上乘2⁄\\sqrt{n^{[l-1]}}，对于tanh函数来说，1⁄n就是一个不错的选择。\n6.3 几种权重初始化的实际效果测试这里使用和正则化时相同的训练验证集进行训练，模型仍为一个三层网络每层节点数分别为2、20、3、1，迭代15000次，依次使用零初始化、随机初始化（参数初始化很大）和He初始化对模型进行训练，对比三种情况的拟合效果。\n6.3.1 零初始化将所有参数初始化为0，将训练到的分类器模型进行填图得到图17，训练验证集的准确度如图18所示。\n\n    \n\n\n图17 零初始化时的模型\n\n\n    \n\n\n图18 零初始化时训练验证集的准确度\n\n可见，这个模型是十分糟糕的，准确度仅在一半左右，将所有输入都预测为0，这验证了我们之前所说的，如果将参数初始化为0，整个网络的对称性会使得每个节点都在做同样的事，这使得每层都等效于只有一个节点，使得这个网络性能和线性的逻辑回归模型差不多，甚至更差。\n6.3.2 随机初始化（参数初始化很大）为了模拟梯度爆炸的情况，我们将随机初始化的参数都乘10，其余设置都与6.3.1节相同。总代价随迭代次数的变化如图19所示，初始总代价很大，这是由初始权重很大造成的，其间总代价出现了几次上升，这正是图16(a)代表的情况，由于梯度爆炸，在谷底两侧不断徘徊。训练得到的模型如图20所示，训练验证集的准确度如图21所示。\n\n    \n\n\n图19 随机初始化总代价曲线（参数初始化很大）\n\n\n    \n\n\n图20 随机初始化时的模型\n\n\n    \n\n\n图21 随机初始化时训练验证集的准确度\n\n可见，梯度爆炸的情况下，训练过程被延长，并且模型效果不佳，应该设法避免这种情况发生。\n6.3.3 He初始化He初始化是用第一个使用它的作者命名的，就是之前提到的对激活函数是ReLu时的初始化方法，其在随机初始化的权重上乘2⁄\\sqrt{n^{[l-1]}}。He初始化下总代价随迭代次数的变化如图22所示，总代价随迭代次数的衰减很快。训练得到的模型如图23所示，训练验证集的准确度如图24所示。\n\n    \n\n\n图22 He初始化总代价曲线\n\n\n    \n\n\n图23 He初始化时的模型\n\n\n    \n\n\n图24 He初始化时训练验证集的准确度\n\n在这种初始化方式下，模型的准确度表现良好，可见好的初始化对缓解梯度消失或梯度爆炸的重要性。\n7.  梯度检验在训练神经网络时，有时虽然程序能够正常运行，但很难检查反向传播是否完全正常工作。一种比较好的评估方法就是采用数值逼近，我们称其为梯度检验。梯度检验通过取代价函数上某点左右两侧十分临近的两点的函数值对该点的导数进行估计，这里采用双边公差的原因是其误差比单边公差小。进行梯度检验要进行的第一件事就是将网络中的参数W^{[1]}  、b^{[1]}  、W^{[2]}  、b^{[2]}  、⋯、W^{[L]}  、b^{[L]} 转换成一个向量\n\n    \n\n\n将dW^{[1]}、db^{[1]}、dW^{[2]}、db^{[2]}、⋯、dW^{[L]}、db^{[L]}转换成一个向量\n\n    \n\n\n总代价函数J是\\theta的函数，可以写成J(θ_1,θ_2,θ_3,⋯)，接下来要做的就是对每个组成元素θ_i使用双边估计得到\n\n    \n\n\n最后对得到的两个向量dθ和dθ_{approx}进行比较来估计反向传播的正确性，这里采用下式对两向量进行比较\n\n    \n\n\n如果计算得到的值为10^{−7}或更小，说明导数逼近很有可能是正确的；如果它的值在10^{−5}范围内，就应该引起注意，模型训练的过程中可能存在问题。发现网络存在问题时，应该仔细检查所有θ项，查看是否有某一个特定的θ_i使得dθ和dθ_{approx}相差较大，并用其追踪求导运算，然后进行调试，直到(18)式的计算结果达到10^{−7}的量级为止。\n","categories":["Learning Materials"]}]