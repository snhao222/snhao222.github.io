<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Sun Hao">
    
    <title>
        
            An Efficient and Accurate Block Latency Predictor |
        
        SnSpace
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/Slogo.svg">
    
<link rel="stylesheet" href="/fontawesome/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/regular.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/brands.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"snhao222.github.io","root":"/","language":"en","path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":false,"expand_all":false,"init_open":true},"style":{"primary_color":"#0066CC","logo":"/images/Slogo.svg","favicon":"/images/Slogo.svg","avatar":"/images/HeadImage.svg","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":false,"header_transparent":false,"background_img":"/images/bg.svg","description":"S  n  S  p  a  c  e","font_color":null,"hitokoto":{"enable":false}},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{},"code_block_tools":{"enable":true,"style":"default"},"side_tools":{},"pjax":{"enable":false},"lazyload":{"enable":false},"version":"3.4.8"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    KEEP.language_code_block = {"copy":"Copy code","copied":"Copied","fold":"Fold code block","folded":"Folded"};
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/Slogo.svg">
                </a>
            
            <a class="logo-title" href="/">
               SnSpace
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/categories">CATEGORIES</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">An Efficient and Accurate Block Latency Predictor</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/HeadImage.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Sun Hao</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2022-11-02 08:32:50</span>
        <span class="mobile">2022-11-02 08:32</span>
    </span>
    
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <h2 id="Study-Survey"><a href="#Study-Survey" class="headerlink" title="Study Survey"></a>Study Survey</h2><html>
<head>
    <style type="text/css">
        table {
            width: 100%;
            border-collapse: collapse;}
        table caption{
            font-size: 1.2em;
            font-weight: bold;
            margin: 1em 0;}
        th,td{
            border: 0px solid #999;
            text-align: center;
            padding: 10px 0;}
        table thead tr{
            background-color: #C9D7F3;
            color: #1D2088;}
        table tbody tr:nth-child(even){
            background-color: #E6EDFA;}
        table tbody tr:hover{
            background-color: #98B3E8;}
        table tbody tr td:first-child{
            color: #000000;}
        table tfoot tr td{
            text-align: right;
            padding-right: 8px;}
        #meta1{
            text-align: left;
            padding: 10px 5px;
        }
        #meta2{
            text-align: center;
            padding: 10px 5px;
            background-color: #CCFFCC;
        }
        #meta3{
            text-align: center;
            padding: 10px 5px;
            background-color: #FFCCCC;
        }
        #meta4{
            text-align: center;
            padding: 10px 5px;
            background-color: #FFFFFF;
        }
    </style>
</head>
<body>
    <table border="1px">
        <caption>Table1: A few types of latency prediction works</caption>
        <thead>
            <tr>
                <th>Type</th>
                <th>Work</th>
                <th>Strength</th>
                <th>Drawback</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>FLOPs based</td>
                <td id="meta1"><sup><a href="./Neurosurgeon：Collaborative Intelligence Between the Cloud and Mobile Edge.pdf">[1]</a></sup>Neurosurgeon<br><sup><a href="./µLayer：Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization.pdf">[2]</a></sup>µLayer<br><sup><a href="./paleo：a performance model for deep neural networks.pdf">[3]</a></sup>PALEO</td>
                <td id="meta1">Light-weight and easy to train.</td>
                <td id="meta1">The latency is not always linear with FLOPs, the accuracy is not high enough.</td>
            </tr>
            <tr>
                <td>Blackbox ML model</td>
                <td id="meta1"><sup><a href="./nn-Meter-Towards-Accurate-Latency-Prediction-of-Deep-Learning-Model-Inference-on-Diverse-Edge-Devices.pdf">[4]</a></sup>nn-Meter<br><sup><a href="./BRP-NAS：Prediction-based NAS using GCNs.pdf">[5]</a></sup>BRP-NAS<br><sup><a href="./TVM：An Automated End-to-End Optimizing Compiler for Deep Learning.pdf">[6]</a></sup>TVM<br><sup><a href="./Learned TPU Cost Model for XLA Tensor Programs.pdf">[7]</a></sup>Learned TPU Cost ...<br><sup><a href="./HELP：Hardware-Adaptive Efficient Latency Prediction for Nas via Meta-Learning.pdf">[13]</a></sup>HELP<br><sup><a href="./Reducing energy consumption of Neural Architecture Search：An inference latency prediction framework.pdf">[14]</a></sup>Reducing energy consumption...<br><sup><a href="./Generalized Latency Performance Estimation for Once-For-All Neural Architecture Search.pdf">[15]</a></sup>Generalized Latency Performance ...<br><sup><a href="./NNLQP：A Multi-Platform Neural Network Latency Query and Prediction System with An Evolving Database.pdf">[16]</a></sup>NNLQP<br><sup><a href="./Predicting Latency of Neural Network Inference.pdf">[17]</a></sup>Predicting Latency of ...</td>
                <td id="meta1">The prediction accuracy is high.</td>
                <td id="meta1">To capture the latency behaviour and achieve high accuracy, the model is complicated, leads to long prediction runtime.</td>
            </tr>
            <tr>
                <td>Consider hardware characteristic</td>
                <td id="meta1"><sup><a href="./paleo：a performance model for deep neural networks.pdf">[3]</a></sup>PALEO<br><sup><a href="./CoDL efficient CPU-GPU co-execution for deep learning inference on mobile devices.pdf">[8]</a></sup>CoDL<br><sup><a href="./FBNet：Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search.pdf">[9]</a></sup>FBNet<br><sup><a href="./Generalized Latency Performance Estimation for Once-For-All Neural Architecture Search.pdf">[15]</a></sup>Generalized Latency Performance ...</td>
                <td id="meta1">With the knowledge of underlying platform, the prediction model could be shrunk, and the predicion process can be more efficient.</td>
                <td id="meta1">Analysing the underlying principle of hardware platform is tough.</td>
            </tr>
            <tr>
                <td>Polynomial Model</td>
                <td id="meta1"><sup><a href="./NeuralPower：Predict and Deploy Energy-Efficient Convolutional Neural Networks.pdf">[10]</a></sup>NeuralPower<br><sup><a href="./Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression.pdf">[11]</a></sup>Predicting Execution Time  ...<br><sup><a href="./Learning to Optimize Halide with Tree Search and Random Programs.pdf">[12]</a></sup>Learning to Optimize Halide ...</td>
                <td id="meta1">Contribute to more flexibility and low prediction error, model complexity can be adjusted by tunning the degree and term number of the polynomial.</td>
                <td id="meta1">The term number and degree of polynomial should be designed elaborately, a model with too much terms will lead to low performance.</td>
            </tr>
            <tr>
                <td>Based on code detail</td>
                <td id="meta1"><sup><a href="./Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression.pdf">[11]</a></sup>Predicting Execution Time  ...<br><sup><a href="./Learning to Optimize Halide with Tree Search and Random Programs.pdf">[12]</a></sup>Learning to Optimize Halide ...</td>
                <td id="meta1">Using code features to assist prediction.</td>
                <td id="meta1">The scalability is poor, because catching features from DNN algorithm is difficult, even some DNN frameworks are closed source.</td>
            </tr>
            <tr>
                <td>Layer-level prediction</td>
                <td id="meta1"><sup><a href="./Neurosurgeon：Collaborative Intelligence Between the Cloud and Mobile Edge.pdf">[1]</a></sup>Neurosurgeon<br><sup><a href="./µLayer：Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization.pdf">[2]</a></sup>µLayer<br><sup><a href="./paleo：a performance model for deep neural networks.pdf">[3]</a></sup>PALEO<br><sup><a href="./CoDL efficient CPU-GPU co-execution for deep learning inference on mobile devices.pdf">[8]</a></sup>CoDL<br><sup><a href="./FBNet：Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search.pdf">[9]</a></sup>FBNet<br><sup><a href="./NeuralPower：Predict and Deploy Energy-Efficient Convolutional Neural Networks.pdf">[12]</a></sup>NeuralPower</td>
                <td id="meta1">Just need to predict the performance of one layer, the block-level prediction process is simplified.</td>
                <td id="meta1">Because the lack of knowledge between operators, the block-level prediction accuracy is low.</td>
            </tr>
            <tr>
                <td>Block-level prediction</td>
                <td id="meta1"><sup><a href="./nn-Meter-Towards-Accurate-Latency-Prediction-of-Deep-Learning-Model-Inference-on-Diverse-Edge-Devices.pdf">[4]</a></sup>nn-Meter<br>nn-Meter<br><sup><a href="./BRP-NAS：Prediction-based NAS using GCNs.pdf">[5]</a></sup>BRP-NAS<br><sup><a href="./Learned TPU Cost Model for XLA Tensor Programs.pdf">[7]</a></sup>Learned TPU Cost ...<br><sup><a href="./HELP：Hardware-Adaptive Efficient Latency Prediction for Nas via Meta-Learning.pdf">[13]</a></sup>HELP<br><sup><a href="./Reducing energy consumption of Neural Architecture Search：An inference latency prediction framework.pdf">[14]</a></sup>Reducing energy consumption...<br><sup><a href="./Generalized Latency Performance Estimation for Once-For-All Neural Architecture Search.pdf">[15]</a></sup>Generalized Latency Performance ...<br><sup><a href="./NNLQP：A Multi-Platform Neural Network Latency Query and Prediction System with An Evolving Database.pdf">[16]</a></sup>NNLQP<br><sup><a href="./Predicting Latency of Neural Network Inference.pdf">[17]</a></sup>Predicting Latency of ...</td>
                <td id="meta1">Take internal connection of block into consideration, achieving high block-level prediction accuracy.</td>
                <td id="meta1">It is hard to catch the internal principle of operators in block.</td>
            </tr>
        </tbody>
    </table>
​    

    <table border="1px">
        <caption>Table2: Comparision on different works</caption>
        <thead>
            <tr>
                <th>Work</th>
                <th>Predictor Size</th>
                <th>Layer-level Accuracy</th>
                <th>Block-level Accuracy</th>
                <th>Scalability</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><sup><a href="./Neurosurgeon：Collaborative Intelligence Between the Cloud and Mobile Edge.pdf">[1]</a></sup>Neurosurgeon</td>
                <td id="meta2">Light</td>
                <td id="meta3">Low</td>
                <td id="meta3">Low</td>
                <td id="meta2">Well</td>
            </tr>
            <tr>
                <td><sup><a href="./µLayer：Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization.pdf">[2]</a></sup>µLayer</td>
                <td id="meta2">Light</td>
                <td id="meta3">Low</td>
                <td id="meta3">Low</td>
                <td id="meta2">Well</td>
            </tr>
            <tr>
                <td><sup><a href="./paleo：a performance model for deep neural networks.pdf">[3]</a></sup>PALEO</td>
                <td id="meta2">Light</td>
                <td id="meta3">Low</td>
                <td id="meta3">Low</td>
                <td id="meta3">Poor</td>
            </tr>
            <tr>
                <td><sup><a href="./nn-Meter-Towards-Accurate-Latency-Prediction-of-Deep-Learning-Model-Inference-on-Diverse-Edge-Devices.pdf">[4]</a></sup>nn-Meter</td>
                <td id="meta3">Heavy</td>
                <td id="meta2">High</td>
                <td id="meta2">High</td>
                <td id="meta3">Poor</td>
            </tr>
            <tr>
                <td><sup><a href="./BRP-NAS：Prediction-based NAS using GCNs.pdf">[5]</a></sup>BRP-NAS</td>
                <td id="meta3">Heavy</td>
                <td id="meta2">High</td>
                <td id="meta2">High</td>
                <td id="meta2">Well</td>
            </tr>
            <tr>
                <td><sup><a href="./TVM：An Automated End-to-End Optimizing Compiler for Deep Learning.pdf">[6]</a></sup>TVM</td>
                <td id="meta3">Heavy</td>
                <td id="meta2">High</td>
                <td id="meta2">High</td>
                <td id="meta2">Well</td>
            </tr>
            <tr>
                <td><sup><a href="./Learned TPU Cost Model for XLA Tensor Programs.pdf">[7]</a></sup>Learned TPU Cost ...</td>
                <td id="meta3">Heavy</td>
                <td id="meta2">High</td>
                <td id="meta2">High</td>
                <td id="meta2">Well</td>
            </tr>
            <tr>
                <td><sup><a href="./CoDL efficient CPU-GPU co-execution for deep learning inference on mobile devices.pdf">[8]</a></sup>CoDL</td>
                <td id="meta2">Light</td>
                <td id="meta2">High</td>
                <td id="meta3">Low</td>
                <td id="meta2">Well</td>
            </tr>
            <tr>
                <td><sup><a href="./FBNet：Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search.pdf">[9]</a></sup>FBNet</td>
                <td id="meta2">Light</td>
                <td id="meta2">High</td>
                <td id="meta3">Low</td>
                <td id="meta3">Poor</td>
            </tr>
            <tr>
                <td><sup><a href="./NeuralPower：Predict and Deploy Energy-Efficient Convolutional Neural Networks.pdf">[10]</a></sup>NeuralPower</td>
                <td id="meta2">Light</td>
                <td id="meta2">High</td>
                <td id="meta3">Low</td>
                <td id="meta2">Well</td>
            </tr>
            <tr>
                <td><sup><a href="./Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression.pdf">[11]</a></sup>Predicting Execution Time  ...</td>
                <td id="meta2">Light</td>
                <td id="meta4">—</td>
                <td id="meta4">—</td>
                <td id="meta3">Poor</td>
            </tr>
            <tr>
                <td><sup><a href="./Learning to Optimize Halide with Tree Search and Random Programs.pdf">[12]</a></sup>Learning to Optimize Halide ...</td>
                <td id="meta2">Light</td>
                <td id="meta4">—</td>
                <td id="meta4">—</td>
                <td id="meta3">Poor</td>
            </tr>
            <tr>
                <td><sup><a href="./HELP：Hardware-Adaptive Efficient Latency Prediction for Nas via Meta-Learning.pdf">[13]</a></sup>HELP</td>
                <td id="meta3">Heavy</td>
                <td id="meta2">High</td>
                <td id="meta2">High</td>
                <td id="meta2">Well</td>
            </tr>
            <tr>
                <td><sup><a href="./Reducing energy consumption of Neural Architecture Search：An inference latency prediction framework.pdf">[14]</a></sup>Reducing energy consumption...</td>
                <td id="meta3">Heavy</td>
                <td id="meta2">High</td>
                <td id="meta2">High</td>
                <td id="meta2">Well</td>
            </tr>
            <tr>
                <td><sup><a href="./Generalized Latency Performance Estimation for Once-For-All Neural Architecture Search.pdf">[15]</a></sup>Generalized Latency Performance ...</td>
                <td id="meta3">Heavy</td>
                <td id="meta2">High</td>
                <td id="meta2">High</td>
                <td id="meta2">Well</td>
            </tr>
            <tr>
                <td><sup><a href="./NNLQP：A Multi-Platform Neural Network Latency Query and Prediction System with An Evolving Database.pdf">[16]</a></sup>NNLQP</td>
                <td id="meta3">Heavy</td>
                <td id="meta2">High</td>
                <td id="meta2">High</td>
                <td id="meta2">Well</td>
            </tr>
            <tr>
                <td><sup><a href="./Predicting Latency of Neural Network Inference.pdf">[17]</a></sup>Predicting Latency of ...</td>
                <td id="meta3">Heavy</td>
                <td id="meta2">High</td>
                <td id="meta2">High</td>
                <td id="meta3">Poor</td>
            </tr>
        </tbody>
</table></body>
</html>

<h2 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h2><p><b>(1) Accurate</b></p>
<p>Simple regression based on FLOPs is infeasible;</p>
<p>Some overhead in the inference process may be considered.</p>
<p><b>(2) Efficient</b></p>
<p>Blackbox neural network with high accuracy is complicated, leads to long delay in execution;</p>
<p>Take some hardware feature into consideration, but pay attention to the scalability.</p>
<p><b>(3) Block-level prediction</b></p>
<p>Summing up the predicted latency of each layer may give rise to accuracy loss, so layer-level latency prediction is not in the consideration;</p>
<p>Operator-level prediction based on operator fusion/ Graph embedding could be a choice.</p>
<h2 id="System-Design"><a href="#System-Design" class="headerlink" title="System Design"></a>System Design</h2><ol>
<li>To make sure the high accuracy of predictor, a general prediction model is trained, the hardware features like accelerator frequency, memory bandwidth  will be  embedded or considered as factors of model. Maybe the Random forest technique would be used to increase accuracy.</li>
<li>To achieve the light-weight characteristic,  I think one of the key contribution of this work could be fine tunning of layer-level predicted latency. In detail, many works have designed light-weight layer-level latency predictor, however their accuracy decrease when a block-level model is predicted, and it is really tough to design a block-level predictor considering accuracy and efficiency at the same time. To resolve above mentioned questions, we put ourselves in another shoes, just use the layer-level predictor, however the latency of each layer should be fine tuned through a little model, which takes hardware configurations into consideration. By the way, LASSO technique could be used to sparse features. </li>
<li>For the scalability, the tunning model would be trained in the offline stage when deployed on a new platform.</li>
</ol>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="1-Find-the-non-linear-latency-response-as-FLOPs-increases"><a href="#1-Find-the-non-linear-latency-response-as-FLOPs-increases" class="headerlink" title="1. Find the non-linear latency response as FLOPs increases"></a>1. Find the non-linear latency response as FLOPs increases</h3><p><b>1）PC experiment</b></p>
<p>*Below Results are measured on a laptop equipped with a Nvidia RTX 3060 GPU &amp; Intel 12th CORE i5 CPU.</p>
<p>Experimental subject: a Conv2D layer (input channel: 32), input a 13*13 data, the output channel is adjusted from 2 to 1024 in the process of experiment.</p>
 <div align="center">
 <img src="/2022/11/02/Lantency-Prediction/CPU latency (PC).png" style="zoom: 20%;">
</div>

<center>Fig.1 Latency response as FLOPs increases (by increasing number of output channel) on CPU</center>

<div align="center">
 <img src="/2022/11/02/Lantency-Prediction/GPU latency (PC).png" style="zoom: 20%;">
</div>

<center>Fig.2 Latency response as FLOPs increases (by increasing number of output channel) on GPU</center>

<p>The results illustrate the non-linear latency response of  CNN layer, the phenomenon is clear in the CPU test. However, the result of test on GPU shows a staircase pattern in the beginning, the trend of latter part is different. The reason for this wired trend is the  influence of GPU frequency, more experiments will be done to figure out the mechanism of this phenomenon.</p>
<p><b>2) Embedded device experiment</b></p>
<p>*Below Results are measured on a Jetson NX development kit.</p>
<p>i. Latency response with different number of output channel</p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/GPU latency against Cout (Jetson NX).png" style="zoom: 20%;">
</div>

<center>Fig.3 Latency response as FLOPs increases (by increasing number of output channel) on GPU</center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/CPU latency (Jetson NX).png" style="zoom: 20%;">
</div>

<center>Fig.4 Latency response as FLOPs increases (by increasing number of output channel) on CPU</center>

<p>ii. Latency response with different input size</p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/GPU latency against height (Jetson NX).png" style="zoom: 20%;">
</div>

<center>Fig.5 Latency response as FLOPs increases (by increasing number of input height) on GPU</center>

<h3 id="2-Some-interesting-findings"><a href="#2-Some-interesting-findings" class="headerlink" title="2. Some interesting findings"></a>2. Some interesting findings</h3><p><b>1) Sudden latency decrease when FLOPs increase</b></p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/Latency under dynamic frequency.png" style="zoom: 20%;">
</div>

<center>Fig.6 Sudden latency decrease under dynamic GPU frequency</center>

<p>From the experiment, I found that the frequency of GPU is dynamic for some reasons, which results in a couple of breakpoint in the FLOPs-Latency curve.</p>
<p><b>2) Inference latency performance with variable GPU frequency</b></p>
<p>Based on the phenomenon in the previous finding, I locked GPU frequency and measured inference latency with continuous output channel number under variable GPU frequency. We can find that the latency curve under different GPU almost follow the same pattern. </p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/Latency with variable GPU frequency.png" style="zoom: 20%;">
</div>

<center>Fig.7 Inference latency with continuous output channel number under variable GPU frequency</center>

<p>Further, I display the ratio of latency under different GPU frequency, and use 114.75MHz as basic frequency.</p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/ratio of latency under variable frequency.png" style="zoom: 20%;">
</div>

<center>Fig.8 The ratio of latency under different GPU frequency</center>

<p>We can get some useful information from Fig.8. First, the latency ratio of two different GPU frequency convergence to a fixed value with the increase of FLOPs. More specifically, that fixed value approach to the reciprocal of frequency ratio with small deviation.</p>
<p><b>3) Inference latency related to output channel and input height</b></p>
<p>To analyze even further, I observed the relationship between computing FLOPs and inference latency. However, the step shape latency pattern shows no clear relationship with FLOPs. Nevertheless, I found some special laws in GPU inference. I measured a series of inference latency with continuous input height based on different latency.</p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/Latency related to output channel and input height.png" style="zoom: 20%;">
</div>

<center>Fig.9 Inference latency related to output channel and input height</center>

<p>We can figure out below findings:</p>
<p>i. The step pattern with input height has no relation with output channel, and the intervals between every two adjacent step levels are equal, which is 32 on test equipment.</p>
<p>ii. the latency curve has some sudden rise on typical output channel. Coincidentally, these special points are exactly the step points in Fig.7, which shows that the influence of input height and output channel are independent, and doesn’t affect each other.</p>
<h3 id="3-Layer-level-latency-predictor-design"><a href="#3-Layer-level-latency-predictor-design" class="headerlink" title="3.Layer-level latency predictor design"></a>3.Layer-level latency predictor design</h3><div align="center">
<img src="/2022/11/02/Lantency-Prediction/Latency related to output channel and input weight.png" style="zoom: 20%;">
</div>

<center>Fig.10 Inference latency related to output channel and input weight</center>

<p>As Fig.9 and Fig.10 illustrated, the $C_{out}$ deploys stepped latency curve when $C_{out}$ change on the latency pattern impacted by other parameters. So, I extract the influence of $C_{out}$ as a special term independent of other items. Next, I will deduce the special term in algebra.</p>
<p>We suppose the fundamental latency function without the influence of the special term as $f_0(x)$, the actual latency function $f_{co}^{p}(x)=(a_{co}+ns_{co})f_0(x)$, in where $a_{co}$ is the influence coefficient respects to the first step of $C_{out}$ pattern, $s_{co}$ is the step height of $C_{out}$ pattern. If we get all the above values, the actual function $f_{co}^{p}(x)$ can be determined, however, all the data we can measured has been influenced by the special term, we cannot capture $f_0(x)$ directly. What’s more, the $C_{out}$ pattern is also influenced by other parameters, we cannot get $a_{co}$ and $s_{co}$ through measurement either. To deal with above difficulty, we can use the following method.</p>
<p>First, we can measure two latency curve with adjacent step numbers, denote as $[a_{co}+n_{co}s_{co}]f_0(x)$ and $[a_{co}+(n_{co}+1)s_{co}]f_0(x)$, and divide the two to get a convergent result $k_{co}$.</p>
<script type="math/tex; mode=display">\frac{[a_{co}+(n_{co}+1)s_{co}]f_0(x)}{[a_{co}+n_{co}s_{co}]f_0(x)}\rightarrow k_{co}</script><p>Through the derivation, we get $\frac{s_{co}}{a_{co}}=\frac{k_{co}-1}{1+n_{co}-n_{co}k_{co}}$,</p>
<p>The predicted latency under $C_{out}$ dimension is $f_{co}^{p}(x)=(a_{co}+ns_{co})f_0(x)=\frac{a_{co}+ns_{co}}{a_{co}+n_{co}s_{co}}(a_{co}+n_{co}s_{co})f_0(x)=\frac{1+n\frac{s_{co}}{a_{co}}}{1+n_{co}\frac{s_{co}}{a_{co}}}(a_{co}+n_{co}s_{co})f_0(x)$</p>
<p>$=\frac{1+\lceil\frac{C_o}{l_{co}}-1\rceil\frac{s_{co}}{a_{co}}}{1+n_{co}\frac{s_{co}}{a_{co}}}(a_{co}+n_{co}s_{co})f_0(x)$, ($l_{co}$ is the step length of $C_{out}$ pattern).</p>
<p>Therefore, we can get the latency patterns under distinct $C_{out}$ through multiplying the latency pattern under the measured curve $[a_{co}+n_{co}s_{co}]f_0(x)$ by a term $\frac{1+\lceil\frac{C_o}{l_{co}}-1\rceil\frac{s_{co}}{a_{co}}}{1+n_{co}\frac{s_{co}}{a_{co}}}$.</p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/Latency related to output height and input channel.png" style="zoom: 20%;">
</div>

<center>Fig.11 Inference latency related to input height and input channel</center>


<div align="center">
<img src="/2022/11/02/Lantency-Prediction/Latency related to output weight and input channel.png" style="zoom: 20%;">
</div>

<center>Fig.12 Inference latency related to input weight and input channel</center>

<p>As Fig.10 and Fig.11 illustrated, the same as the influence of $C_{out}$ on the other dimension of parameters, the input height and weight dimensions show step influence on the inference latency when the input channel changes. Therefore, we compress the four dimensions of latency curve ($\textit{i.e.}, C_{out}, H, W, C_{in}$) into the $C_{in}$ dimension. The specific process is as below.</p>
<p>We first measure four curves with GPU frequency $F_0$:</p>
<script type="math/tex; mode=display">f{(C_i)}=(C_i ; H^{(n_H)},W^{(n_W)},C_o^{(n_{co})}),</script><script type="math/tex; mode=display">f_H{(C_i)}=(C_i ; H^{(n_H+1)},W^{(n_W)},C_o^{(n_{co})}),</script><script type="math/tex; mode=display">f_W{(C_i)}=(C_i ; H^{(n_H)},W^{(n_W+1)},C_o^{(n_{co})}),</script><script type="math/tex; mode=display">f_{co}{(C_i)}=(C_i ; H^{(n_H)},W^{(n_W)},C_o^{(n_{co}+1)}).</script><p>The predicted latency curve under four dimensions is expressed as $f_p(H,W,C_o,C_i)=T_H(H)T_W(W)T_{C_o}(C_o)f(Ci)$.</p>
<p>Where,</p>
<p>$T_H(H)=\frac{1+\lceil\frac{H}{l_H}-1\rceil\frac{s_H}{a_H}}{1+n_H\frac{s_H}{a_H}}$, $\frac{s_H}{a_H}=\frac{k_H-1}{1+n_H-n_Hk_H}$, $k_H\rightarrow\frac{f_H(C_i)}{f(C_i)}$,</p>
<p>$T_W(W)=\frac{1+\lceil\frac{W}{l_W}-1\rceil\frac{s_W}{a_W}}{1+n_W\frac{s_W}{a_W}}$, $\frac{s_W}{a_W}=\frac{k_W-1}{1+n_W-n_Wk_W}$, $k_W\rightarrow\frac{f_W(C_i)}{f(C_i)}$,</p>
<p>$T_{C_o}(H)=\frac{1+\lceil\frac{C_o}{l_{C_o}}-1\rceil\frac{s_{C_o}}{a_{C_o}}}{1+n_{C_o}\frac{s_{C_o}}{a_{C_o}}}$, $\frac{s_{C_o}}{a_{C_o}}=\frac{k_{C_o}-1}{1+n_{C_o}-n_{C_o}k_{C_o}}$, $k_{C_o}\rightarrow\frac{f_{C_o}(C_i)}{f(C_i)}$.</p>
<p>Based on the finding in section 2-2), we add a special term to the predicted function, which reflects the influence of GPU frequency variation, as below:</p>
<script type="math/tex; mode=display">f_{co}{(C_i)}=(C_i ; H^{(n_H)},W^{(n_W)},C_o^{(n_{co}+1)})\times F,</script><script type="math/tex; mode=display">F=\frac{F_0}{F_c}.</script><p>$F_c$ is the current frequency of GPU.</p>
<h3 id="4-Layer-level-latency-predictor-evaluation"><a href="#4-Layer-level-latency-predictor-evaluation" class="headerlink" title="4. Layer-level latency predictor evaluation"></a>4. Layer-level latency predictor evaluation</h3><p>We evaluate the performance of the layer-level latency predictor preliminarily under two parameter settings.</p>
<p>The only priori data we have is four curves as below figure shows.</p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/evaluation priori data.png" style="zoom: 20%;">
</div>

<center>Fig.13 Priori data of four curves</center>

<p>The comparison  between the predicted latency curve and real measured latency curve under two parameter settings is as below.</p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/evaluation comparision 1.png" style="zoom: 20%;">
</div>

<center>Fig.14 Prediction performance evaluation (H=92, W=68, Co=72) [Avg Error: 4.6%]</center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/evaluation comparision 2.png" style="zoom: 20%;">
</div>

<center>Fig.15 Prediction performance evaluation (H=48, W=80, Co=160) [Avg Error: 7.5%]</center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/Layer-level Prediction under variable frequency.png" style="zoom: 20%;">
</div>

<center>Fig.16 Layer-level latency prediction under variable GPU frequency [Avg Error: 12.5%]</center>


<h3 id="5-Operator-fusion"><a href="#5-Operator-fusion" class="headerlink" title="5. Operator fusion"></a>5. Operator fusion</h3><p><b>1) PyTorch</b></p>
<p>Below experiment is set to explore the optimization when execute some specific operators in sequence, in which the operator fusion is the most vital one. This experiment is executed on PyTorch framework and measured inference latency of 1) single Conv operator, 2) single bn operator, 3) Conv+bn in sequence under variable output channels, and the sum of 1) and 2) is calculated in compare with latency of 3). </p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/conv+bn(64×64).png" style="zoom: 20%;">
</div>

<center>Fig.17 Difference of connected and separated operators < conv+bn > in PyTorch</center>

<p>From the result, we can find that 3) consumes less latency than sum of 1) and 2), which maybe caused by the framework optimization.</p>
<p>Further more, a series of experiments on different combination of operators to figure out  whether the operator fusion optimization exist when processing DNN models based on deep learning framework, e.g., PyTorch in this experiment.</p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/conv+bn+relu(64×64).png" style="zoom: 20%;">
</div>

<center>Fig.18 Difference of connected and separated operators < conv+bn+relu >  in PyTorch</center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/bn+conv(64×64).png" style="zoom: 20%;">
</div>

<center>Fig.19 Difference of connected and separated operators < bn+conv >  in PyTorch</center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/conv+relu(64×64).png" style="zoom: 20%;">
</div>

<center>Fig.20 Difference of connected and separated operators < conv+relu >  in PyTorch</center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/pool+bn(64×64).png" style="zoom: 20%;">
</div>

<center>Fig.21 Difference of connected and separated operators < pool+bn >  in PyTorch</center>

<p>The average latency reduction ratio of connected and separated operators is recorded in below table.</p>
<center><b>Table3. The average latency reduction ratio of connected and separated operators</b></center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/latency reduction ratio.png" style="zoom: 100%;">
</div>

<p>However, when increasing the computation, we get below experiment results.</p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/pool+bn(224×224).png" style="zoom: 20%;">
</div>

<center>Fig.22 Difference of connected and separated operators < pool+bn >  in PyTorch (high computation)</center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/conv+bn(224×224).png" style="zoom: 20%;">
</div>

<center>Fig.23 Difference of connected and separated operators < conv+bn >  in PyTorch (high computation)</center>

<center><b>Table4. The average latency reduction ratio of connected and separated operators</b></center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/latency reduction ratio (high computation).png" style="zoom: 50%;">
</div>

<p>When the computation increase, the latency reduction ratio of connected operators decrease sharply. This phenomenon shows that the PyTorch framework may not do operator fusion optimization for operators, and the optimization performance could caused by other optimization techniques or error in measurement.</p>
<p><b>2) TensorRT</b></p>
<p>In this part, we explore the operator fusion optimization performance in TensorRT, which is a model inference acceleration framework produced by NVIDIA. All the operators are packed into model and processed by a TensorRT engine to a TensorRT model, below figures illustrate the latency performance difference of connected operators and separate operators.</p>
<div align="center">
<img src="/2022/11/02/Lantency-Prediction/Trt-conv+bn(224×224).png" style="zoom: 20%;">
</div>

<center>Fig.24 Difference of connected and separated operators < conv+bn >  in TensorRT</center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/Trt-conv+relu(224×224).png" style="zoom: 20%;">
</div>

<center>Fig.25 Difference of connected and separated operators < conv+relu >  in TensorRT</center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/Trt-conv+pool(224×224).png" style="zoom: 20%;">
</div>

<center>Fig.26 Difference of connected and separated operators < conv+pool >  in TensorRT</center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/Trt-conv+conv(224×224).png" style="zoom: 20%;">
</div>

<center>Fig.27 Difference of connected and separated operators < conv+conv >  in TensorRT</center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/Trt-bn+add(224×224).png" style="zoom: 20%;">
</div>

<center>Fig.28 Difference of connected and separated operators < bn+add >  in TensorRT</center>

<center><b>Table5. The average latency reduction ratio of connected and separated operators</b></center>

<div align="center">
<img src="/2022/11/02/Lantency-Prediction/latency reduction ratio(TensorRT).png" style="zoom: 100%;">
</div>

<p>Through the above results, we can reach a conclusion that, TensorRT do employs operator fusion optimization to some kinds of operators. The difference between connected and separated operators  when operator fusion happens is much higher than PyTorch, which doesn’t do operator fusion optimization for operators. </p>
<h3 id="6-Predictor-Design"><a href="#6-Predictor-Design" class="headerlink" title="6. Predictor Design"></a>6. Predictor Design</h3><div align="center">
<img src="/2022/11/02/Lantency-Prediction/Predictor Architecture.png" style="zoom: 30%;">
</div>

<center>Fig.29 Predictor Architecture</center>

<h2 id="Alternative-Techniques"><a href="#Alternative-Techniques" class="headerlink" title="Alternative Techniques"></a>Alternative Techniques</h2><p>Polynomial model</p>
<p>Feature sparsing</p>
<p>Random forest</p>
<p>Graph embedding</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul class="copyright-info-content">
        
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

            </div>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/2023/03/26/MUSES/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">MUSES：Massive UAV Swarm Elastic collaborative intelligence System</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2022/10/31/Elastic-Collaborative-Edge-Intelligence-for-UAVs/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Elastic Collaborative Edge Intelligence for UAVs</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments"></i>&nbsp;Comments
    </div>
    

        
            
    <div class="valine-container">
        <script 
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script >
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'AIhtvUXrzUWmPIOmYNjrGExW-gzGzoHsz',
                    appKey: 'e4HGIfsWiHCnIXco4g7xtuAA',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: 'Add your comments',
                    lang: 'en'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Sun Hao';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('false') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            
<footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
                <span>2022</span> -
            
            2023
            
                &nbsp;<i class="fas fa-heart icon-animate"></i>
                &nbsp;<a href="/">Sun Hao</a>
            
        </div>
        
            <script async 
                    src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                
                
                    Totalview&nbsp;<span id="busuanzi_value_site_pv"></span>
                
            </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme
            &nbsp;
            <a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.8</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-up-right-and-down-left-from-center"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Study-Survey"><span class="nav-text">Study Survey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Target"><span class="nav-text">Target</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#System-Design"><span class="nav-text">System Design</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Experiments"><span class="nav-text">Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Find-the-non-linear-latency-response-as-FLOPs-increases"><span class="nav-text">1. Find the non-linear latency response as FLOPs increases</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Some-interesting-findings"><span class="nav-text">2. Some interesting findings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Layer-level-latency-predictor-design"><span class="nav-text">3.Layer-level latency predictor design</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Layer-level-latency-predictor-evaluation"><span class="nav-text">4. Layer-level latency predictor evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Operator-fusion"><span class="nav-text">5. Operator fusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Predictor-Design"><span class="nav-text">6. Predictor Design</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Alternative-Techniques"><span class="nav-text">Alternative Techniques</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="close-popup-btn">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>





    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-block-tools.js"></script>





<div class="post-scripts">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
