<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Sun Hao">
    
    <title>
        
            深度学习方法探析——浅层神经网络 |
        
        SnSpace
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/Slogo.svg">
    
<link rel="stylesheet" href="/fontawesome/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/regular.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/brands.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"example.com","root":"/","language":"en"};
    KEEP.theme_config = {"toc":{"enable":false,"number":false,"expand_all":false,"init_open":false},"style":{"primary_color":"#0066CC","logo":"/images/Slogo.svg","favicon":"/images/Slogo.svg","avatar":"/images/avatar.svg","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":false,"header_transparent":false,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving.","font_color":null,"hitokoto":{"enable":false}},"scroll":{"progress_bar":{"enable":false},"percent":{"enable":false}}},"local_search":{"enable":false,"preload":false},"code_copy":{},"code_block_tools":{"enable":false,"style":"default"},"side_tools":{},"pjax":{"enable":false},"lazyload":{"enable":false},"version":"3.4.8"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    KEEP.language_code_block = {"copy":"Copy code","copied":"Copied","fold":"Fold code block","folded":"Folded"};
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/Slogo.svg">
                </a>
            
            <a class="logo-title" href="/">
               SnSpace
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                    
                </ul>
            </div>
            <div class="mobile">
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">深度学习方法探析——浅层神经网络</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Sun Hao</span>
                        
                            <span class="author-label">Lv1</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2022-10-06 19:03:52</span>
        <span class="mobile">2022-10-06 19:03</span>
    </span>
    
    

    
    
    
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <h1 id="1-逻辑回归"><a href="#1-逻辑回归" class="headerlink" title="1.逻辑回归"></a>1.逻辑回归</h1><p>逻辑回归可以理解为不具备<em>hidden layer</em>(隐藏层)的神经网络，其模型如图1所示。逻辑回归能通过线性计算预测输入特征值对应的输出，其中的特征权重<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image002.gif" alt="img">和偏差<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image004.gif" alt="img">决定着预测的准确度，因此获得合适的<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image002.gif" alt="img">和<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image004.gif" alt="img">是此模型的重点。使用输入加权的原因可以从人的神经元中找到答案，图2是人的神经元模型，可能同时有多个电信号作用于神经元，当它们的加权和大于神经元的阈值时，其就会在轴突产生输出电信号，神经网络就是由此发展而来的。</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image006.gif" alt="img">  <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image008.gif" alt="img"></p>
<p>图1 逻辑回归模型          图2 神经元模型</p>
<p><strong>1.1</strong> <strong>梯度下降法逻辑回归</strong></p>
<p>利用给定的一组带标记的训练集进行代数运算获取<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image002.gif" alt="img">和<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image004.gif" alt="img">是一项艰巨的工作，甚至是难以完成的，人工智能的发展也因此一度停滞不前。梯度下降法的提出解决了这一问题，虽然其获得的可能不是最优解，但至少让这一过程有解。梯度下降法的思路是：初始随机选择参数，根据预测值与期望值的偏差反向传播误差，进而调整参数的大小。就像从山的任意位置沿着梯度的方向朝着谷底前进，谷底就是预测偏差最小的位置，也就是模型训练的目标点。表1列出了本文会用到的符号解释。</p>
<p><strong>表****1</strong> <strong>本文用到的符号及解释</strong></p>
<table>
<thead>
<tr>
<th><strong>符号</strong></th>
<th><strong>解释</strong></th>
<th><strong>维数</strong></th>
</tr>
</thead>
<tbody><tr>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image010.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image012.gif" alt="img">个输入样本</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image014.gif" alt="img"></td>
</tr>
<tr>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image016.gif" alt="img"></td>
<td>实际值</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image018.gif" alt="img"></td>
</tr>
<tr>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image002.gif" alt="img"></td>
<td>特征权重</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image020.gif" alt="img"></td>
</tr>
<tr>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image004.gif" alt="img"></td>
<td>偏差</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image020.gif" alt="img"></td>
</tr>
<tr>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image022.gif" alt="img"></td>
<td>预测值</td>
<td>实数</td>
</tr>
<tr>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image024.gif" alt="img"></td>
<td>学习率</td>
<td>实数</td>
</tr>
<tr>
<td>损失函数</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image026.gif" alt="img"></td>
<td>实数</td>
</tr>
<tr>
<td>代价函数</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image028.gif" alt="img"></td>
<td>实数</td>
</tr>
</tbody></table>
<p>当输入值多于一个时，需要程序循环多次以计算对应的预测值，但在编程时，大量使用for循环会极大影响程序执行速度。为了解决这个问题，我们尽量采用向量的形式表示各个值，应用python内嵌函数可以轻松地处理向量的运算，经过比较，向量形式比for循环形式快数百倍。因此，本文均采用向量形式进行计算。</p>
<p>逻辑回归的过程可以分为forward propagation (向前传播)和backward propagation(向后传播)两个过程。向前传播通过上面所述的过程计算当前参数对应预测值的代价(或损失)，向后传播采用链式法则计算总代价(或总损失)对各参数的偏导数，根据设定的学习率更新参数。之后重复前述操作，直到达到理想的代价或者迭代次数为止。</p>
<p>具体的逻辑回归模型训练过程如下：</p>
<ol>
<li>设定参数<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image030.gif" alt="img">的初值；</li>
</ol>
<p>循环训练次数：</p>
<ol start="2">
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image032.gif" alt="img">；</p>
</li>
<li><pre><code>![img](file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image034.gif)；
</code></pre>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image036.gif" alt="img">；</p>
</li>
<li><pre><code>![img](file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image038.gif)；
</code></pre>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image040.gif" alt="img">；</p>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image042.gif" alt="img">；</p>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image044.gif" alt="img">；</p>
</li>
<li><p>得到<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image002.gif" alt="img">和<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image004.gif" alt="img">.</p>
</li>
</ol>
<p>现对4)、5)、6)式进行适量推导：</p>
<p>​                  <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image046.gif" alt="img">                                         (1)</p>
<p>推广到向量形式可得</p>
<p>​                                     <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image036.gif" alt="img">                  (2)</p>
<p>利用(2)式可计算出<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image048.gif" alt="img">和<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image050.gif" alt="img"></p>
<p>   <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image052.gif" alt="img">                         (3)</p>
<p>   <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image054.gif" alt="img">                              (4)</p>
<p><strong>1.2</strong> <strong>编程实现逻辑回归</strong></p>
<p>现通过python编程搭建逻辑回归模型实现“猫识别”，程序采用模块化方式编写，用到的函数如表2所示。本文采用可下载的h5文件数据集进行模型训练，也可采用CSV文件数据集。</p>
<p><strong>表<strong><strong>2 “</strong></strong>猫识别<strong><strong>”</strong></strong>程序用到的函数</strong></p>
<table>
<thead>
<tr>
<th><strong>函数名</strong></th>
<th><strong>功能</strong></th>
<th><strong>输入</strong></th>
<th><strong>输出</strong></th>
</tr>
</thead>
<tbody><tr>
<td>sigmoid(z)</td>
<td>计算<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image056.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image058.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image060.gif" alt="img"></td>
</tr>
<tr>
<td>initialize_with_zeros(dim)</td>
<td>初始化参数值</td>
<td>单样本长度</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image062.gif" alt="img"></td>
</tr>
<tr>
<td>propagate(w, b, X, Y)</td>
<td>计算正向传播和反向传播</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image064.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image066.gif" alt="img"></td>
</tr>
<tr>
<td>optimize(w,b,X,Y,num_iterations,  learning_rate,print_cost &#x3D; False)</td>
<td>更新参数</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image068.gif" alt="img">,迭代次数</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image070.gif" alt="img"></td>
</tr>
<tr>
<td>predict(w, b, X)</td>
<td>预测</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image072.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image022.gif" alt="img"></td>
</tr>
</tbody></table>
<p>本文模型训练迭代10000次，学习率采用0.005。运行程序，可以看到总代价随着训练次数逐渐减小，如图3所示。</p>
<p>下面随意下载一张猫的图片交给程序进行预测，图4为交给程序的猫图片，程序对图片预测后输出该图片是猫的概率，若概率大于0.5则判定该图片有猫，并显示预测结果，如图5所示。可以看到，程序预测该图片有99.66%的概率为猫。</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image074.jpg" alt="img"></p>
<p>图3 逻辑回归程序不同迭代次数的总代价</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image076.jpg" alt="img"><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image078.jpg" alt="img"></p>
<p>图4 预测图片（含猫）          图5 预测结果（含猫）</p>
<p>为了测试程序不会把所有图片认定为猫，再对一不含猫的图片进行预测，如图6所示。预测结果如图6所示，程序预测该图片有33.74%的概率为猫，认为这不是一张含有猫的图片。可以看出，程序可以正确分辨有无猫在图片中。</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image080.jpg" alt="img"><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image082.jpg" alt="img"></p>
<p>图6 预测图片（不含猫）         图7 预测结果（不含猫）</p>
<p>经过多次测试，发现程序对正面较清晰的猫识别概率很高，有些情况下存在识别错误。另外，对于一些与猫相近的狗的正面照也存在识别为猫的情况。</p>
<p>\1.   浅层神经网络</p>
<p>逻辑回归可以看为只有一层的神经网络，也可以理解为人脑的一个神经元，将多个逻辑回归中的单元叠起来就变成了神经网络，如图8所示。其中，输入层也称为第0层，因此下图的神经网络被称为2层神经网络。</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image084.gif" alt="img"></p>
<p>图8 神经网络模型</p>
<p><strong>2.1****神经网络的表示</strong></p>
<p>神经网络的节点数比逻辑回归多，需要对逻辑回归的向量形式做一定的调整。本文采用上标 [1]表示第1层神经网络，第0层的<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image010.gif" alt="img">可用<strong>A</strong>[0]表示，<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image002.gif" alt="img">从之前的1维向量变为矩阵<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image086.gif" alt="img">，其中，为方便后续计算，已将对应的<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image088.gif" alt="img">进行转置，如(5)式所示。</p>
<p>​                                   <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image090.gif" alt="img">               (5)</p>
<p>每层的输出<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image092.gif" alt="img">可通过(6)式计算</p>
<p>​                                <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image094.gif" alt="img">             (6)</p>
<p>其中，<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image096.gif" alt="img">相当于第0层的输入，即<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image010.gif" alt="img">。下面以第1层为例推导该过程的维度对应关系：</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image098.gif" alt="img"></p>
<p><strong>1.1.1</strong> <strong>forward propagation(<strong><strong>正向传播</strong></strong>)</strong></p>
<p>只有2层的浅层神经网络的正向传播可由以下4个式子求得</p>
<p>​                                  <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image100.gif" alt="img">              (7)</p>
<p>​                                    <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image102.gif" alt="img">                 (8)</p>
<p>​                                 <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image104.gif" alt="img">             (9)</p>
<p>​                                    <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image106.gif" alt="img">                (10)</p>
<p><strong>1.1.2</strong> <strong>backward propagation(<strong><strong>反向传播</strong></strong>)</strong></p>
<p>反向传播可总结为以下6个公式 (输出层采用sigmoid函数作为激活函数)</p>
<p>​                                   <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image108.gif" alt="img">               (11)</p>
<p>​                                 <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image110.gif" alt="img">            (12)</p>
<p>​                                 <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image112.gif" alt="img">            (13)</p>
<p>​                            <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image114.gif" alt="img">        (14)</p>
<p>​                                  <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image116.gif" alt="img">             (15)</p>
<p>​                                 <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image118.gif" alt="img">            (16)</p>
<p>其中，(11)、(12)、(13)、(15)和(16)式原理同(2)-(4)式，现对(14)式进行适当推导。运用链式法则，可得</p>
<p>​                            <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image120.gif" alt="img">       (17)</p>
<p>其中，</p>
<p>​                                   <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image122.gif" alt="img">              (18)</p>
<p>​                         <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image124.gif" alt="img">     (19)</p>
<p>综合(17)-(19)式以上各式可得</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image114.gif" alt="img"></p>
<p><strong>1.1.3</strong> <strong>浅层神经网络训练过程</strong></p>
<p>具体的浅层神经网络训练过程如下(以2层为例)：</p>
<ol>
<li>设定参数<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image126.gif" alt="img">的初值；</li>
</ol>
<p>循环训练次数：</p>
<ol start="2">
<li><pre><code>![img](file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image100.gif)；
</code></pre>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image102.gif" alt="img">；</p>
</li>
<li><pre><code>![img](file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image104.gif)；
</code></pre>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image106.gif" alt="img">；</p>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image108.gif" alt="img">；</p>
</li>
<li><pre><code>![img](file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image110.gif)；
</code></pre>
</li>
<li><pre><code>![img](file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image112.gif)；
</code></pre>
</li>
<li><pre><code>![img](file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image114.gif)；
</code></pre>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image116.gif" alt="img">；</p>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image118.gif" alt="img"></p>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image128.gif" alt="img">；</p>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image130.gif" alt="img">；</p>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image132.gif" alt="img">；</p>
</li>
<li><p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image134.gif" alt="img">；</p>
</li>
<li><p>得到<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image136.gif" alt="img">、<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image138.gif" alt="img">和<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image140.gif" alt="img">、<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image142.gif" alt="img">.</p>
</li>
</ol>
<p><strong>1.1.4</strong> <strong>激活函数的选择</strong></p>
<p>神经网络需要非线性的激活函数，因为如果激活函数是线性的，那么无论有多少隐藏层，都可以看作是一组线性组合，那么都会浓缩为1层，这显然是毫无意义的。在逻辑回归中用到的是sigmoid函数，如图9(a)所示，然而另外一种非线性函数tanh在大多数情况是更受欢迎的，如图9(b)所示，它覆盖了-1到1的值域，这在神经网络中会取得更好的效果，但如果模型所解决的是一个二分类问题，期望得到的是0或1，可以考虑在输出层使用sigmoid函数。这两个函数都存在一个问题，当<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image058.gif" alt="img">太大或太小时斜率接近于0，这会导致学习饱和，所以可以采用ReLu函数或Leaky ReLu函数代替，如图9(c)、(d)所示，他们都可以保证<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image058.gif" alt="img">较大时不错的收敛速度。</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image144.gif" alt="img">   <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image146.gif" alt="img"></p>
<p>(a)sigmoid函数            (b)tanh函数</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image148.gif" alt="img">   <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image150.gif" alt="img"></p>
<p>(c)ReLu函数            (d)Leaky ReLu函数</p>
<p>图9 几种激活函数</p>
<p>前述计算过程中，激活函数<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image152.gif" alt="img">的导数经常被用到，因此本文将几种激活函数的导数整理到表3中。</p>
<p><strong>表****3</strong> <strong>几种激活函数的导数</strong></p>
<table>
<thead>
<tr>
<th><strong>函数</strong></th>
<th><strong>公式表示</strong></th>
<th><strong>导数</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Sigmoid</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image154.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image156.gif" alt="img"></td>
</tr>
<tr>
<td>tanh</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image158.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image160.gif" alt="img"></td>
</tr>
<tr>
<td>ReLu</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image162.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image164.gif" alt="img"></td>
</tr>
<tr>
<td>Leaky ReLu</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image166.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image168.gif" alt="img"></td>
</tr>
</tbody></table>
<p><strong>1.1.5</strong> <strong>随机初始化</strong></p>
<p>训练神经网络时，权重随机初始化是十分重要的。当把权重<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image170.gif" alt="img">全部初始化为0时，同层的隐藏单元计算着同一个函数，他们对输出的影响也全部相同，无论经过多少次迭代他们都计算着相同的函数。也就是说，这和只有一个隐藏单元没有任何差别。为了解决这一问题，我们应当将<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image170.gif" alt="img">随机初始化，而<img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image004.gif" alt="img">没有这个问题，可以将其初始化为0。</p>
<p><strong>2.2****编程实现浅层神经网络</strong></p>
<p>现编程搭建只有一个隐藏层的浅层神经网络，隐藏层设置5个节点，用到的函数如表4所示。</p>
<p><strong>表****4</strong> <strong>浅层神经网络用到的函数</strong></p>
<table>
<thead>
<tr>
<th><strong>函数名</strong></th>
<th><strong>功能</strong></th>
<th><strong>输入</strong></th>
<th><strong>输出</strong></th>
</tr>
</thead>
<tbody><tr>
<td>layer_sizes(X, Y)</td>
<td>计算输入、输出层节点数</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image172.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image174.gif" alt="img"></td>
</tr>
<tr>
<td>initialize_parameters(n_x,  n_h, n_y)</td>
<td>初始化参数值</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image176.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image178.gif" alt="img"></td>
</tr>
<tr>
<td>forward_propagation(X,  parameters)</td>
<td>计算正向传播</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image180.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image182.gif" alt="img"></td>
</tr>
<tr>
<td>backward_propagation(parameters,  cache, X, Y)</td>
<td>计算反向传播</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image184.gif" alt="img">  <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image172.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image186.gif" alt="img">  <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image188.gif" alt="img"></td>
</tr>
<tr>
<td>compute_cost(A2, Y,  parameters)</td>
<td>计算代价</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image190.gif" alt="img"></td>
<td>cost</td>
</tr>
<tr>
<td>update_parameters(parameters,  grads, learning_rate &#x3D; 0.005)</td>
<td>更新参数</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image192.gif" alt="img">  <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image186.gif" alt="img">  <img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image188.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image178.gif" alt="img"></td>
</tr>
<tr>
<td>nn_model(X, Y, n_h,  num_iterations &#x3D; 10000, print_cost&#x3D;False)</td>
<td>模型训练</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image194.gif" alt="img">迭代次数</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image178.gif" alt="img"></td>
</tr>
<tr>
<td>predict(parameters, X)</td>
<td>预测</td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image180.gif" alt="img"></td>
<td><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image022.gif" alt="img"></td>
</tr>
</tbody></table>
<p>为了方便比较，本文采用与逻辑回归程序相同的数据集进行训练，迭代次数同为10000次，总代价的变化如图10所示。可以发现，相同的迭代次数下，浅层神经网络比逻辑回归单节点网络的代价0.035704要小。</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image196.jpg" alt="img"></p>
<p>图10 浅层神经网络程序不同迭代次数的总代价</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image198.jpg" alt="img"><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image200.jpg" alt="img"></p>
<p>图11 预测图片（含猫）          图12 预测结果（含猫）</p>
<p>现使用与逻辑回归程序相同的两张图片对浅层神经网络进行测试。如图11所示，程序预测该图片有99.51%的概率为猫图片，相比逻辑回归单节点99.66%的概率略有降低，但两种情况下预测的概率都接近100%，可以认为识别效果都十分不错，并不能看出优劣。为了更加清晰地比较两种模型，我们选取一张不太标准的猫图片进行识别，如图13所示。</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image202.jpg" alt="img"></p>
<p>图13 非标准含猫图片</p>
<p>逻辑回归的单节点模型预测的结果如图14所示，浅层神经网络预测的结果如图15所示。</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image204.jpg" alt="img"> </p>
<p>图14 逻辑回归单节点模型预测结果图</p>
<p><img src="file:///C:/Users/孙浩/AppData/Local/Temp/msohtmlclip1/01/clip_image206.jpg" alt="img"></p>
<p>15 浅层神经网络预测结果</p>
<p>可以看到，浅层神经网络的预测结果比单节点模型概率高8.86%，浅层神经网络的预测能力更强。</p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul class="copyright-info-content">
        <li>Post title：深度学习方法探析——浅层神经网络</li>
        <li>Post author：Sun Hao</li>
        <li>Create time：2022-10-06 19:03:52</li>
        <li>
            Post link：http://example.com/2022/10/06/深度学习方法探析——浅层神经网络/
        </li>
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

            </div>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/2022/10/06/test1/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">test1</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2022/10/06/hello-world/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Hello World</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            
<footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
                <span>2020</span> -
            
            2022
            
                &nbsp;<i class="fas fa-heart icon-animate"></i>
                &nbsp;<a href="/">Sun Hao</a>
            
        </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme
            &nbsp;
            <a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.8</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-up-right-and-down-left-from-center"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    

    <div class="image-viewer-container">
    <img src="">
</div>


    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>










<div class="post-scripts">
    
</div>



</body>
</html>
