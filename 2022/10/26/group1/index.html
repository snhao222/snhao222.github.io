<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Sun Hao">
    
    <title>
        
            group：DNN Latency Prediction |
        
        SnSpace
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/Slogo.svg">
    
<link rel="stylesheet" href="/fontawesome/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/regular.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/brands.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"snhao222.github.io","root":"/","language":"en","path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":false,"expand_all":false,"init_open":true},"style":{"primary_color":"#0066CC","logo":"/images/Slogo.svg","favicon":"/images/Slogo.svg","avatar":"/images/HeadImage.svg","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":false,"header_transparent":false,"background_img":"/images/bg.svg","description":"S  n  S  p  a  c  e","font_color":null,"hitokoto":{"enable":false}},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{},"code_block_tools":{"enable":true,"style":"default"},"side_tools":{},"pjax":{"enable":false},"lazyload":{"enable":false},"version":"3.4.8"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    KEEP.language_code_block = {"copy":"Copy code","copied":"Copied","fold":"Fold code block","folded":"Folded"};
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/Slogo.svg">
                </a>
            
            <a class="logo-title" href="/">
               SnSpace
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/categories">CATEGORIES</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">group：DNN Latency Prediction</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/HeadImage.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Sun Hao</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2022-10-26 19:47:12</span>
        <span class="mobile">2022-10-26 19:47</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/Paper-Notes/">Paper Notes</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <h2 id="BRP-NAS-Prediction-based-NAS-using-GCNs"><a href="#BRP-NAS-Prediction-based-NAS-using-GCNs" class="headerlink" title="BRP-NAS: Prediction-based NAS using GCNs"></a>BRP-NAS: Prediction-based NAS using GCNs</h2><blockquote>
<p> 来        源：34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada</p>
<p>原文速递：<a href="./BRP-NAS：Prediction-based NAS using GCNs.pdf" title="点击直接下载原文">BRP-NAS：Prediction-based NAS using GCNs</a></p>
</blockquote>
<p>与层级时延预测不同，该工作将模型的结构信息也作为时延预测的一个参量，直接预测完整模型的时延。对于图<script type="math/tex">g=(V,E)</script>，（<script type="math/tex">V</script>是<script type="math/tex">N</script>个具有<script type="math/tex">D</script>个特征的节点集，<script type="math/tex">E</script>是边集），将一个特征描述<script type="math/tex">X\in \mathbb{R}^{N\times D}</script>通过一个邻接矩阵<script type="math/tex">A\in \mathbb{R}^{N\times N}</script>进行编码作为图卷积网络（GCN）的输入。对于一个L层GCN，层间的传播规则为：</p>
<script type="math/tex; mode=display">
H^{l+1}=f(H^l,A)=\sigma(AH^lW^l)</script><p>其中，<script type="math/tex">H^l</script>和<script type="math/tex">W^l</script>分别是第<script type="math/tex">l</script>层的特征图和权重矩阵，<script type="math/tex">\sigma(\bullet)</script>是一个非线性激活函数。</p>
<p>该文使用一个4层GCN模型，每层有600个隐藏单元，引入一个全局节点（连接所有节点）汇总所有节点级信息进行图嵌入。</p>
<blockquote>
<p>1）考虑整个网络的特性，而非是对单层时延预测值的简单加和，准确度更高；</p>
<p>2）未考虑设备特性，是完全黑盒的预测，模型尺寸可能比较大；</p>
<p>3）泛化能力差，对新模型的支持性差。</p>
</blockquote>
<HR>

<h2 id="Learned-TPU-Cost-Model-for-XLA-Tensor-Programs"><a href="#Learned-TPU-Cost-Model-for-XLA-Tensor-Programs" class="headerlink" title="Learned TPU Cost Model for XLA Tensor Programs"></a>Learned TPU Cost Model for XLA Tensor Programs</h2><blockquote>
<p>来        源：33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada</p>
<p>原文速递：<a href="./Learned TPU Cost Model for XLA Tensor Programs.pdf" title="点击直接下载原文">Learned TPU Cost Model for XLA Tensor Programs</a></p>
</blockquote>
<p>该工作设计了一个基于学习的神经网络性能评估模型，用于预测每个kernel的执行时间<script type="math/tex">y'</script>，该模型如下图所示。</p>
<div align="center">
    <img src="/2022/10/26/group1/图片1.png" style="zoom: 80%;">
</div>

<center>图1 神经网络性能评估模型</center>

<p>第<script type="math/tex">i</script>个操作的操作码<script type="math/tex">x^o</script>通过嵌入查找表嵌入为一个向量，一个操作的特征<script type="math/tex">X^f</script>包含输出tensor形状、striding、padding、卷积核尺寸、tensor重叠等。然后使用一个前馈层<script type="math/tex">f_1</script>接一个GraphSAGE合并操作码和操作特征信息，得到节点嵌入<script type="math/tex">\varepsilon ^k</script>。之后，通过计算<script type="math/tex">\varepsilon</script>每一行的均值和最大值生成kernel嵌入，将均值和最大值向量的合并结果传入最终的前馈层生成kernel预测结果。</p>
<blockquote>
<p>1）该工作考虑了框架在进行神经网络推理时算子间的影响，采用图嵌入的方式进行时延预测，比单层时延预测加和的模型时延预测准确；</p>
<p>2）该工作是基于学习的神经网络预测方法，计算和训练开销可能较大。</p>
</blockquote>
<HR>

<h2 id="FBNet-Hardware-Aware-Efficient-ConvNet-Design-via-Differentiable-Neural-Architecture-Search"><a href="#FBNet-Hardware-Aware-Efficient-ConvNet-Design-via-Differentiable-Neural-Architecture-Search" class="headerlink" title="FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search"></a>FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</h2><blockquote>
<p>来        源：Computer Vision and Pattern Recognition 2019</p>
<p>原文速递：<a href="./FBNet：Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search.pdf" title="点击直接下载原文">FBNet：Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</a></p>
</blockquote>
<p>该工作致力于进行准确高效的神经网络搜索（NAS），注意到FLOP数并不总能准确地反映神经网络在设备上的推理时延。该工作测量了搜索空间内每个算子在设备上的运行时延，并使用一个标定表模型预测每个算子的时延，通过计算算子时延之和预测模型总时延。</p>
<div align="center">
    <img src="/2022/10/26/group1/图片2.png" style="zoom: 70%;">
</div>

<blockquote>
<p>1）该方法考虑到时延预测的效率，采用标定表代替预测模型，是一种轻量级的方案；</p>
<p>2）该方法在设备转换时可能存在需要重新标定的问题，工作量较大；</p>
<p>3）该方法未考虑框架在运行时的算子融合，存在精度低的问题。</p>
</blockquote>
<HR>

<h2 id="PALEO：A-Performance-Model-for-Deep-Neural-Networks"><a href="#PALEO：A-Performance-Model-for-Deep-Neural-Networks" class="headerlink" title="PALEO：A Performance Model for Deep Neural Networks"></a>PALEO：A Performance Model for Deep Neural Networks</h2><blockquote>
<p>来        源：The International Conference on Learning Representations (ICLR) 2017</p>
<p>原文速递：<a href="./paleo：a performance model for deep neural networks.pdf" title="点击直接下载原文">PALEO：A Performance Model for Deep Neural Networks</a></p>
</blockquote>
<p>该工作通过分析DNN在设备上进行计算的过程对神经网络的推理时延进行预测，其建模方法如下图所示，分为计算时延和通信时延两部分，并综合考虑了网络结构和硬件配置。</p>
<div align="center">
    <img src="/2022/10/26/group1/图片3.png" style="zoom: 100%;">
</div>

<center>图2 PALEO模型方法</center>

<p>该工作将每个神经网络节点<script type="math/tex">u</script>的计算时延建模为三部分：获取父节点输入的时间<script type="math/tex">R(Pa(u))</script>、在设备<script type="math/tex">d</script>上执行计算<script type="math/tex">f</script>的时间、将模型输出写入内存的时延<script type="math/tex">W(f,d)</script>。</p>
<script type="math/tex; mode=display">
T(u)=R(Pa(u))+C(f,d)+W(f,d)</script><p>计算时延基于FLOPs计数和设备运行速度计算：<script type="math/tex">C(f,d)=FLOPs(f)/speed(d)</script>，IO的时延通过计算内存占用/IO带宽获得。对于完整网络的时延预测，该工作通过计算所有层的时延和获得：<script type="math/tex">T(N)=\sum_{i=1}^nT(u^{(i)})</script>。</p>
<blockquote>
<p>1）该工作分析了在设备上进行推理需要的开销，能在一定程度上降低基于FLOPs时延预测的误差；</p>
<p>2）基于FLOPs的时延预测误差较大；</p>
<p>3）该工作对完整模型的时延预测方法采用多层时延的加和，未考虑算子融合机制，误差较大。</p>
</blockquote>
<HR>

<h2 id="Predicting-Execution-Time-of-Computer-Programs-Using-Sparse-Polynomial-Regression"><a href="#Predicting-Execution-Time-of-Computer-Programs-Using-Sparse-Polynomial-Regression" class="headerlink" title="Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression"></a>Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</h2><blockquote>
<p>来        源：NIPS’10: Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1December 2010 Pages 883–891</p>
<p>原文速递：<a href="./Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression.pdf" title="点击直接下载原文">Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
</blockquote>
<p>该工作侧重于对电脑应用程序运行时间的预测，与直接进行模型训练不同，其通过分析代码特性提取出多个程序的特征，并使用稀疏多项式回归模型预测程序运行时间。</p>
<p>稀疏多项式回归参考了LASSO的工作，从程序中可以分析出许多特征，如循环数、分支数、变量值等，而这些特征并不都对预测结果产生较大贡献，但大量的特征会使得多项式项数增多，降低预测效率。因此，可以将对结果影响小的特征的系数置零，只留下对结果影响大的特征子集。</p>
<blockquote>
<p>1）该方法使用多项式模型预测程序运行时延，准确性优于基于FLOPs的线性模型；</p>
<p>2）该方法通过代码分析的方式预测电脑程序的运行时间，但对于神经网络的泛化性较差，许多代码并不开源，无法获得代码中的详细信息。</p>
</blockquote>
<h2 id="TVM：An-Automated-End-to-End-Optimizing-Compiler-for-Deep-Learning"><a href="#TVM：An-Automated-End-to-End-Optimizing-Compiler-for-Deep-Learning" class="headerlink" title="TVM：An Automated End-to-End Optimizing Compiler for Deep Learning"></a>TVM：An Automated End-to-End Optimizing Compiler for Deep Learning</h2><blockquote>
<p>来        源：the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI ’18). October 8–10, 2018 • Carlsbad, CA, USA</p>
<p>原文速递：<a href="./TVM：An Automated End-to-End Optimizing Compiler for Deep Learning.pdf" title="点击直接下载原文">TVM：An Automated End-to-End Optimizing Compiler for Deep Learning</a></p>
</blockquote>
<p>该工作使用基于机器学习的代价模型，考虑到一个好的代价模型需要考虑影响性能的全部因素：内存访问模式、数据重用、管道依赖性等，现代硬件的复杂性高，并且针对每个新的目标硬件都需要新的代价模型，将是一个十分繁琐的过程。该方法在多轮预测的过程中测量模型实际运行时间，周期性地对模型进行更新，逐渐提高模型准确度。该方法在经过对比后使用梯度提升树模型（Gradient Tree Boosting Model），涉及到集中学习地范畴。</p>
<blockquote>
<p>1）该方法通过运行时对模型的逐渐优化提高模型在不同设备上的可迁移性；</p>
<p>2）该方法完全基于机器学习，训练和计算开销较大。</p>
</blockquote>
<HR>

<h2 id="Learning-to-Optimize-Halide-with-Tree-Search-and-Random-Programs"><a href="#Learning-to-Optimize-Halide-with-Tree-Search-and-Random-Programs" class="headerlink" title="Learning to Optimize Halide with Tree Search and Random Programs"></a>Learning to Optimize Halide with Tree Search and Random Programs</h2><blockquote>
<p>来        源：ACM Transactions on Graphics, Volume 38, Issue 4, August 2019, Article No.: 121, pp 1–12 <a class="link" target="_blank" rel="noopener" href="https://doi.org/10.1145/3306346.3322967">https://doi.org/10.1145/3306346.3322967<i class="fas fa-external-link-alt"></i></a></p>
<p>原文速递：<a href="./Learning to Optimize Halide with Tree Search and Random Programs.pdf" title="点击直接下载原文">Learning to Optimize Halide with Tree Search and Random Programs</a></p>
</blockquote>
<p>该工作采用通过神经网络预测程序运行时间的方法，同时考虑到完全用网络进行黑盒预测需要一个很大的网络，并且很难训练，该工作的代价模型使用人工设计的项向量和小神经网络预测的系数向量点乘获得程序的运行时间。这些人工设计的项是特征的非线性组合，作者希望通过这样的设计方式使这些项与运行时间成比例地扩展。神经网络只需要调整每一项的权重，对于对最终表现无关的项，可将其系数置为0。</p>
<blockquote>
<p>1）该方法考虑了预测器的预测效率，通过人工设计的多项式将预测网络变成小神经网络；</p>
<p>2）该方法需要考虑程序的代码特征，而对于非开源程序，该方法的泛化性较差。</p>
</blockquote>
<HR>

<h2 id="HELP：Hardware-Adaptive-Efficient-Latency-Prediction-for-NAS-via-Meta-Learning"><a href="#HELP：Hardware-Adaptive-Efficient-Latency-Prediction-for-NAS-via-Meta-Learning" class="headerlink" title="HELP：Hardware-Adaptive Efficient Latency Prediction for NAS via Meta-Learning"></a>HELP：Hardware-Adaptive Efficient Latency Prediction for NAS via Meta-Learning</h2><blockquote>
<p>来        源：35th Conference on Neural Information Processing Systems (NeurIPS 2021)</p>
<p>原文速递：<a href="./HELP：Hardware-Adaptive Efficient Latency Prediction for Nas via Meta-Learning.pdf" title="点击直接下载原文">HELP：Hardware-Adaptive Efficient Latency Prediction for Nas via Meta-Learning</a></p>
</blockquote>
<p>该工作注意到许多工作的时延预测只针对特定设备进行设计，应用在每个新设备时都需要对模型重新训练，而每次训练都需要大量的测量数据，影响了NAS工作的效率。因此，该工作提出了一种基于元学习的硬件设备自适应时延预测器，在预测器部署到新设备时，只需要采集少量数据就可以实现模型迁移。</p>
<blockquote>
<p>1）该工作考虑了预测模型在不同设备上的重新训练问题，使用元学习减少在不同设备上训练模型的代价；</p>
<p>2）该工作采用黑盒函数预测推理时延，要获得准确的预测结果需要复杂的函数，预测器效率不高。</p>
</blockquote>
<HR>

<h2 id="Reducing-energy-consumption-of-Neural-Architecture-Search-An-inference-latency-prediction-framework"><a href="#Reducing-energy-consumption-of-Neural-Architecture-Search-An-inference-latency-prediction-framework" class="headerlink" title="Reducing energy consumption of Neural Architecture Search: An inference latency prediction framework"></a>Reducing energy consumption of Neural Architecture Search: An inference latency prediction framework</h2><blockquote>
<p>来        源：Sustainable Cities and Society Volume 67, April 2021, 102747</p>
<p>原文速递：<a href="./Reducing energy consumption of Neural Architecture Search：An inference latency prediction framework.pdf" title="点击直接下载原文">Reducing energy consumption of Neural Architecture Search：An inference latency prediction framework</a></p>
</blockquote>
<p>该工作关注于更好地进行NAS，避免在实际设备上多次运行神经网络造成的碳排放和能源消耗，通过时延预测器直接获得神经网络在目标设备上的性能情况。具体地，该工作使用LSTM网络进行特征提取，使用LightGBM进行时延回归。</p>
<blockquote>
<p>1）该文参考了多种特征嵌入、预测模型，能够获得较高的预测精度；</p>
<p>2）该工作使用神经网络预测神经网络在设备上的时延，训练和预测的开销大，效率较低。</p>
</blockquote>
<HR>

<h2 id="Generalized-Latency-Performance-Estimation-for-Once-For-All-Neural-Architecture-Search"><a href="#Generalized-Latency-Performance-Estimation-for-Once-For-All-Neural-Architecture-Search" class="headerlink" title="Generalized Latency Performance Estimation for Once-For-All Neural Architecture Search"></a>Generalized Latency Performance Estimation for Once-For-All Neural Architecture Search</h2><blockquote>
<p>来        源：CoRR abs/2101.00732 (2021)</p>
<p>原文速递：<a href="./Generalized Latency Performance Estimation for Once-For-All Neural Architecture Search.pdf" title="点击直接下载原文">Generalized Latency Performance Estimation for Once-For-All Neural Architecture Search</a></p>
</blockquote>
<p>该工作设计的时延预测器使用网络图嵌入的方式对神经网络进行训练，通过神经网络预测模型运行的时延。同时，该工作考虑到预测器在不同硬件设备上的泛化能力，将GPU被广泛使用的参数—核心数、内存尺寸、内存带宽加入到嵌入向量中进行训练，以便使模型能够快速部署到不同的硬件平台上，然后通过微调的方式使模型适应不同的硬件平台。</p>
<blockquote>
<p>1）该工作采用图嵌入的方式进行时延预测，考虑了模型内部的内在联系，对模型块的时延预测更准确；</p>
<p>2）该工作完全使用神经网络预测模型运行时延，计算开销和模型尺寸都较大。</p>
</blockquote>
<HR>

<h2 id="NNLQP：A-Multi-Platform-Neural-Network-Latency-Query-and-Prediction-System-with-An-Evolving-Database"><a href="#NNLQP：A-Multi-Platform-Neural-Network-Latency-Query-and-Prediction-System-with-An-Evolving-Database" class="headerlink" title="NNLQP：A Multi-Platform Neural Network Latency Query and Prediction System with An Evolving Database"></a>NNLQP：A Multi-Platform Neural Network Latency Query and Prediction System with An Evolving Database</h2><blockquote>
<p>来        源：ICPP ’22, August 29-September 1, 2022, Bordeaux, France</p>
<p>原文速递：<a href="./NNLQP：A Multi-Platform Neural Network Latency Query and Prediction System with An Evolving Database.pdf" title="点击直接下载原文">NNLQP：A Multi-Platform Neural Network Latency Query and Prediction System with An Evolving Database</a></p>
</blockquote>
<p>该工作为了降低在多各平台上进行模型运行时延采样的难度，设计时延查询系统，自动测量模型在目标硬件上的时延并存储到MySQL数据库上进行存储。为了获得准确的时延预测器，该工作使用统一的图嵌入方式将模型图和节点特征嵌入到one-hot向量中，并使用带有FC、Relu和Dropout层的网络训练时延预测器。应用到不同硬件设备时使用迁移学习的方式进行微调，提高模型的准确度。</p>
<blockquote>
<p>1）该工作采用图嵌入的方式进行时延预测，考虑了模型内部的内在联系，对模型块的时延预测更准确；</p>
<p>2）该工作详细介绍了高效进行数据采样的方法，能获得大量用于模型训练的数据；</p>
<p>3）该工作完全使用神经网络预测模型运行时延，计算开销和模型尺寸都较大。</p>
</blockquote>
<HR>

<h2 id="Predicting-Latency-of-Neural-Network-Inference"><a href="#Predicting-Latency-of-Neural-Network-Inference" class="headerlink" title="Predicting Latency of Neural Network Inference"></a>Predicting Latency of Neural Network Inference</h2><blockquote>
<p>来        源：CS230: Deep Learning, Fall 2020, Stanford University, CA. </p>
<p>原文速递：<a href="./Predicting Latency of Neural Network Inference.pdf" title="点击直接下载原文">Predicting Latency of Neural Network Inference</a></p>
</blockquote>
<p>该工作使用图嵌入的方式预测神经网络在CPU和GPU上的推理时延，使用展平的网络属性列表和邻接矩阵作为预测模型的输入，预测模型使用深度神经网络。为了找到合适的深度神经网络，该工作将每个隐藏层的数量设定为100，从1到20搜索隐藏层数量这个超参数，使用在验证集上绝对误差最小的超参数。</p>
<p><div align="center">
    <img src="/2022/10/26/group1/图片4.png" style="zoom: 100%;">
</div></p>
<center>图3 基于图嵌入的时延预测器结构</center>


<blockquote>
<p>1）该工作将模型图结构嵌入作为预测模型的输入，相较于层级别的时延预测，准确度更高；</p>
<p>2）该方法对于不同硬件平台的泛化能力较差，更换平台后训练代价高；</p>
<p>3）该工作使用的预测模型是黑盒的，模型较大，预测效率低。</p>
</blockquote>
</HR></HR></HR></HR></HR></HR></HR></HR></HR></HR>
        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul class="copyright-info-content">
        
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

            </div>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/2022/10/29/Linux%E5%9F%BA%E7%A1%80/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Linux基础</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2022/10/26/nn-meter/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">nn-Meter：Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments"></i>&nbsp;Comments
    </div>
    

        
            
    <div class="valine-container">
        <script 
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script >
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'AIhtvUXrzUWmPIOmYNjrGExW-gzGzoHsz',
                    appKey: 'e4HGIfsWiHCnIXco4g7xtuAA',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: 'Add your comments',
                    lang: 'en'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Sun Hao';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('false') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            
<footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
                <span>2022</span> -
            
            2023
            
                &nbsp;<i class="fas fa-heart icon-animate"></i>
                &nbsp;<a href="/">Sun Hao</a>
            
        </div>
        
            <script async 
                    src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                
                
                    Totalview&nbsp;<span id="busuanzi_value_site_pv"></span>
                
            </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme
            &nbsp;
            <a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.8</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-up-right-and-down-left-from-center"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#BRP-NAS-Prediction-based-NAS-using-GCNs"><span class="nav-text">BRP-NAS: Prediction-based NAS using GCNs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learned-TPU-Cost-Model-for-XLA-Tensor-Programs"><span class="nav-text">Learned TPU Cost Model for XLA Tensor Programs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FBNet-Hardware-Aware-Efficient-ConvNet-Design-via-Differentiable-Neural-Architecture-Search"><span class="nav-text">FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PALEO%EF%BC%9AA-Performance-Model-for-Deep-Neural-Networks"><span class="nav-text">PALEO：A Performance Model for Deep Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Predicting-Execution-Time-of-Computer-Programs-Using-Sparse-Polynomial-Regression"><span class="nav-text">Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TVM%EF%BC%9AAn-Automated-End-to-End-Optimizing-Compiler-for-Deep-Learning"><span class="nav-text">TVM：An Automated End-to-End Optimizing Compiler for Deep Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learning-to-Optimize-Halide-with-Tree-Search-and-Random-Programs"><span class="nav-text">Learning to Optimize Halide with Tree Search and Random Programs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HELP%EF%BC%9AHardware-Adaptive-Efficient-Latency-Prediction-for-NAS-via-Meta-Learning"><span class="nav-text">HELP：Hardware-Adaptive Efficient Latency Prediction for NAS via Meta-Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reducing-energy-consumption-of-Neural-Architecture-Search-An-inference-latency-prediction-framework"><span class="nav-text">Reducing energy consumption of Neural Architecture Search: An inference latency prediction framework</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generalized-Latency-Performance-Estimation-for-Once-For-All-Neural-Architecture-Search"><span class="nav-text">Generalized Latency Performance Estimation for Once-For-All Neural Architecture Search</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NNLQP%EF%BC%9AA-Multi-Platform-Neural-Network-Latency-Query-and-Prediction-System-with-An-Evolving-Database"><span class="nav-text">NNLQP：A Multi-Platform Neural Network Latency Query and Prediction System with An Evolving Database</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Predicting-Latency-of-Neural-Network-Inference"><span class="nav-text">Predicting Latency of Neural Network Inference</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="close-popup-btn">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>





    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-block-tools.js"></script>





<div class="post-scripts">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>



</body>
</html>
