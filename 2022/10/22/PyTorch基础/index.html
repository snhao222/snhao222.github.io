<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Sun Hao">
    
    <title>
        
            PyTorch基础 |
        
        SnSpace
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/Slogo.svg">
    
<link rel="stylesheet" href="/fontawesome/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/regular.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/brands.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"snhao222.github.io","root":"/","language":"en","path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":false,"expand_all":false,"init_open":true},"style":{"primary_color":"#0066CC","logo":"/images/Slogo.svg","favicon":"/images/Slogo.svg","avatar":"/images/HeadImage.svg","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":false,"header_transparent":false,"background_img":"/images/bg.svg","description":"S  n  S  p  a  c  e","font_color":null,"hitokoto":{"enable":false}},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{},"code_block_tools":{"enable":true,"style":"default"},"side_tools":{},"pjax":{"enable":false},"lazyload":{"enable":false},"version":"3.4.8"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    KEEP.language_code_block = {"copy":"Copy code","copied":"Copied","fold":"Fold code block","folded":"Folded"};
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/Slogo.svg">
                </a>
            
            <a class="logo-title" href="/">
               SnSpace
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/categories">CATEGORIES</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">PyTorch基础</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/HeadImage.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Sun Hao</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2022-10-22 10:51:28</span>
        <span class="mobile">2022-10-22 10:51</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/Learning-Materials/">Learning Materials</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>PyTorch是Facebook团队于2017年1月发布的一个深度学习框架，虽然晚于TensorFlow、Keras等框架，但自发布之日起，其关注度就在不断上升，目前在GitHub上的热度已超过Theano、Caffe、MXNet等框架。</p>
<p>PyTorch是一个建立在Torch库之上的Python包，旨在加速深度学习应用。它提供一种类似Numpy的抽象方法来表征张量（或多维数组），可以利用GPU来加速训练。由于PyTorch采用了动态计算图（Dynamic Computational Graph）结构，且基于tape的Autograd系统的深度神经网络。其他很多框架，比如TensorFlow（TensorFlow2.0也加入了动态网络的支持）、Caffe、CNTK、Theano等，采用静态计算图。使用PyTorch，通过一种称为Reverse-mode auto-differentiation（反向模式自动微分）的技术，可以零延迟或零成本地任意改变你的网络的行为。<br>Torch是PyTorch中的一个重要包，它包含了多维张量的数据结构以及基于其上的多种数学操作。<br>自2015年谷歌开源TensorFlow以来，深度学习框架之争越来越激烈，全球多个看重AI研究与应用的科技巨头均在加大这方面的投入。PyTorch从2017年年初发布以来，可谓是异军突起，短时间内取得了一系列成果，成为明星框架。</p>
<p>PyTorch由4个主要的包组成：</p>
<p><script type="math/tex">\cdot</script>torch：类似于Numpy的通用数组库，可将张量类型转换为torch.cuda.TensorFloat，并在GPU上进行计算；</p>
<p><script type="math/tex">\cdot</script>torch.autograd：用于构建计算图形并自动获取梯度的包；</p>
<p><script type="math/tex">\cdot</script>torch.nn：具有共享层和损失函数的神经网络库；</p>
<p><script type="math/tex">\cdot</script>torch.optim：具有通用优化算法（如SGD、Adam等）的优化包。</p>
<h2 id="2-Numpy与Tensor"><a href="#2-Numpy与Tensor" class="headerlink" title="2. Numpy与Tensor"></a>2. Numpy与Tensor</h2><p>上一内容介绍了Numpy，了解到其存取数据非常方便，而且还拥有大量的函数，所以深得数据处理、机器学习者喜爱。这节内容将介绍PyTorch的Tensor，它可以是零维（又称为标量或一个数）、一维、二维及多维的数组。Tensor自称为神经网络界的Numpy，它与Numpy相似，二者可以共享内存，且之间的转换非常方便和高效。不过它们也有不同之处，最大的区别就是Numpy会把ndarray放在CPU中进行加速运算，而由Torch产生的Tensor会放在GPU中进行加速运算（假设当前环境有GPU）。</p>
<h3 id="2-1-创建Tensor"><a href="#2-1-创建Tensor" class="headerlink" title="2.1 创建Tensor"></a>2.1 创建Tensor</h3><p>创建Tensor的方法有很多，可以从列表或ndarray等类型进行构建，也可根据指定的形状构建。常见的创建Tensor的方法可参考表1。</p>
<center><b>表1 常见的创建Tensor的方法</b></center>

<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片1.png" style="zoom: 100%;">
</div>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#根据list数据生成Tensor</span></span><br><span class="line">torch.Tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="comment">#根据指定形状生成Tensor</span></span><br><span class="line">torch.Tensor(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment">#根据给定的数组生成Tensor</span></span><br><span class="line">t=torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="comment">#查看Tensor的形状</span></span><br><span class="line">t.size()</span><br><span class="line"><span class="comment">#shape与size()等价</span></span><br><span class="line">t.shape</span><br><span class="line"><span class="comment">#根据已有形状创建Tensor</span></span><br><span class="line">torch.Tensor(t.size())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成一个单位矩阵</span></span><br><span class="line">torch.eye(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#自动生成全是0的矩阵</span></span><br><span class="line">torch.zeros(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment">#根据规则生成数据</span></span><br><span class="line">torch.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">#生成满足均匀分布随机数</span></span><br><span class="line">torch.rand(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment">#生成满足标准分布随机数</span></span><br><span class="line">torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment">#返回所给数据形状相同，值全为0的张量</span></span><br><span class="line">torch.zeros_like(torch.rand(<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<h3 id="2-2-修改Tensor形状"><a href="#2-2-修改Tensor形状" class="headerlink" title="2.2 修改Tensor形状"></a>2.2 修改Tensor形状</h3><p>在处理数据、构建网络层等过程中，经常需要了解Tensor的形状、修改Tensor的形状。与修改Numpy的形状类似，修改Tenor的形状也有很多类似函数，具体可参考表2。</p>
<center><b>表2 常用修改Tensor形状的函数</b></center>

<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片2.png" style="zoom: 100%;">
</div>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成一个形状为2x3的矩阵</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#查看矩阵的形状</span></span><br><span class="line">x.size()   <span class="comment">#结果为torch.Size([2, 3])</span></span><br><span class="line"><span class="comment">#查看x的维度</span></span><br><span class="line">x.dim()    <span class="comment">#结果为2</span></span><br><span class="line"><span class="comment">#把x变为3x2的矩阵</span></span><br><span class="line">x.view(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#把x展平为1维向量</span></span><br><span class="line">y=x.view(-<span class="number">1</span>)  </span><br><span class="line">y.shape</span><br><span class="line"><span class="comment">#添加一个维度</span></span><br><span class="line">z=torch.unsqueeze(y,<span class="number">0</span>)</span><br><span class="line"><span class="comment">#查看z的形状</span></span><br><span class="line">z.size()   <span class="comment">#结果为torch.Size([1, 6])</span></span><br><span class="line"><span class="comment">#计算Z的元素个数</span></span><br><span class="line">z.numel()   <span class="comment">#结果为6</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><b>torch.view与torch.reshpae的异同</b><br>1）reshape()可以由torch.reshape()，也可由torch.Tensor.reshape()调用。但view()只可由torch.Tensor.view()来调用；<br>2）对于一个将要被view的Tensor，新的size必须与原来的size与stride兼容。否则，在view之前必须调用contiguous()方法；<br>3）同样也是返回与input数据量相同，但形状不同的Tensor。若满足view的条件，则不会copy，若不满足，则会copy；<br>4）如果你只想重塑张量，请使用torch.reshape。如果你还关注内存使用情况并希望确保两个张量共享相同的数据，请使用torch.view。</p>
</blockquote>
<h3 id="2-3-索引操作"><a href="#2-3-索引操作" class="headerlink" title="2.3 索引操作"></a>2.3 索引操作</h3><p>Tensor的索引操作与Numpy类似，一般情况下索引结果与源数据共享内存。从Tensor获取元素除了可以通过索引，也可以借助一些函数，常用的选择函数可参考表3。</p>
<center><b>表3 常用索引操作函数</b></center>

<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片3.png" style="zoom: 100%;">
</div>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">设置一个随机种子</span><br><span class="line">torch.manual_seed(<span class="number">100</span>) </span><br><span class="line"><span class="comment">#生成一个形状为2x3的矩阵</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#根据索引获取第1行，所有数据</span></span><br><span class="line">x[<span class="number">0</span>,:]</span><br><span class="line"><span class="comment">#获取最后一列数据</span></span><br><span class="line">x[:,-<span class="number">1</span>]</span><br><span class="line"><span class="comment">#生成是否大于0的Byter张量</span></span><br><span class="line">mask=x&gt;<span class="number">0</span></span><br><span class="line"><span class="comment">#获取大于0的值</span></span><br><span class="line">torch.masked_select(x,mask)</span><br><span class="line"><span class="comment">#获取非0下标,即行，列索引</span></span><br><span class="line">torch.nonzero(mask)</span><br><span class="line"><span class="comment">#获取指定索引对应的值,输出根据以下规则得到</span></span><br><span class="line"><span class="comment">#out[i][j] = input[index[i][j]][j]  # if dim == 0</span></span><br><span class="line"><span class="comment">#out[i][j] = input[i][index[i][j]]  # if dim == 1</span></span><br><span class="line">index=torch.LongTensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">torch.gather(x,<span class="number">0</span>,index)</span><br><span class="line">index=torch.LongTensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">a=torch.gather(x,<span class="number">1</span>,index)</span><br><span class="line"><span class="comment">#把a的值返回到一个2x3的0矩阵中</span></span><br><span class="line">z=torch.zeros(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">z.scatter_(<span class="number">1</span>,index,a)</span><br></pre></td></tr></table></figure>
<h3 id="2-4-广播机制"><a href="#2-4-广播机制" class="headerlink" title="2.4 广播机制"></a>2.4 广播机制</h3><p>广播机制是向量运算的重要技巧。除了Numpy支持广播机制外，PyTorch也支持广播机制。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.arange(<span class="number">0</span>, <span class="number">40</span>, <span class="number">10</span>).reshape(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">B = np.arange(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#把ndarray转换为Tensor</span></span><br><span class="line">A1=torch.from_numpy(A)  <span class="comment">#形状为4x1</span></span><br><span class="line">B1=torch.from_numpy(B)  <span class="comment">#形状为3</span></span><br><span class="line"><span class="comment">#Tensor自动实现广播</span></span><br><span class="line">C=A1+B1</span><br><span class="line"><span class="comment">#我们可以根据广播机制，手工进行配置</span></span><br><span class="line"><span class="comment">#根据规则1，B1需要向A1看齐，把B变为（1, 3）</span></span><br><span class="line">B2=B1.unsqueeze(<span class="number">0</span>)  <span class="comment">#B2的形状为1x3</span></span><br><span class="line"><span class="comment">#使用expand函数重复数组，分别的4x3的矩阵</span></span><br><span class="line">A2=A1.expand(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">B3=B2.expand(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#然后进行相加,C1与C结果一致</span></span><br><span class="line">C1=A2+B3</span><br></pre></td></tr></table></figure>
<h3 id="2-5-逐元素操作"><a href="#2-5-逐元素操作" class="headerlink" title="2.5 逐元素操作"></a>2.5 逐元素操作</h3><p>与Numpy一样，Tensor也有逐元素操作（Element-Wise），且操作内容相似，但使用函数可能不尽相同。大部分数学运算都属于逐元素操作，其输入与输出的形状相同。常见的逐元素操作可参考表4。</p>
<center><b>表4 常见逐元素操作</b></center>

<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片4.png" style="zoom: 100%;">
</div>

<blockquote>
<p>这些操作均会创建新的Tensor，如果需要就地操作，可以使用这些方法的下划线版本，例如abs_。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">t = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">t1 = torch.randn(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">t2 = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#t+0.1*(t1/t2)</span></span><br><span class="line">torch.addcdiv(t, <span class="number">0.1</span>, t1, t2)</span><br><span class="line"><span class="comment">#计算sigmoid</span></span><br><span class="line">torch.sigmoid(t)</span><br><span class="line"><span class="comment">#将t限制在[0,1]之间</span></span><br><span class="line">torch.clamp(t,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">#t+2进行就地运算</span></span><br><span class="line">t.add_(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-6-归并操作"><a href="#2-6-归并操作" class="headerlink" title="2.6 归并操作"></a>2.6 归并操作</h3><p>归并操作顾名思义，就是对输入进行归并或合计等操作，这类操作的输入输出形状一般并不相同，而且往往是输入大于输出形状。归并操作可以对整个Tensor，也可以沿着某个维度进行归并。常见的归并操作可参考表5。</p>
<center><b>表5 常见归并操作</b></center>

<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片5.png" style="zoom: 100%;">
</div>

<blockquote>
<p>归并操作一般涉及一个dim参数，指定沿哪个维进行归并。另一个参数是keepdim，说明输出结果中是否保留维度1，缺省情况是False，即不保留。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成一个含6个数的向量</span></span><br><span class="line">a=torch.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">6</span>)</span><br><span class="line"><span class="comment">#使用view方法，把a变为2x3矩阵</span></span><br><span class="line">a=a.view((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment">#沿y轴方向累加，即dim=0</span></span><br><span class="line">b=a.<span class="built_in">sum</span>(dim=<span class="number">0</span>)   <span class="comment">#b的形状为[3]</span></span><br><span class="line"><span class="comment">#沿y轴方向累加，即dim=0,并保留含1的维度</span></span><br><span class="line">b=a.<span class="built_in">sum</span>(dim=<span class="number">0</span>,keepdim=<span class="literal">True</span>) <span class="comment">#b的形状为[1,3]</span></span><br></pre></td></tr></table></figure>
<h3 id="2-7-比较操作"><a href="#2-7-比较操作" class="headerlink" title="2.7 比较操作"></a>2.7 比较操作</h3><p>比较操作一般是进行逐元素比较，有些是按指定方向比较。常用的比较函数可参考表6。</p>
<center><b>表6 常用的比较函数</b></center>

<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片6.jpg" style="zoom: 100%;">
</div>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x=torch.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">6</span>).view(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment">#求所有元素的最大值</span></span><br><span class="line">torch.<span class="built_in">max</span>(x)   <span class="comment">#结果为10</span></span><br><span class="line"><span class="comment">#求y轴方向的最大值</span></span><br><span class="line">torch.<span class="built_in">max</span>(x,dim=<span class="number">0</span>)  <span class="comment">#结果为[6,8,10]</span></span><br><span class="line"><span class="comment">#求最大的2个元素</span></span><br><span class="line">torch.topk(x,<span class="number">1</span>,dim=<span class="number">0</span>)  <span class="comment">#结果为[6,8,10],对应索引为tensor([[1, 1, 1])</span></span><br></pre></td></tr></table></figure>
<h3 id="2-8-矩阵操作"><a href="#2-8-矩阵操作" class="headerlink" title="2.8 矩阵操作"></a>2.8 矩阵操作</h3><p>机器学习和深度学习中存在大量的矩阵运算，常用的算法有两种：一种是逐元素乘法，另外一种是点积乘法。PyTorch中常用的矩阵函数可参考表7。</p>
<center><b>表7 常用矩阵函数</b></center>

<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片7.jpg" style="zoom: 100%;">
</div>

<blockquote>
<p>1）Torch的dot与Numpy的dot有点不同，Torch中的dot是对两个为1D张量进行点积运算，Numpy中的dot无此限制；<br>2）mm是对2D的矩阵进行点积，bmm对含batch的3D矩阵进行点积运算；<br>3）转置运算会导致存储空间不连续，需要调用contiguous方法转为连续。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a=torch.tensor([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b=torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">torch.dot(a,b)  <span class="comment">#运行结果为18</span></span><br><span class="line">x=torch.randint(<span class="number">10</span>,(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">y=torch.randint(<span class="number">6</span>,(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">torch.mm(x,y)</span><br><span class="line">x=torch.randint(<span class="number">10</span>,(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">y=torch.randint(<span class="number">6</span>,(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">torch.bmm(x,y)</span><br></pre></td></tr></table></figure>
<h3 id="2-9-PyTorch与Numpy比较"><a href="#2-9-PyTorch与Numpy比较" class="headerlink" title="2.9 PyTorch与Numpy比较"></a>2.9 PyTorch与Numpy比较</h3><p>PyTorch与Numpy有很多类似的地方，并且有很多相同的操作函数名称，或虽然函数名称不同但含义相同；当然也有一些虽然函数名称相同，但含义不尽相同。有些很容易混淆，Numpy与Pytorch函数的一些主要区别可参考表8。</p>
<center><b>表8 Pytorch与Numpy函数对照表</b></center>

<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片8.png" style="zoom: 100%;">
</div>

<h2 id="3-使用Tensor及Autograd实现机器学习"><a href="#3-使用Tensor及Autograd实现机器学习" class="headerlink" title="3. 使用Tensor及Autograd实现机器学习"></a>3. 使用Tensor及Autograd实现机器学习</h2><p>在神经网络中，一个重要内容就是进行参数学习，而参数学习离不开求导，那么PyTorch是如何进行求导的呢？<br>现在大部分深度学习架构都有自动求导的功能，PyTorch也不例外，torch.autograd包就是用来自动求导的。Autograd包为张量上所有的操作提供了自动求导功能，而torch.Tensor和torch.Function为Autograd的两个核心类，它们相互连接并生成一个有向无环图（DAG）。</p>
<p>本节将使用PyTorch的autograd自动求导包及对应的Tensor，利用自动反向传播来求梯度，无须手工计算梯度。</p>
<p>1）导入需要的库。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<p>2）生成训练数据，并可视化数据分布情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">t.manual_seed(<span class="number">100</span>) </span><br><span class="line">dtype = t.<span class="built_in">float</span></span><br><span class="line"><span class="comment">#生成x坐标数据，x为tenor，需要把x的形状转换为100x1</span></span><br><span class="line">x = t.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>) </span><br><span class="line"><span class="comment">#生成y坐标数据，y为tenor，形状为100x1，另加上一些噪声</span></span><br><span class="line">y = <span class="number">3</span>*x.<span class="built_in">pow</span>(<span class="number">2</span>) +<span class="number">2</span>+ <span class="number">0.2</span>*torch.rand(x.size())   </span><br><span class="line"><span class="comment"># 画图，把tensor数据转换为numpy数据</span></span><br><span class="line">plt.scatter(x.numpy(), y.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片9.png" style="zoom: 70%;">
</div>

<p>3）初始化权重参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机初始化参数，参数w、b为需要学习的，故需requires_grad=True</span></span><br><span class="line">w = t.randn(<span class="number">1</span>,<span class="number">1</span>, dtype=dtype,requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = t.zeros(<span class="number">1</span>,<span class="number">1</span>, dtype=dtype, requires_grad=<span class="literal">True</span>) </span><br></pre></td></tr></table></figure>
<p>4）训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">lr =<span class="number">0.001</span> <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">800</span>):</span><br><span class="line">    <span class="comment"># 前向传播，并定义损失函数loss</span></span><br><span class="line">    y_pred = x.<span class="built_in">pow</span>(<span class="number">2</span>).mm(w) + b</span><br><span class="line">    loss = <span class="number">0.5</span> * (y_pred - y) ** <span class="number">2</span></span><br><span class="line">    loss = loss.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自动计算梯度，梯度存放在grad属性中</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 手动更新参数，需要用torch.no_grad()，使上下文环境中切断自动求导的计算</span></span><br><span class="line">    <span class="keyword">with</span> t.no_grad():</span><br><span class="line">        w -= lr * w.grad</span><br><span class="line">        b -= lr * b.grad</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度清零</span></span><br><span class="line">        w.grad.zero_()</span><br><span class="line">        b.grad.zero_()</span><br></pre></td></tr></table></figure>
<p>5）可视化训练结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x.numpy(), y_pred.detach().numpy(),<span class="string">&#x27;r-&#x27;</span>,label=<span class="string">&#x27;predict&#x27;</span>)<span class="comment">#predict</span></span><br><span class="line">plt.scatter(x.numpy(), y.numpy(),color=<span class="string">&#x27;blue&#x27;</span>,marker=<span class="string">&#x27;o&#x27;</span>,label=<span class="string">&#x27;true&#x27;</span>) <span class="comment"># true data</span></span><br><span class="line">plt.xlim(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">plt.ylim(<span class="number">2</span>,<span class="number">6</span>)  </span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(w, b)</span><br></pre></td></tr></table></figure>
<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片10.png" style="zoom: 70%;">
</div>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">运行结果：</span><br><span class="line">tensor([[<span class="number">2.9648</span>]], requires_grad=<span class="literal">True</span>) tensor([[<span class="number">2.1145</span>]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="4-PyTorch神经网络工具箱"><a href="#4-PyTorch神经网络工具箱" class="headerlink" title="4. PyTorch神经网络工具箱"></a>4. PyTorch神经网络工具箱</h2><h3 id="4-1-神经网络核心组件"><a href="#4-1-神经网络核心组件" class="headerlink" title="4.1 神经网络核心组件"></a>4.1 神经网络核心组件</h3><p>神经网络看起来很复杂，节点很多，层数多，参数更多。但核心部分或组件不多，把这些组件确定后，这个神经网络基本就确定了。这些核心组件包括：<br>1）层：神经网络的基本结构，将输入张量转换为输出张量。<br>2）模型：层构成的网络。<br>3）损失函数：参数学习的目标函数，通过最小化损失函数来学习各种参数。<br>4）优化器：如何使损失函数最小，这就涉及优化器。<br>当然这些核心组件不是独立的，它们之间，以及它们与神经网络其他组件之间有密切关系。为便于读者理解，我们可以把这些关键组件及相互关系，用图1表示。</p>
<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片11.jpg" style="zoom: 45%;">
</div>

<center>图1 神经网络关键组件及相互关系示意图</center>

<p>多个层链接在一起构成一个模型或网络，输入数据通过这个模型转换为预测值，然后损失函数把预测值与真实值进行比较，得到损失值（损失值可以是距离、概率值等），该损失值用于衡量预测值与目标结果的匹配或相似程度，优化器利用损失值更新权重参数，从而使损失值越来越小。这是一个循环过程，当损失值达到一个阀值或循环次数到达指定次数，循环结束。</p>
<h3 id="4-2-神经网络工具箱nn"><a href="#4-2-神经网络工具箱nn" class="headerlink" title="4.2 神经网络工具箱nn"></a>4.2 神经网络工具箱nn</h3><p>前面我们使用Autograd及Tensor实现机器学习实例时，需要做不少设置，如对叶子节点的参数requires_grad设置为True，然后调用backward，再从grad属性中提取梯度。对于大规模的网络，Autograd太过于底层和烦琐。为了简单、有效解决这个问题，nn是一个有效工具。在nn工具箱中有两个重要模块：nn.Model、nn.functional。</p>
<p>nn中的大多数层（Layer）在functional中都有与之对应的函数。nn.functional中函数与nn.Module中的Layer的主要区别是后者继承Module类，会自动提取可学习的参数。而nn.functional更像是纯函数。两者功能相同，且性能也没有很大区别，那么如何选择呢？像卷积层、全连接层、Dropout层等因含有可学习参数，一般使用nn.Module，而激活函数、池化层不含可学习参数，可以使用nn.functional中对应的函数。</p>
<h4 id="4-2-1-nn-Module"><a href="#4-2-1-nn-Module" class="headerlink" title="4.2.1 nn.Module"></a>4.2.1 nn.Module</h4><p>nn.Module是nn的一个核心数据结构，它可以是神经网络的某个层（Layer），也可以是包含多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，生成自己的网络/层。nn中已实现了绝大多数层，包括全连接层、损失层、激活层、卷积层、循环层等，这些层都是nn.Module的子类，能够自动检测到自己的Parameter，并将其作为学习参数，且针对GPU运行进行了cuDNN优化。</p>
<h4 id="4-2-2-nn-functional"><a href="#4-2-2-nn-functional" class="headerlink" title="4.2.2 nn.functional"></a>4.2.2 nn.functional</h4><p>nn中的层，一类是继承了nn.Module，其命名一般为nn.Xxx（第一个是大写），如nn.Linear、nn.Conv2d、nn.CrossEntropyLoss等。另一类是nn.functional中的函数，其名称一般为nn.funtional.xxx，如nn.funtional.linear、nn.funtional.conv2d、nn.funtional.cross_entropy等。从功能来说两者相当，基于nn.Moudle能实现的层，使用nn.funtional也可实现，反之亦然，而且性能方面两者也没有太大差异。不过在具体使用时，两者还是有区别，主要区别如下：</p>
<blockquote>
<p>1）nn.Xxx继承于nn.Module，nn.Xxx需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。它能够很好地与nn.Sequential结合使用，而nn.functional.xxx无法与nn.Sequential结合使用。<br>2）nn.Xxx不需要自己定义和管理weight、bias参数；而nn.functional.xxx需要自己定义weight、bias参数，每次调用的时候都需要手动传入weight、bias等参数，不利于代码复用。<br>3）Dropout操作在训练和测试阶段是有区别的，使用nn.Xxx方式定义Dropout，在调用model.eval()之后，自动实现状态的转换，而使用nn.functional.xxx却无此功能。</p>
</blockquote>
<p>总的来说，两种功能都是相同的，但PyTorch官方推荐：具有学习参数的（例如，conv2d,linear,batch_norm)采用nn.Xxx方式。没有学习参数的（例如，maxpool、loss func、activation func）等根据个人选择使用nn.functional.xxx或者nn.Xxx方式。</p>
<h3 id="4-3-优化器"><a href="#4-3-优化器" class="headerlink" title="4.3 优化器"></a>4.3 优化器</h3><p>PyTorch常用的优化方法都封装在torch.optim里面，其设计很灵活，可以扩展为自定义的优化方法。所有的优化方法都是继承了基类optim.Optimizer，并实现了自己的优化步骤。最常用的优化算法就是梯度下降法及其各种变种，这类优化算法通过使用参数的梯度值更新参数。</p>
<p>使用优化器的一般步骤为：</p>
<p>（1）建立优化器实例<br>导入optim模块，实例化SGD优化器，这里使用动量参数momentum（该值一般在（0,1）之间），是SGD的改进版，效果一般比不使用动量规则的要好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)</span><br></pre></td></tr></table></figure>
<blockquote>
<p> 以下步骤在训练模型的for循环中。</p>
</blockquote>
<p>（2）向前传播<br>把输入数据传入神经网络Net实例化对象model中，自动执行forward函数，得到out输出值，然后用out与标记label计算损失值loss。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out = model(img)</span><br><span class="line">loss = criterion(out, label)</span><br></pre></td></tr></table></figure>
<p>（3）清空梯度<br>缺省情况梯度是累加的，在梯度反向传播前，先需把梯度清零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>（4）反向传播<br>基于损失值，把梯度进行反向传播。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>
<p>（5）更新参数<br>基于当前梯度（存储在参数的.grad属性中）更新参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p><b>动态修改学习率参数</b></p>
<p>修改参数的方式可以通过修改参数optimizer.params_groups或新建optimizer。新建optimizer比较简单，optimizer十分轻量级，所以开销很小。但是新的优化器会初始化动量等状态信息，这对于使用动量的优化器（momentum参数的SGD）可能会造成收敛中的震荡。所以，这里直接采用修改参数optimizer.params_groups。<br>optimizer.param_groups：长度1的list，optimizer.param_groups[0]：长度为6的字典，包括权重参数、lr、momentum等参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(optimizer.param_groups[<span class="number">0</span>])</span><br><span class="line">运行结果：</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure>
<p>动态修改学习率参数示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoches):</span><br><span class="line"><span class="comment">#动态修改参数学习率</span></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">5</span>==<span class="number">0</span>:</span><br><span class="line">        optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]*=<span class="number">0.1</span></span><br><span class="line">        <span class="built_in">print</span>(optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> img, label <span class="keyword">in</span> train_loader:</span><br><span class="line"><span class="comment">######</span></span><br></pre></td></tr></table></figure>
<h3 id="4-4-神经网络构建实例"><a href="#4-4-神经网络构建实例" class="headerlink" title="4.4 神经网络构建实例"></a>4.4 神经网络构建实例</h3><p>使用PyTorch构建神经网络使用的主要工具（或类）及相互关系如图2所示。如4.2节所述，构建网络层可基于Module类或函数（nn.functional）。</p>
<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片12.png" style="zoom: 45%;">
</div>

<center>图2 PyTorch实现神经网络的主要工具及相互关系</center>

<p>下面将利用神经网络完成对手写数字进行识别的实例，来说明如何借助nn工具箱来实现一个神经网络，并对神经网络有个直观了解。主要步骤有：</p>
<blockquote>
<p>1）利用PyTorch内置函数mnist下载数据集；<br>2）利用torchvision对数据进行预处理，调用torch.utils建立一个数据迭代器；<br>3）可视化源数据；<br>4）利用nn工具箱构建神经网络模型；<br>5）实例化模型，并定义损失函数及优化器；<br>6）训练模型；<br>7）可视化结果。</p>
</blockquote>
<p>神经网络的结构如图3所示，使用两个隐含层，每层激活函数为ReLU，最后使用torch.max(out,1)找出张量out最大值对应索引作为预测值。</p>
<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片13.png" style="zoom: 45%;">
</div>

<center>图3 实例使用的神经网络结构图</center>

<h4 id="4-4-1-数据准备"><a href="#4-4-1-数据准备" class="headerlink" title="4.4.1 数据准备"></a>4.4.1 数据准备</h4><p>（1）导入模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 导入 PyTorch 内置的 mnist 数据</span></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> mnist </span><br><span class="line"><span class="comment">#导入预处理模块</span></span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment">#导入nn及优化器</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">#导入画图库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<p>（2）定义超参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_batch_size = <span class="number">64</span></span><br><span class="line">test_batch_size = <span class="number">128</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">num_epoches = <span class="number">20</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">momentum = <span class="number">0.5</span></span><br></pre></td></tr></table></figure>
<p>（3）下载数据并对数据进行预处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义预处理函数，这些预处理依次放在Compose函数中。</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>])])</span><br><span class="line"><span class="comment">#下载数据，并对数据进行预处理</span></span><br><span class="line">train_dataset = mnist.MNIST(<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = mnist.MNIST(<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform)</span><br><span class="line"><span class="comment">#dataloader是一个可迭代对象，可以想迭代器一样使用。</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>1）transforms.Compose可以把一些转换函数组合在一起；<br>2）Normalize([0.5],[0.5])对张量进行归一化，这里两个0.5分别表示对张量进行归一化的全局平均值和方差。因图像是灰色的只有一个通道，如果有多个通道，需要有多个数字，如3个通道，应该是Normalize([m1,m2,m3],[n1,n2,n3])；<br>3）download参数控制是否需要下载，如果./data目录下已有MNIST，可选择False；<br>4）用DataLoader得到生成器，这可节省内存；<br>5）shuffle=True表示对数据进行打乱操作。</p>
</blockquote>
<h4 id="4-4-2-可视化源数据"><a href="#4-4-2-可视化源数据" class="headerlink" title="4.4.2 可视化源数据"></a>4.4.2 可视化源数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">examples = <span class="built_in">enumerate</span>(test_loader)</span><br><span class="line">batch_idx, (example_data, example_targets) = <span class="built_in">next</span>(examples)</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">  plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">  plt.tight_layout()</span><br><span class="line">  plt.imshow(example_data[i][<span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>, interpolation=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&quot;Ground Truth: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(example_targets[i]))</span><br><span class="line">  plt.xticks([])</span><br><span class="line">  plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片15.png" style="zoom: 100%;">
</div>

<h4 id="4-4-3-模型构建"><a href="#4-4-3-模型构建" class="headerlink" title="4.4.3 模型构建"></a>4.4.3 模型构建</h4><p>（1）构建网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用sequential构建网络，Sequential()函数的功能是将网络的层组合到一起</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, n_hidden_1, n_hidden_2, out_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.layer1 = nn.Sequential(nn.Linear(in_dim, n_hidden_1),nn.BatchNorm1d(n_hidden_1))</span><br><span class="line">        self.layer2 = nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2),nn.BatchNorm1d (n_hidden_2))</span><br><span class="line">        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2, out_dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.layer1(x))</span><br><span class="line">        x = F.relu(self.layer2(x))</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>（2）实例化网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#检测是否有可用的GPU，有则使用，否则使用CPU</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment">#实例化网络</span></span><br><span class="line">model = Net(<span class="number">28</span> * <span class="number">28</span>, <span class="number">300</span>, <span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)</span><br></pre></td></tr></table></figure>
<h4 id="4-4-4-模型训练"><a href="#4-4-4-模型训练" class="headerlink" title="4.4.4 模型训练"></a>4.4.4 模型训练</h4><p>这里使用for循环进行迭代，其中包括对训练数据的训练模型，然后用测试数据的验证模型。</p>
<p>（1）模型训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">losses = []</span><br><span class="line">acces = []</span><br><span class="line">eval_losses = []</span><br><span class="line">eval_acces = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoches):</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    train_acc = <span class="number">0</span></span><br><span class="line">    <span class="comment">#将module设置为训练模式</span></span><br><span class="line">    model.train()</span><br><span class="line">	<span class="comment">#动态修改参数学习率</span></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">5</span>==<span class="number">0</span>:</span><br><span class="line">        optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]*=<span class="number">0.1</span></span><br><span class="line">    <span class="keyword">for</span> img, label <span class="keyword">in</span> train_loader:</span><br><span class="line">        img=img.to(device)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        img = img.view(img.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        out = model(img)</span><br><span class="line">        loss = criterion(out, label)</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># 记录误差</span></span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        <span class="comment"># 计算分类的准确率</span></span><br><span class="line">        _, pred = out.<span class="built_in">max</span>(<span class="number">1</span>)</span><br><span class="line">        num_correct = (pred == label).<span class="built_in">sum</span>().item()</span><br><span class="line">        acc = num_correct / img.shape[<span class="number">0</span>]</span><br><span class="line">        train_acc += acc</span><br><span class="line"></span><br><span class="line">    losses.append(train_loss / <span class="built_in">len</span>(train_loader))</span><br><span class="line">    acces.append(train_acc / <span class="built_in">len</span>(train_loader))</span><br><span class="line">    <span class="comment"># 在测试集上检验效果</span></span><br><span class="line">    eval_loss = <span class="number">0</span></span><br><span class="line">    eval_acc = <span class="number">0</span></span><br><span class="line">	<span class="comment"># 将模型改为预测模式（会将所有的training属性设置为False）</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">for</span> img, label <span class="keyword">in</span> test_loader:</span><br><span class="line">        img=img.to(device)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        img = img.view(img.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = model(img)</span><br><span class="line">        loss = criterion(out, label)</span><br><span class="line">        <span class="comment"># 记录误差</span></span><br><span class="line">        eval_loss += loss.item()</span><br><span class="line">        <span class="comment"># 记录准确率</span></span><br><span class="line">        _, pred = out.<span class="built_in">max</span>(<span class="number">1</span>)</span><br><span class="line">        num_correct = (pred == label).<span class="built_in">sum</span>().item()</span><br><span class="line">        acc = num_correct / img.shape[<span class="number">0</span>]</span><br><span class="line">        eval_acc += acc</span><br><span class="line"></span><br><span class="line">    eval_losses.append(eval_loss / <span class="built_in">len</span>(test_loader))</span><br><span class="line">    eval_acces.append(eval_acc / <span class="built_in">len</span>(test_loader))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#123;&#125;, Train Loss: &#123;:.4f&#125;, Train Acc: &#123;:.4f&#125;, Test Loss: &#123;:.4f&#125;, Test Acc: &#123;:.4f&#125;&#x27;</span></span><br><span class="line">          .<span class="built_in">format</span>(epoch, train_loss / <span class="built_in">len</span>(train_loader), train_acc / <span class="built_in">len</span>(train_loader), </span><br><span class="line">                     eval_loss / <span class="built_in">len</span>(test_loader), eval_acc / <span class="built_in">len</span>(test_loader)))</span><br></pre></td></tr></table></figure>
<p>迭代结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">epoch:<span class="number">0</span>, Train Loss:<span class="number">0.9885</span>, Train Acc:<span class="number">0.7987</span>, Test Loss:<span class="number">0.5235</span>, Test Acc:<span class="number">0.9073</span></span><br><span class="line">epoch:<span class="number">1</span>, Train Loss:<span class="number">0.4622</span>, Train Acc:<span class="number">0.9034</span>, Test Loss:<span class="number">0.3412</span>, Test Acc:<span class="number">0.9277</span></span><br><span class="line">epoch:<span class="number">2</span>, Train Loss:<span class="number">0.3403</span>, Train Acc:<span class="number">0.9208</span>, Test Loss:<span class="number">0.2666</span>, Test Acc:<span class="number">0.9376</span></span><br><span class="line">epoch:<span class="number">3</span>, Train Loss:<span class="number">0.2761</span>, Train Acc:<span class="number">0.9335</span>, Test Loss:<span class="number">0.2217</span>, Test Acc:<span class="number">0.9458</span></span><br><span class="line">epoch:<span class="number">4</span>, Train Loss:<span class="number">0.2379</span>, Train Acc:<span class="number">0.9410</span>, Test Loss:<span class="number">0.1946</span>, Test Acc:<span class="number">0.9511</span></span><br><span class="line">epoch:<span class="number">5</span>, Train Loss:<span class="number">0.2174</span>, Train Acc:<span class="number">0.9467</span>, Test Loss:<span class="number">0.1922</span>, Test Acc:<span class="number">0.9510</span></span><br><span class="line">epoch:<span class="number">6</span>, Train Loss:<span class="number">0.2137</span>, Train Acc:<span class="number">0.9482</span>, Test Loss:<span class="number">0.1891</span>, Test Acc:<span class="number">0.9517</span></span><br><span class="line">epoch:<span class="number">7</span>, Train Loss:<span class="number">0.2120</span>, Train Acc:<span class="number">0.9475</span>, Test Loss:<span class="number">0.1872</span>, Test Acc:<span class="number">0.9529</span></span><br><span class="line">epoch:<span class="number">8</span>, Train Loss:<span class="number">0.2091</span>, Train Acc:<span class="number">0.9487</span>, Test Loss:<span class="number">0.1843</span>, Test Acc:<span class="number">0.9530</span></span><br><span class="line">epoch:<span class="number">9</span>, Train Loss:<span class="number">0.2067</span>, Train Acc:<span class="number">0.9491</span>, Test Loss:<span class="number">0.1831</span>, Test Acc:<span class="number">0.9530</span></span><br><span class="line">epoch:<span class="number">10</span>, Train Loss:<span class="number">0.2043</span>, Train Acc:<span class="number">0.9498</span>, Test Loss:<span class="number">0.1841</span>, Test Acc:<span class="number">0.9540</span></span><br><span class="line">epoch:<span class="number">11</span>, Train Loss:<span class="number">0.2046</span>, Train Acc:<span class="number">0.9498</span>, Test Loss:<span class="number">0.1826</span>, Test Acc:<span class="number">0.9545</span></span><br><span class="line">epoch:<span class="number">12</span>, Train Loss:<span class="number">0.2037</span>, Train Acc:<span class="number">0.9497</span>, Test Loss:<span class="number">0.1814</span>, Test Acc:<span class="number">0.9542</span></span><br><span class="line">epoch:<span class="number">13</span>, Train Loss:<span class="number">0.2038</span>, Train Acc:<span class="number">0.9494</span>, Test Loss:<span class="number">0.1813</span>, Test Acc:<span class="number">0.9537</span></span><br><span class="line">epoch:<span class="number">14</span>, Train Loss:<span class="number">0.2044</span>, Train Acc:<span class="number">0.9500</span>, Test Loss:<span class="number">0.1805</span>, Test Acc:<span class="number">0.9547</span></span><br><span class="line">epoch:<span class="number">15</span>, Train Loss:<span class="number">0.2034</span>, Train Acc:<span class="number">0.9497</span>, Test Loss:<span class="number">0.1828</span>, Test Acc:<span class="number">0.9536</span></span><br><span class="line">epoch:<span class="number">16</span>, Train Loss:<span class="number">0.2026</span>, Train Acc:<span class="number">0.9508</span>, Test Loss:<span class="number">0.1815</span>, Test Acc:<span class="number">0.9533</span></span><br><span class="line">epoch:<span class="number">17</span>, Train Loss:<span class="number">0.2025</span>, Train Acc:<span class="number">0.9504</span>, Test Loss:<span class="number">0.1838</span>, Test Acc:<span class="number">0.9536</span></span><br><span class="line">epoch:<span class="number">18</span>, Train Loss:<span class="number">0.2034</span>, Train Acc:<span class="number">0.9501</span>, Test Loss:<span class="number">0.1827</span>, Test Acc:<span class="number">0.9542</span></span><br><span class="line">epoch:<span class="number">19</span>, Train Loss:<span class="number">0.2034</span>, Train Acc:<span class="number">0.9505</span>, Test Loss:<span class="number">0.1804</span>, Test Acc:<span class="number">0.9528</span></span><br></pre></td></tr></table></figure>
<p>该神经网络的结构比较简单，只用了两层，也没有使用Dropout层，迭代20次，测试准确率达到98%左右，效果还可以。不过，还是有提升空间，如果采用cnn、Dropout等层，应该还可以提升模型性能。</p>
<p>（2）可视化训练及测试损失值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">&#x27;trainloss&#x27;</span>)</span><br><span class="line">plt.plot(np.arange(<span class="built_in">len</span>(losses)), losses)</span><br><span class="line">plt.legend([<span class="string">&#x27;Train Loss&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片14.png" style="zoom: 90%;">
</div>

<h2 id="5-PyTorch数据处理工具箱"><a href="#5-PyTorch数据处理工具箱" class="headerlink" title="5. PyTorch数据处理工具箱"></a>5. PyTorch数据处理工具箱</h2><p>上节利用PyTorch的torchvision、data等包，下载及预处理MNIST数据集。数据下载和预处理是机器学习、深度学习实际项目中耗时又重要的任务，尤其是数据预处理，关系到数据质量和模型性能，往往要占据项目的大部分时间。好在PyTorch为此提供了专门的数据下载、数据处理包，使用这些包，可极大地提高我们的开发效率及数据质量。</p>
<p>PyTorch涉及数据处理（数据装载、数据预处理、数据增强等）主要工具包及相互关系如图4所示。</p>
<div align="center">
    <img src="/2022/10/22/PyTorch%E5%9F%BA%E7%A1%80/图片16.png" style="zoom: 100%;">
</div>

<center>图4 PyTorch主要数据处理工具</center>

<p>图4的左边是torch.utils.data工具包，它包括以下4个类。</p>
<blockquote>
<p>1）Dataset：是一个抽象类，其他数据集需要继承这个类，并且覆写其中的两个方法（_getitem<strong>、</strong>len__）；<br>2）DataLoader：定义一个新的迭代器，实现批量（batch）读取，打乱数据（shuffle）并提供并行加速等功能；<br>3）random_split：把数据集随机拆分为给定长度的非重叠的新数据集；<br>4）*sampler：多种采样函数。</p>
</blockquote>
<p>图4中间是PyTorch可视化处理工具（Torchvision），其是PyTorch的一个视觉处理工具包，独立于PyTorch，需要另外安装。它包括4个类，各类的主要功能如下。</p>
<blockquote>
<p>1）datasets：提供常用的数据集加载，设计上都是继承自torch.utils.data.Dataset，主要包括MMIST、CIFAR10/100、ImageNet和COCO等；<br>2）models：提供深度学习中各种经典的网络结构以及训练好的模型（如果选择pretrained=True），包括AlexNet、VGG系列、ResNet系列、Inception系列等；<br>3）transforms：常用的数据预处理操作，主要包括对Tensor及PIL Image对象的操作；<br>4）utils：含两个函数，一个是make_grid，它能将多张图片拼接在一个网格中；另一个是save_img，它能将Tensor保存成图片。</p>
</blockquote>
<h3 id="5-1-utils-data"><a href="#5-1-utils-data" class="headerlink" title="5.1 utils.data"></a>5.1 utils.data</h3><p>utils.data包括Dataset和DataLoader。torch.utils.data.Dataset为抽象类，自定义数据集需要继承这个类，并实现两个函数，一个是__len__，另一个是__getitem<strong>，前者提供数据的大小（size），后者通过给定索引获取数据和标签。\</strong>getitem__一次只能获取一个数据，所以需要通过torch.utils.data.DataLoader来定义一个新的迭代器，实现batch读取。下面定义一个简单的数据集，然后通过具体使用Dataset及DataLoader，获得一个直观的认识。</p>
<p>（1）导入需要的模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<p>（2）定义获取数据集的类。</p>
<p>该类继承基类Dataset，自定义一个数据集及对应标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TestDataset</span>(data.Dataset):<span class="comment">#继承Dataset</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.Data=np.asarray([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">2</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>]])<span class="comment">#一些由2维向量表示的数据集</span></span><br><span class="line">        self.Label=np.asarray([<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])<span class="comment">#这是数据集对应的标签</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment">#把numpy转换为Tensor</span></span><br><span class="line">        txt=torch.from_numpy(self.Data[index])</span><br><span class="line">        label=torch.tensor(self.Label[index])</span><br><span class="line">        <span class="keyword">return</span> txt,label </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.Data)</span><br></pre></td></tr></table></figure>
<p>（3）获取数据集中数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Test=TestDataset()</span><br><span class="line"><span class="built_in">print</span>(Test[<span class="number">2</span>])  <span class="comment">#相当于调用__getitem__(2)</span></span><br><span class="line"><span class="built_in">print</span>(Test.__len__())</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensor([<span class="number">2</span>, <span class="number">1</span>]), tensor(<span class="number">0</span>))</span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>以上数据以tuple返回，每次只返回一个样本。实际上，Dateset只负责数据的抽取，调用一次__getitem__只返回一个样本。如果希望批量处理（batch），还要同时进行shuffle和并行加速等操作，可选择DataLoader。DataLoader的格式为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">data.DataLoader(</span><br><span class="line">    dataset,</span><br><span class="line">    batch_size=<span class="number">1</span>,</span><br><span class="line">    shuffle=<span class="literal">False</span>,</span><br><span class="line">    sampler=<span class="literal">None</span>,</span><br><span class="line">    batch_sampler=<span class="literal">None</span>,</span><br><span class="line">    num_workers=<span class="number">0</span>,</span><br><span class="line">    collate_fn=&lt;function default_collate at <span class="number">0x7f108ee01620</span>&gt;,</span><br><span class="line">    pin_memory=<span class="literal">False</span>,</span><br><span class="line">    drop_last=<span class="literal">False</span>,</span><br><span class="line">    timeout=<span class="number">0</span>,</span><br><span class="line">    worker_init_fn=<span class="literal">None</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>主要参数说明：</p>
<blockquote>
<p>dataset：加载的数据集；<br>batch_size：批大小；<br>shuffle：是否将数据打乱；<br>sampler：样本抽样；<br>num_workers：使用多进程加载的进程数，0代表不使用多进程；<br>collate_fn：如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可；<br>pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些；<br>drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">test_loader = data.DataLoader(Test,batch_size=<span class="number">2</span>,shuffle=<span class="literal">False</span>,num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> i,traindata <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;i:&#x27;</span>,i)</span><br><span class="line">    Data,Label=traindata</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;data:&#x27;</span>,Data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Label:&#x27;</span>,Label)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">i: <span class="number">0</span></span><br><span class="line">data: tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">Label: tensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">i: <span class="number">1</span></span><br><span class="line">data: tensor([[<span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">Label: tensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">i: <span class="number">2</span></span><br><span class="line">data: tensor([[<span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">Label: tensor([<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>可以看出，这是批量读取，可以像使用迭代器一样使用它，比如对它进行循环操作。不过由于它不是迭代器，我们可以通过iter命令将其转换为迭代器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataiter=<span class="built_in">iter</span>(test_loader)</span><br><span class="line">imgs,labels=<span class="built_in">next</span>(dataiter)</span><br></pre></td></tr></table></figure>
<h3 id="5-2-torchvision"><a href="#5-2-torchvision" class="headerlink" title="5.2 torchvision"></a>5.2 torchvision</h3><p>如图4所示，torchvision有4个功能模块：model、datasets、transforms和utils，上文中使用datasets下载了一些经典数据集，本节介绍如何使用datasets的ImageFolder处理数据集，以及如何使用transforms对数据进行预处理、增强等。</p>
<h4 id="5-2-1-ImageFolder"><a href="#5-2-1-ImageFolder" class="headerlink" title="5.2.1 ImageFolder"></a>5.2.1 ImageFolder</h4><p>当文件依据标签处于不同文件下时，如：<br>─── data<br>    ├── zhangliu<br>    │   ├── 001.jpg<br>    │   └── 002.jpg<br>    ├── wuhua<br>    │   ├── 001.jpg<br>    │   └── 002.jpg<br>    ……………..<br>可以利用torchvision.datasets.ImageFolder来直接构造出dataset：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loader = datasets.ImageFolder(path, transform=<span class="literal">None</span>)</span><br><span class="line">loader = data.DataLoader(dataset)</span><br></pre></td></tr></table></figure>
<p>ImageFolder会将目录中的文件夹名自动转化成序列，当DataLoader载入时，标签自动就是整数序列了。</p>
<h4 id="5-2-2-transforms"><a href="#5-2-2-transforms" class="headerlink" title="5.2.2 transforms"></a>5.2.2 transforms</h4><p>transforms提供了对PIL Image对象和Tensor对象的常用操作。</p>
<p>（1）对PIL Image的常见操作</p>
<blockquote>
<p>Scale/Resize：调整尺寸，长宽比保持不变；<br>CenterCrop、RandomCrop、RandomSizedCrop：裁剪图片，CenterCrop和RandomCrop在crop时是固定size，RandomResizedCrop则是random size的crop；<br>Pad：填充；<br>ToTensor：把一个取值范围是[0,255]的PIL.Image转换成Tensor。形状为（H,W,C）的Numpy.ndarray转换成形状为[C,H,W]，取值范围是[0,1.0]的torch.FloatTensor；<br>RandomHorizontalFlip：图像随机水平翻转，翻转概率为0.5；<br>RandomVerticalFlip：图像随机垂直翻转；<br>ColorJitter：修改亮度、对比度和饱和度。</p>
</blockquote>
<p>（2）对Tensor的常见操作</p>
<blockquote>
<p>Normalize：标准化，即，减均值，除以标准差；<br>ToPILImage：将Tensor转为PIL Image。</p>
</blockquote>
<p>如果要对数据集进行多个操作，可通过Compose将这些操作像管道一样拼接起来，类似于nn.Sequential。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">transforms.Compose([</span><br><span class="line">    <span class="comment">#将给定的 PIL.Image 进行中心切割，得到给定的 size，</span></span><br><span class="line">    <span class="comment">#size 可以是 tuple，(target_height, target_width)。</span></span><br><span class="line">    <span class="comment">#size 也可以是一个 Integer，在这种情况下，切出来的图片形状是正方形。            </span></span><br><span class="line">    transforms.CenterCrop(<span class="number">10</span>),</span><br><span class="line">    <span class="comment">#切割中心点的位置随机选取</span></span><br><span class="line">    transforms.RandomCrop(<span class="number">20</span>, padding=<span class="number">0</span>),</span><br><span class="line">    <span class="comment">#把一个取值范围是 [0, 255] 的 PIL.Image 或者 shape 为 (H, W, C) 的 numpy.ndarray，</span></span><br><span class="line">    <span class="comment">#转换为形状为 (C, H, W)，取值范围是 [0, 1] 的 torch.FloatTensor</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    <span class="comment">#规范化到[-1,1]</span></span><br><span class="line">    transforms.Normalize(mean = (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), std = (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>本内容旨在为读者提供PyTorch的一些主要基础内容，仅包含PyTorch完整内容的一部分，欲学习更多PyTorch相关内容，请查看：<a class="link" target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html<i class="fas fa-external-link-alt"></i></a></p>
</blockquote>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul class="copyright-info-content">
        
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

            </div>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/2022/10/22/Topic1/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Topic1：Is Accurate and Light-weight DNN Multi-layer Latency Prediction Possible?</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2022/10/21/NeuralPower/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">NeuralPower：Predict and Deploy Energy-Efficient Convolutional Neural Networks</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
            <div class="comment-container">
                <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments"></i>&nbsp;Comments
    </div>
    

        
            
    <div class="valine-container">
        <script 
                src="//cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script>
        <div id="vcomments"></div>
        <script >
            function loadValine() {
                new Valine({
                    el: '#vcomments',
                    appId: 'AIhtvUXrzUWmPIOmYNjrGExW-gzGzoHsz',
                    appKey: 'e4HGIfsWiHCnIXco4g7xtuAA',
                    meta: ['nick', 'mail', 'link'],
                    avatar: 'wavatar',
                    enableQQ: true,
                    placeholder: 'Add your comments',
                    lang: 'en'.toLowerCase()
                });

                function getAuthor(language) {
                    switch (language) {
                        case 'en':
                            return 'Author';
                        case 'zh-CN':
                            return '博主';
                        default:
                            return 'Master';
                    }
                }

                // Add "Author" identify
                const getValineDomTimer = setInterval(() => {
                    const vcards = document.querySelectorAll('#vcomments .vcards .vcard');
                    if (vcards.length > 0) {
                        let author = 'Sun Hao';

                        if (author) {
                            for (let vcard of vcards) {
                                const vnick_dom = vcard.querySelector('.vhead .vnick');
                                const vnick = vnick_dom.innerHTML;
                                if (vnick === author) {
                                    vnick_dom.innerHTML = `${vnick} <span class="author">${getAuthor(KEEP.hexo_config.language)}</span>`
                                }
                            }
                        }
                        clearInterval(getValineDomTimer);
                    } else {
                        clearInterval(getValineDomTimer);
                    }
                }, 2000);
            }

            if ('false') {
                const loadValineTimeout = setTimeout(() => {
                    loadValine();
                    clearTimeout(loadValineTimeout);
                }, 1000);
            } else {
                window.addEventListener('DOMContentLoaded', loadValine);
            }
        </script>
    </div>



        
    
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            
<footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
                <span>2022</span> -
            
            2023
            
                &nbsp;<i class="fas fa-heart icon-animate"></i>
                &nbsp;<a href="/">Sun Hao</a>
            
        </div>
        
            <script async 
                    src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                
                
                    Totalview&nbsp;<span id="busuanzi_value_site_pv"></span>
                
            </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme
            &nbsp;
            <a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.8</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fas fa-comment"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-up-right-and-down-left-from-center"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%BC%95%E8%A8%80"><span class="nav-text">1. 引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Numpy%E4%B8%8ETensor"><span class="nav-text">2. Numpy与Tensor</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E5%88%9B%E5%BB%BATensor"><span class="nav-text">2.1 创建Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E4%BF%AE%E6%94%B9Tensor%E5%BD%A2%E7%8A%B6"><span class="nav-text">2.2 修改Tensor形状</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E7%B4%A2%E5%BC%95%E6%93%8D%E4%BD%9C"><span class="nav-text">2.3 索引操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="nav-text">2.4 广播机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-%E9%80%90%E5%85%83%E7%B4%A0%E6%93%8D%E4%BD%9C"><span class="nav-text">2.5 逐元素操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-%E5%BD%92%E5%B9%B6%E6%93%8D%E4%BD%9C"><span class="nav-text">2.6 归并操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-%E6%AF%94%E8%BE%83%E6%93%8D%E4%BD%9C"><span class="nav-text">2.7 比较操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-%E7%9F%A9%E9%98%B5%E6%93%8D%E4%BD%9C"><span class="nav-text">2.8 矩阵操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-9-PyTorch%E4%B8%8ENumpy%E6%AF%94%E8%BE%83"><span class="nav-text">2.9 PyTorch与Numpy比较</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E4%BD%BF%E7%94%A8Tensor%E5%8F%8AAutograd%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-text">3. 使用Tensor及Autograd实现机器学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-PyTorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7%E7%AE%B1"><span class="nav-text">4. PyTorch神经网络工具箱</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="nav-text">4.1 神经网络核心组件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7%E7%AE%B1nn"><span class="nav-text">4.2 神经网络工具箱nn</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1-nn-Module"><span class="nav-text">4.2.1 nn.Module</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-2-nn-functional"><span class="nav-text">4.2.2 nn.functional</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-text">4.3 优化器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%84%E5%BB%BA%E5%AE%9E%E4%BE%8B"><span class="nav-text">4.4 神经网络构建实例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-1-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="nav-text">4.4.1 数据准备</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-2-%E5%8F%AF%E8%A7%86%E5%8C%96%E6%BA%90%E6%95%B0%E6%8D%AE"><span class="nav-text">4.4.2 可视化源数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-3-%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="nav-text">4.4.3 模型构建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-4-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-text">4.4.4 模型训练</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-PyTorch%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%E7%AE%B1"><span class="nav-text">5. PyTorch数据处理工具箱</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-utils-data"><span class="nav-text">5.1 utils.data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-torchvision"><span class="nav-text">5.2 torchvision</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-1-ImageFolder"><span class="nav-text">5.2.1 ImageFolder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-2-transforms"><span class="nav-text">5.2.2 transforms</span></a></li></ol></li></ol></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="close-popup-btn">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>





    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-block-tools.js"></script>





<div class="post-scripts">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>



</body>
</html>
